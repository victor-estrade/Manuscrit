%!TEX root = ../thesis.tex
%*******************************************************************************
%******************************  5th  Chapter **********************************
%*******************************************************************************

\chapter{Experimental results}
\label{chap:xp}
% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter5/Figs/Raster/}{Chapter5/Figs/PDF/}{Chapter5/Figs/}}
\else
    \graphicspath{{Chapter5/Figs/Vector/}{Chapter5/Figs/}}
\fi



\section{Toy d'isabelle} % (fold)
\label{sec:toy_d_isabelle}

MNIST les 1 vs les 0.
Projection 1D.
L'observable est le nombre de pixel alumé et la systematique est la luminosité.
Ça donne une observable affectée par la systématique.

Mais si l'observable est la rondeur le l'image alors on a une observable orthogonale à la systématique.




\section{Technical difficulties} % (fold)
\label{sec:technical_difficulties}

Where do I write about the numerous issues ?
\begin{itemize}
  \item Inferno is unstable
  \item Pivot is unstable
  \item TP requires Jacobian vector product or takes crazy amount of time to train
  \item Regressor requires slow Adam and is probably a pain to train with gaussian mixture
  \item 
\end{itemize}



The benchmark is controlled with many parameters
\begin{itemize}
  \item Number of test samples
  \item Parameter of the toy model
  \item True value of the parameter of interest and nuisance parameters
  \item Sampling random seed
  \item TODO finish this list
\end{itemize}








\section{Performances on toys} % (fold)
\label{sec:performances_on_toys}

The idea is to do one subsection for one result 
\begin{itemize}
  \item Result 1 : The more test samples the better the inference
  \item Result 2 : Calib > Prior as expected
  \item Result final : Who is the best ? 
  \item 1. Baseline on nominal is as expected
  \item 2. All other methods on nominal also behave as expected
  \item 4. Regressor performances does not depend on the number of samples which is weird
  \item 5. Regressor estimated variance is probably broken
  \item 3. Asymetry with alpha because the problem itself is asymetric
\end{itemize}


Let's start with the results on the two toy problems (see \autoref{sec:toy_datasets} for details).
Toys provides a fully controled environement on various parameters of the problem to confront the proposed methods against different difficulties.
The first one is the small number of samples to do inference (\autoref{sub:performance_according_to_sample_size}).










\subsection{Performance according to sample size} % (fold)
\label{sub:performance_according_to_sample_size}

Here is summaried the performances measured according to the number of sample of the test set.

On the nominal test set the MSE behave as expected \ie decreases fast ($\frac{1}{\sqrt{N}}$) as the size of the test set increase.
Only direct regression seems to not depend on the test size.
This is unexpected and cannot only be explained by the fact that the neural network is trained with fixed dataset size and did not learn to improve as the test set size increase...




\victor{Performance en fonction du nombre d'events dans le dataset de test ? Evolution de la variance stat et syst en fonction de N\_samples.}


\begin{figure}[ht!]
  \centering
  \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{COMPARE/GG-prior/GradientBoostingModel/profusion_nominal_n_samples_mse.png}
    \caption{Gradient boosting}
    % \label{fig:gg-prior_GB_profusion_nominal_n_samples_mse}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{COMPARE/GG-prior/NeuralNetClassifier/profusion_nominal_n_samples_mse.png}
    \caption{Neural network classifier}
    % \label{fig:gg-calib_best_average_errplot_mse}
  \end{subfigure}

  \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{COMPARE/GG-prior/DataAugmentation/profusion_nominal_n_samples_mse.png}
    \caption{Data augmentation}
    % \label{fig:gg-prior_GB_profusion_nominal_n_samples_mse}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{COMPARE/GG-prior/TangentPropClassifier/profusion_nominal_n_samples_mse.png}
    \caption{Tangent Prop}
    % \label{fig:gg-calib_best_average_errplot_mse}
  \end{subfigure}

  \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{COMPARE/GG-prior/Inferno/profusion_nominal_n_samples_mse.png}
    \caption{Inferno}
    % \label{fig:gg-prior_GB_profusion_nominal_n_samples_mse}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{COMPARE/GG-prior/Regressor/profusion_nominal_n_samples_mse.png}
    \caption{Regressor}
    % \label{fig:gg-calib_best_average_errplot_mse}
  \end{subfigure}


  \caption{MSE on nominal data for the baseline decrease as the number of samples increases.}
  \label{fig:gg_baseline_nominal_n_samples_mse}
\end{figure}











\subsection{Calibration influence} % (fold)
\label{sub:calibration_influence}


In this benchmark two way of calibrating the data is used.
\begin{enumerate}
  \item Trust a given prior
  \item Infer the nuisance parameter distribution using a regressor
\end{enumerate}

Here we show that the later is a better choice and improves significantly the performances of all methods.
Indeed using only a poorly informed prior leads the baseline to a biased estimator.
\autoref{fig:gg_baseline_compare_calib_estimator} shows the estimation of a neural net classifier.
It is clear that the estimation depends on $\alpha^\star$ and is biased if $\alpha^\star$ is not the nominal value.
\victor{TODO : GB \& TP \& REG show the same behaviour. INFERNO is not affected. DA is more robust. Waiting for PIVOT results}

Using the given data to improve calibration leads to an unbiased estimator.
\victor{TODO : Someone may argu that it is because My prior calibration is bad. But my simulator cannot do better...}






\begin{figure}[ht!]
  \centering
  \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{COMPARE/GG-prior/NeuralNetClassifier/profusion_true_mu_target_mean.png}
    \caption{Prior calibration}
    % \label{fig:gg-prior_GB_profusion_nominal_n_samples_mse}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{COMPARE/GG-calib/NeuralNetClassifier/profusion_true_mu_target_mean.png}
    \caption{Using data in calibration to infer $\alpha$}
    % \label{fig:gg-calib_best_average_errplot_mse}
  \end{subfigure}
  \caption{Many estimations of neural net classifier. Each point is colored according to the true value of $\alpha$. The x-axis is representing the true value of $\mu$. Each point is the average of the estimated interest parameter ($\hmu$) over cross validation of one neural network classifier (one hyper-parameter set). On the left is the estimations of }
  \label{fig:gg_baseline_compare_calib_estimator}
\end{figure}


The 1D toy problem is asymetric by definition which leads to an asymetry in performances when using the prior calibration.


\begin{figure}[ht!]
  \centering
  \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{COMPARE/GG-prior/NeuralNetClassifier/profusion_n_samples_mse.png}
    \caption{Prior calibration}
    % \label{fig:gg-prior_GB_profusion_nominal_n_samples_mse}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{COMPARE/GG-calib/NeuralNetClassifier/profusion_n_samples_mse.png}
    \caption{Using data in calibration to infer $\alpha$}
    % \label{fig:gg-calib_best_average_errplot_mse}
  \end{subfigure}
  \caption{Calibration is the main influence on the inference.}
  \label{fig:gg_baseline_compare_calib_n_samples_mse}
\end{figure}









\subsection{Compare methods} % (fold)
\label{sub:compare_methods}


The good news is that systematic aware learning and the direct approach is giving better results that the baseline (\autoref{fig:compare_gg_best_mse}).

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{COMPARE/GG-prior/BEST_MSE/GG-prior_best_average_N=2000-boxplot_mse.png}
    \caption{Boxplot of MSE on Prior}
    \label{fig:gg-prior_best_average_boxplot_mse}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{COMPARE/GG-prior/BEST_MSE/GG-prior_best_average_N=2000-errplot_mse.png}
    \caption{Average MSE $\pm$ variance on Prior}
    \label{fig:gg-prior_best_average_errplot_mse}
  \end{subfigure}

  \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{COMPARE/GG-calib/BEST_MSE/GG-calib_best_average_N=2000-boxplot_mse.png}
    \caption{Boxplot of MSE on Calibrated GG}
    \label{fig:gg-prior_best_average_boxplot_mse}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{COMPARE/GG-calib/BEST_MSE/GG-calib_best_average_N=2000-errplot_mse.png}
    \caption{Average MSE $\pm$ variance on Calibrated GG}
    \label{fig:gg-calib_best_average_errplot_mse}
  \end{subfigure}

  \caption{Best MSE on GG with 2000 test samples with calibration on data. Distribution according to $\mu^\star$ and $\alpha^\star$.}
  \label{fig:compare_gg_best_mse}
\end{figure}

Reg-Marginal is very good on the 1D toy problem which may be explained by the simplicity of the problem.
The average of the only observable $x$ is linearly connected to the parameter of interest (see \autoref{fig:gg_mean_link}).
The only extra work the marginal regressor has to do is to infer $\alpha$ to adjust itsinference which is also very simple in this toy.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=0.49\linewidth]{GG/mean_link.png}
  \caption{link between $\mu^\star$ and the average of $x$ on GG toy problem}
  \label{fig:gg_mean_link}
\end{figure}


Inferno is doing very well including on the Prior calibration.

Tangent propagation is failing to learn a robust distribution.
This is expected as the only feature $x$ is both very correlated to the parameter of interest but also to the nuisance parameter.
Making the classification good and independant from $\alpha$ at the sample level is not possible.

Data augmentation is very good !
Data augmentation is learning to be less confident about the classification which spread the score distribution of the signals.
This spreading leads to splitting signal events into 2 bins instead on one (see \autoref{fig:gg_prior_distrib_summaries}).
\textbf{But how is it helping inference ???}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{GG-prior/NN/valid_distrib.png}
    \caption{NN score distribution}
    % \label{fig:gg-prior_best_average_boxplot_mse}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{GG-prior/DA/valid_distrib.png}
    \caption{DA score distribution}
    % \label{fig:gg-prior_best_average_errplot_mse}
  \end{subfigure}

  \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{GG-prior/NN/valid_summaries.png}
    \caption{NN summaries}
    % \label{fig:gg-prior_best_average_boxplot_mse}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{GG-prior/DA/valid_summaries.png}
    \caption{DA summaries}
    % \label{fig:gg-calib_best_average_errplot_mse}
  \end{subfigure}

  \caption{Score distribution and bin counts for NN classifier and data augmentation}
  \label{fig:gg_prior_distrib_summaries}
\end{figure}













\section{Old stuff of chap 5} % (fold)
\label{sec:old_stuff_of_chap_5}


\begin{figure}[ht!]
  \centering
  \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{minitoy/marginal_y.png}
    \caption{$p(y|x)$}
    \label{fig:marginal_y}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{minitoy/marginal_alpha.png}
    \caption{$p(\alpha|x)$}
    \label{fig:marginal_alpha}
  \end{subfigure}
  \caption{Posterior probabilities for $y$ (left) and $\alpha$ (right)}
  \label{fig:marginals_gg}
\end{figure}

\emph{Remarque :}
L'erreur statistique et l'erreur systématique diminuent ensemble avec l'augmentation du nombre de donnée.
Plus on a de donnée meilleur est l'inférence sur $\alpha$.
Donc c'est normal !

Du coup j'ai vraiment du mal avec le concepte de l'erreur systématique qui ne diminue que doucement avec l'augmentation des données.

Si l'effet est invisible sur les données alors on a pas de problème.
Si l'effet est visible alors on peut contraindre + les paramètres de nuisance puisqu'on peut le mesurer !


\begin{figure}[ht!]
  \centering
  \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{s3d2/marginal_r.png}
    \caption{$p(r|x)$}
    \label{fig:marginal_r}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{s3d2/marginal_lam.png}
    \caption{$p(\lambda|x)$}
    \label{fig:marginal_lambda}
  \end{subfigure}

  \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{s3d2/marginal_mu.png}
    \caption{$p(\mu|x)$}
    \label{fig:marginal_mu}
  \end{subfigure}
  \caption{Posterior probabilities for $y$ (left) and $\alpha$ (right)}
  \label{fig:marginals_s3d2}
\end{figure}







\section{Real data} % (fold)
\label{sec:real_data}


\victor{Find out why this is does not work : \url{https://arxiv.org/pdf/1909.03081.pdf} ??}
\victor{La simulation est trop précise. cf resultat sur l'impossibilité de séparer les domaines créé}








\subsection{Nothing beats the baseline} % (fold)
\label{sub:nothing_beats_the_baseline}

\content{Show results on Higgs}







\subsection{Impossible to separate between domains} % (fold)
\label{sub:impossible_to_separate_between_domains}

\content{Show results of classifier trying to separate events between "extreme" values of nuisance params}









\subsection{Results with or without calibration} % (fold)
\label{sub:results_with_or_without_calibration}

\content{Compare results with or without using the current data in the calibration}







\subsection{Too rare signals} % (fold)
\label{sub:too_rare_signals}


\content{Explain and show that the issue comes from the imbalance between signals and backgrounds}

The scatter plot properties is much more dependent to noise or nuisance parameters than to the parameter of interest.




\section{Forest robustness} % (fold)
\label{sec:forest_robustness}

\victor{Grad boost is robust because trees disagrees between themselves}


