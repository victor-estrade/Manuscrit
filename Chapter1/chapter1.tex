%!TEX root = ../thesis.tex
%*******************************************************************************
%*********************************** First Chapter *****************************
%*******************************************************************************

\chapter{An exemple of data-driven science}  %Title of the First Chapter
\label{chap:intro_stat}
\ifpdf
    \graphicspath{{Chapter1/Figs/Raster/}{Chapter1/Figs/PDF/}{Chapter1/Figs/}}
\else
    \graphicspath{{Chapter1/Figs/Vector/}{Chapter1/Figs/}}
\fi

\victor{This is a example of comment}

\cecile{This is a example of comment}

\isabelle{This is a example of comment}

\david{This is a example of comment}

\topic{This is a example of topic}
\content{This is a example of content}


\topic{Simulations combined with machine learning make possible to extract knowledge even in highly complex and stochastic process like High Energy Physics}

\content{Objectif : Improving the precision of parameter estimation in a special case of the inverse problem in the presence of systematic effect.}






\section{Inverse problem at LHC} % (fold)
\label{sec:inverse_problem_at_lhc}

% section inverse_problem_at_lhc (end)

\topic{C'est très complexe mais on a pas besoin de toute cette complexité pour saisir le problème.}





\subsection{The system} % (fold)
\label{sub:the_system}

The system producing the data studied in this analysis is the famous Large Hardon Collider (LHC).
Without getting into the details a particle collider is a machine accelerating small bricks of matter, protons in this case, in opposite direction to smash them against each others.
The resulting collision produces high energy particles whose properties are captured by the various measurement apparatus which is reduced here for simplicity to a "giant camera".

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.8\linewidth]{particle_collider_0}
    \caption{Very simple particle collider}
    \label{fig:particle_collider_0}
\end{figure}


These collisions can be classified into 2 kinds :
\begin{itemize}
	\item the soft collisions when the protons "missed" each other and does not produce high energy particles
	\item the hard collisions when the protons smashed on each other and produces many particles
\end{itemize}

\begin{figure}[htb]
  \centering
  \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{particle_collider_soft}
    \caption{soft collision}
    \label{fig:soft_collision}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{particle_collider_hard}
    \caption{hard collision}
    \label{fig:hard_collision}
  \end{subfigure}
  \caption{soft collision (left) and hard collision (right)}
  \label{fig:collision}
\end{figure}

The hard collisions, named \emph{event} in the field of High Energy Physics (HEP), are way rarer than the soft one.
The process creating the high energy particles is fundamentally stochastic.
Meaning that the nature and properties (eg. kinematics) of the process producing particles are not fully predictable but follow
probability distributions whose shapes and properties are deterministics.

The vast majority of the processes are already well known.
To produce (very) rare therefore interesting processes the accelerator must produce a collosal number of collisions.

The LHC is an international collaboration, involving thousands of poeple and housing many HEP experiments.
The one that is of interest in this work is a measurement of the frequency of a specific process ($H \to \tau \tau$).
Basically an event produced a Higgs boson which desintegrated into 2 $\tau$ particles.
Each event following this process is defined as a \emph{signal} event versus the \emph{background} events that gather all the other processes.
Leading to a counting experiment of the number of signals $s$ and the number of backgrounds $b$ to give access to the quantity of interest $\frac{s}{s + b}$.

In many interesting cases, including ours, the nature of the event (signal / background) are not among the possible measurement that can be made.
Counting signals and backgrounds requires some extra work.
Especially a very accurate description of the various steps in the experiment which is the role of the model.


\content{Introduce cross section estimations / mixture coefficient inference ?}
$m$ is therefore connected to branch factor (cross section ?) of the Higgs boson which is the parameter we want to measure.
\victor{Link between cross-section, branch factor, luminosity ?}





\subsection{The model} % (fold)
\label{sub:the_model}

The Standard Model \needcite (SM) is the theory describing quantum physics including particle collisions and productions.
The counting experiment final objective is to improve our knowledge of one of the free parameter of the SM.
In other word we are fitting the SM parameters to the data of the experiment.

Let's start by defining the quanties describing the process.
First, the estimated parameter is $\mu$.
In our case it is the frequency of the signal process.
The true value of $\mu$, assuming that the model is perfect, is noted $\mu^\star$ as opposed to $\hmu$ the value estimated with the data.
Of course the objective is to build an estimator $\hmu$ whose value is as close as possible to the true value $\mu^\star$.
Second, the observables ie the data collected by the apparatus for a single event is $x$.
The LHC produces a large quantity of events while ensuring that the conditions are the same for all events.
Hence we can assume that the full dataset $D$ contains $N$ independant and identically distributed (iid) events $D = \{x_i\}_{i=1}^N$.

Going from the fundamental parameters to the observables is very complex and not in the direct interest of this study.
The process can be summaried into four major steps :
\begin{enumerate}
	\item The collision between particles
	\item The particle production from the collision
	\item The journey of the created particles to the apparatus (particle desintegrations into other particles, interactions between produced particles, etc)
	\item Reaction of the measurement apparatus to the particles going through it.
\end{enumerate}

Each of these steps requires special care from the community to be accurately described.
\victor{Maybe a few numbers to show how much it is complicated (nb poeple working on it, time required, nb of paper, etc)}

Although modeling what happens between the particle production and the apparatus is possible it is by definition impossible to get data about what really happened.
Leading to the necessity to infer what happened before the measurement from the data.
But also turning the vast majority of the intermediate quantities into latent or hidden variables.
Latent variables are noted $z$ or $z_i$.
One of this latent variable was already mentioned previously \autoref{sub:the_system} : the \emph{label} of the event indicating the nature of the particule, in our case if it is a \emph{signal} or a \emph{background} event.

\victor{L'objectif est de donner un apperçu de la complexité de la chose. Sans entrer + que ça dans les détails...}
\victor{TODO : Donner un aperçu de la complexité de chaque étape en 1 ou 2 phrases.}

Finally gathering all the ingredients builds a simplified model of the process as :
\begin{equation}
	\label{eq:model_simple}
	p(x, \mu, z) = p(x|z) p(z | \mu) p(\mu)
\end{equation}
In other words the fundamental parameter $\mu$ shapes the distribution of the latent variable $z$.
And the data distribution of $x$ depends on the latent variable $z$.





\subsection{Classic parameter estimation} % (fold)
\label{sub:classic_parameter_estimation}

\topic{Bayesian inference gives access to the full posterior distribution}


Assuming that the model provides a likelihood $p(x | \mu)$ the Bayes theorem indicates how to access the posterior pobability.
\begin{equation}	
    p(\mu | x) = \frac{p(x|\mu) p(\mu)}{p(x)} = \frac{p(x|\mu) p(\mu)}{\int_\mu p(x|\mu) p(\mu)}
\end{equation}

The prior $p(\mu)$ contains all the current knowledge about the parameter. 
If no knowledge is available a non informative prior is chosen, usually a uniform distribution over the domain of $\mu$.
The inference is then straitforward.
Computing $p(\mu | x)$ for the possible values $\mu$ \ie where the prior probability is not zero.
The integral on the denominator can either be computed by hand or approximated with Monte Carlo.

Often the full posterior is not required and only the most probable value of the parameter is infered.
When no prior knowledge on the parameter of interest is available maximum a posteriori and maximum likelihood estimators are strickly equivalent.
\begin{align}
	\argmax_\mu p(\mu | x) &= \argmax_\mu p(\mu | x) \\
							&= \argmax_\mu \frac{p(x|\mu) p(\mu)}{p(x)} \\
							&= \argmax_\mu p(x|\mu) p(\mu) \\
							&= \argmax_\mu p(x|\mu)\\
\end{align}
Solving this maximization usually involves either analytical computing of the formulas \victor{Why not givin an example in the appendix ?} or an numerical optimization process (eg. gradient ascent, coordinate ascent)








\subsection{Inverse problem} % (fold)
\label{sub:inverse_problem}

\topic{These methods don't scale to high dimensions}

Unfortunately the likelihood provided by the model is not tracktable.
The process leading from the collisions to the obvervables involves numerous latent variables $z_1, ..., z_n$ where $n=10^6$.
Making the marginal likelihood intraclable because of high dimensional integrals.
\begin{equation}
	\label{eq:intractable_integral}
	p(x|\mu) = \int_{z_1} \int_{z_2} ... \int_{z_n} p(x|\mu, z_1, z_2, ..., z_n) p(z_1) p(z_2) ... p(z_n)
\end{equation}

\victor{What do we know about numerical stability of high dimensional integral ?}
\content{Maybe a "simple" example that shows how often in real life this setting may happen}

Moreover sometimes simplifications needs to be made in order to build the model \needcite making the computed likelihood only a approximation of the true likelihood.
Some other times computable formulas simply do not exist \needcite.

\content{Approximation requiered to build some part of LHC simulation as an example}
\content{Here or in the previous subsection : 1/2 page on the processes to simulate an event. In order to make clear how complicated it is.}


However building a simulator working only in forward mode allowing to sample from $p(x|\mu)$ is easier and possible in cour case.
The objective is then to infer causal parameters from observations hence reversing the forward process which goes from causal parameters to observations.
This is more generaly known as the \emph{inverse problem}.
There is no general solution yet to this problem altough it occures often in experimental science.


\begin{figure}[htb]
    \centering
    \includegraphics[width=0.8\linewidth]{inverse_problem}
    \caption{The inverse problem objective is to go from the observables to the causal parameter of the model embeded by the simulator}
    \label{fig:inverse_problem}
\end{figure}








\section{Inference through simulation} % (fold)
\label{sec:inference_through_simulation}

\victor{Or "Inference with/using simulations" ?}

\topic{Simulations/models are useful to explain data (therefore understand the undelying process that generated the data)}


\content{Dans un monde simple tout est "facile"}
\content{Introduire le problème d'extraction d'un coefficient de mélange(lié à cross-section)}









\subsection{Hand crafted dimension reduction} % (fold)
\label{sub:hand_crafted_dimension_reduction}

Since the likelihood cannot be computed directly one solution is to sample many events and apply density estimation methods.
Examples : histograms, kernel density estimation.
Tackling the inverse problem with density estimation is subject to the curse of dimensionality.
Restricting the observables to only one or two dimensions requires those dimension to carry all or most the information about the estimated parameter.

The first idea to achieve such constraint is to use domain knowledge.
Bassically ask the theorist to find a computable quantity from the possible measurement allowing to infer the parameter.
\victor{TODO : Fouiller les années 80 à la recherche d'exemple}

\victor{Discard the less relevants ones (feature selection). Pas sûr que ça soit vraiment utilisé. Demander à David.}

The second possibility is to use machine learning to reduce the dimension of the data.


\victor{Parzen windows to estimated density to get a likelihood. Maybe beats curse of dimension.}
Parzen windows can estimate densities !
So we can use it on the simulated data to define the likelihood function.






\subsection{Count estimation} % (fold)
\label{sub:count_estimation}


This section borrows many results from \cite{Neal:2007zz}.

The stochatic phenomenon of interest here displays the following generative process :
\begin{equation}
	\label{eq:mixture_model}
	p(x|\mu) = \mu p(x|S) + (1-\mu) p(x|B)
\end{equation}
where $x$ is the set of observable features of the studied event gathered in a vector.
Events are split into 2 classes : the signals $S$ and the backgrounds $B$.
Note that $S$ and $B$ are one of the numerous latent variable $z_i$ in \autoref{eq:intractable_integral}.
$\mu$ is the mixture coefficient between signals and backgrounds.

$\mu$ can be seen as the probability for an event to be a signal $p(S)$. 
It naturally follows that $1-\mu$ is the probability for an event to be a background $p(B)=1-p(S)$.
\autoref{eq:mixture_model} can be re-written as
\begin{equation}
	p(x) = p(S)p(x|S) + p(B)p(x|B)
\end{equation}

As previously explained the likelihoods $p(x|S)$ and $p(x|B)$ are intractable because they involves high dimension integrals.
However building a simulator working only in forward mode allowing to sample from $p(x|\mu)$ is possible.
This allow us to build a training dataset for some machine learning later.

Measurements are made from a large bunch of independant and identically distributed events $D=\{x_i\}_{i=1}^N$.

\begin{align*}
	p(D|\mu) =& \prod_{i=1}^N \mu p(x|S) + (1-\mu) p(x|B) \\
	       =& \prod_{i=1}^N p(x|B) \left [(1-\mu) + \mu \frac{p(x|S)}{p(x|B)} \right ]\\
\label{eq:Fisher-Neyman}
	       =& \underbrace{\left[ \prod_{i=1}^N p(x|B) \right ]}_{h(x)} \times 
	       \underbrace{\left [\prod_{i=1}^N (1-\mu) + \mu \frac{p(x|S)}{p(x|B)} \right ]}_{g_\mu(T(x))}
\end{align*}
with $T(x) = \frac{p(x|S)}{p(x|B)} $

The Fisher-Neyman factorization theorem \needcite states that $T(x)$ is a sufficient summary statistic to obtain $\mu$

The maximum likelihood estimator, noted $\hat \mu$, is commonly used to estimate the parameter of interest.
This is a reasonable choice when we do not have prior knowledge as in this example.

\begin{equation}
	\hat \mu = \argmax_\mu p(\mu | D)
\end{equation}

It is more convenient (numerical stability) to express the result as a deviation from the prediction of the Standard Model.
The deviation is defined as :

\begin{equation}
	\mu = \frac{p(S)}{p_{SM}(S)} = \frac{\mu_0}{p_{SM}(S)}
\end{equation}
$p_{SM}(S)$ is the expected probability to get a signal following the Standard Model.
Recovering the frequency $\mu_0$ from $\mu$ is trivially done with $\mu_0 = \mu p_{SM}(S)$.

The estimator is now :
\begin{align}
	\hmu =& \argmax_\mu p(\mu | D) \\
	     =& \argmax_\mu \frac{p(\mu)}{p(D)} p(D | \mu) \\
	     =& \argmax_\mu p(\mu) p(D | \mu) \\
	     =& \argmax_\mu  p(D | \mu) \\
	     =& \argmax_\mu  \prod_{i=1}^N g_\mu(T(x)) \\
\end{align}


$T(x)$ can be obtained using a classifier $c$ trained to separate signals and backgrounds.
A Bayes optimal classifier output gives :
\begin{equation}
	c(x) = \frac{n_s p(x|S)}{(1-n_s) p(x|B) + n_s p(x|S)}
\end{equation}
where $n_s$ is the fraction of signals used in the training dataset.

\begin{equation}
	T(x) = \frac{c(x)}{(1-c(x))} \frac{(1-n_s)}{n_s} 
\end{equation}


Note : $c$ is also a sufficient summary statistic
\begin{equation}
	g_\mu(T(x)) = 1 - \mu p_{SM}(S) + \mu p_{SM}(S) \times \frac{c(x)}{(1-c(x))} \frac{(1-n_s)}{n_s} = f_\mu(c(x))
\end{equation}

\content{Limitation : what happens if the classifier is not (Bayes) optimal ? Can we detect it ?}
\content{Physicis alos uses the Neyman-Pearson theorem on statistical test to justify maximum likelihood estimator usage. Should I include it here ? or in \autoref{sub:maximum_likelihood} ?}





\subsection{Poisson count process} % (fold)
\label{sub:poisson_count_process}

The system allows 2 kind of collisions : soft and hard collisions.
The hard collisions are very rare compared to the soft collisions.
The data can be modeled by counting iid Bernouilli rare process which is well approximated by a Poisson distribution \needcite when the number of samples is large.
The hard collision are splitted into 2 categories the signal events and the background events.
The amount of background event $b$ is known (with high precision) thanks to measurements made in the data space where few or no signals can be found.

The total amout of event $n$ follows a Poisson distribution of parameter $s + b$.
Since usually $s$ is very small the parameter of interest is a deviation $\mu$ from an expected value $s$.

\begin{equation}
	n \sim Poisson(\mu s + b)
\end{equation}

Giving the likelihood :
\begin{equation}
	P(n| \mu, s, b) = \frac{(\mu s +b)^n }{n!} e^{-(\mu s + b)}
\end{equation}

A computable likelihood is now available !
This allow to use more classic parameter estimation like maximum likelihood.
The only drawback is that the nature of the event (signal/background) is not available in the data.
But a classifier can be trained to separate signals and backgrounds from training data extracted from the simulator.
As seen previously a classifier decision function is a sufficient summary statistic making this method optimal.
\victor{optimal method = strong claim. Better be sure about it}

To improve inference the classifier is also used to select a bunch of regions rich in signals.
\victor{(yes but how/why !!) Probably a simple result link to signal / noise ratio but I can't find it...}
Leading to a binned likelihood :

\begin{equation}
	p(D|\mu) = \prod_{i=1}^M Poisson(n_i | \mu, s_i, b_i) = \prod_{i=1}^M \frac{(\mu s_i + b_i)^n }{n!} e^{-(\mu s_i + b_i)}
\end{equation}










\section{Systematic effects} % (fold)
\label{sec:systematic_effects}

\topic{Systematic effects make inference more complex by introducing nuisance parameters}

The parameter of interest is not the only causal parameter in real life experiments.
Many other parameters are required to explain the observed data.
Here is described the methods to take into account those additional parameters and the impact on inference. 



\subsection{Definition} % (fold)
\label{sub:definition}

Real life data modeling often requires several parameters to be fully described.
On the other hand only one or a few of these parameters are the object of one study.
This leads to asign paramters into two classes : the parameter of interest and the nuisance parameters.

Nuisance parameters can appear from :
\begin{itemize}
	\item apparatus imperfections
	\item theory flaws
	\item fundamental parameter uncertainties
	\item etc
\end{itemize}

For example : Bob wants to estimate the efficiency of its hotplate by measuring the time required to heat 1L of water in a saucepan to boiling temperature.
Each time Bob need boiling water to cook he will run this little experiment.
Bob measured the temperature of the water and the air and the air pressure and know the electric power of his hotplate.
Unfortunatelly Bob is using tap water and cannot measure the amount of impurities in the water which have a influence on the boiling temperature.
The impurities concentration in the water is now a nuisance parameter of the problem (fundamental uncertainty).
Moreover the thermometer is a cheap one and may be biased toward lower or upper temperature introducing another nuisance parameter (apparatus imperfection).
Finally to simplify the computation Bob assumed that the heat produced by the hotplate would be mostly transmitted to the water although this approximation is too strong Bob will take it into account as an additionnal nuisance parameter (theory flaws).





\content{Schéma inverse problem with nuisance parameters}






\subsection{Classifier optimality} % (fold)
\label{sub:classifier_optimality}


The introduction of nuisance parameters, noted $\alpha$, in \autoref{eq:Fisher-Neyman} makes it impossible to use the Fisher-Neyman theorem.

\begin{equation}
	p(D|\mu, \alpha) = \underbrace{\left[ \prod_{i=1}^N p(x|B, \alpha) \right ]}_{h_\alpha(x)} \times 
       \underbrace{\left [\prod_{i=1}^N (1-\mu) + \mu \frac{p(x|S, \alpha)}{p(x|B, \alpha)} \right ]}_{g_\mu(T_\alpha(x))}
\end{equation}
Now $h_\alpha(x)$ does not depends only in $x$.


\content{Poisson likelihood is not broken but weakened}



\subsection{Variance estimations} % (fold)
\label{sub:variance_estimations}

For now the concerns were mostly the inference of the parameter.
But this is only half the answer.
The other half is the confidence interval.
The confidence interval can be divided into 2 parts.
The one comming mostly from the lack of data is called \emph{statistical variance} and the other takes origin from the sensitivity of the inference to nuisance parameter and is named \emph{systematic variance}.

Reminder of the variance definition :

$\VV(Y) = \EE[(Y - \EE[Y])^2] = \EE(Y^2) - [\EE(Y)]^2$

Law of total variance \needcite :

\begin{eqnarray}
\label{eq:total_variance_law}
    \VV[Y] =& \EE_X \left (\VV[Y|X] \right ) &+ \VV_X \left (\EE[Y|X]\right ) \\
    \VV[Y] =& \EE_X \left (\VV[Y|X] \right ) &+ \EE_X \left ( (\EE [Y|X]  - \EE[Y])^2\right )
\end{eqnarray}


Substituing with our problem the estimator $Y = \mu|x$ and the nuisance parameter $X = \alpha$ :

\begin{equation}
\label{eq:stat_and_syst_variance_definition}
\mathbb{V}[\mu|x] 
	= \underbrace{\mathbb{E}_{\alpha \sim p(\alpha|x)} \left (\mathbb{V}[\mu|x, \alpha] \right )}_{V_{stat}} 
	+ \underbrace{\mathbb{E}_{\alpha \sim p(\alpha|x)} \left ( (\mathbb{E} [\mu|x, \alpha]  - \mathbb{E}[\mu|x])^2\right )}_{V_{syst}}
\end{equation}

The variance of our estimator is splitted into :
the average of the variances of our estimator and the deviation from the average estimator induced by the nuisance parameter.







\subsection{Profiled likelihood} % (fold)
\label{sub:profiled_likelihood}

\topic{The current way of measuring the variance of our estimation to include systematics is "profiled likelihood"}

\content{Explain how profiled likelihood works and why is works}
\content{Introduce Fisher information matrix and Cramer Rao bound (since we will use it later for INFERNO)}


references : 
\begin{itemize}
	\item \url{https://arxiv.org/abs/physics/0403059}
	\item \url{https://arxiv.org/abs/1007.1727}
	\item \url{https://arxiv.org/abs/physics/0408039} coverage study of marginalization of nuisance params
\end{itemize}




\section{Summary} % (fold)
\label{sec:summary}


\subsection{Workflow} % (fold)
\label{sub:workflow}

\victor{En fait qu'est-ce que je veux dire ici ?}

In practice the data go through many processes like trigger selection, tracking, event reconstructions, etc computing hand crafted features to reduce the dimensionality from $\RR^{100 000}$ to $\RR^{40}$ then using machine learning to reduce to $\RR$.
Although machine learning is slowly becoming part of these steps \needcite.
All the domain knowledge is concentrated in the simulator and the hand crafted data processing.


\begin{figure}[htb]
    \centering
    \includegraphics[width=0.3\linewidth]{workflow}
    \caption{The workflow of maximum likelihood inference using an optimizer}
    \label{fig:workflow}
\end{figure}





\subsection{Looking for better} % (fold)
\label{sub:looking_for_better}

\topic{This document is about going further than the "simple" classifier method}

\content{Annonce de la suite / du plan du manuscrit}


