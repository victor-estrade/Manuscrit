%!TEX root = ../thesis.tex
%*******************************************************************************
%********************************** Second Chapter *****************************
%*******************************************************************************

\chapter{Context}  %Title of the First Chapter
\label{chap:context}
\ifpdf
    \graphicspath{{Chapter2/Figs/Raster/}{Chapter2/Figs/PDF/}{Chapter2/Figs/}}
\else
    \graphicspath{{Chapter2/Figs/Vector/}{Chapter2/Figs/}}
\fi

\section{Physics} % (fold)
\label{sec:physics}


\subsection{Notions de particle physics} % (fold)
\label{sec:notions_de_particle_physics}

\subsection{Notion d'ingenierie des détecteurs} % (fold)
\label{sec:notion_d_ingenierie_des_détecteurs}


\subsection{Pipeline de traitement} % (fold)
\label{sec:pipeline_de_traitement}

+ flow chart

\subsection{Définition du probleme} % (fold)
\label{sec:définition_du_probleme}

\victor{On ne connait pas les paramètres de nuisance précisément mais on a un interval/distribution}

\victor{PAR : Estimating a cross section}

We are studying a stochatic phenomenon which generative process is described as :

\begin{equation}
	\label{eq:mixture_model}
	p(x|m) = m p(x|s) + (1-m) p(x|b)
\end{equation}
where $x$ is the set of observable features of the studied event gathered in a vector.
Events are split into 2 classes : the signals $S$ and the backgrounds $B$.
$m$ is the mixture coefficient between suignals and backgrounds.

$m$ can be seen as the probability for an event to be a signal $p(s)$. 
It naturally follows that $1-m$ is the probability for an event to be a background $p(b)=1-p(s)$.
\autoref{eq:mixture_model} can be written as
\begin{equation}
	p(x) = p(s)p(x|s) + p(b)p(x|b)
\end{equation}

In many interesting cases the nature of the event (signal/background) are not among the possible measurement that can be made.
Moreover the likelihoods $p(x|s)$ and $p(x|b)$ are intractable because of high dimension integrals or simply because computable formulas do not exist.
However building a simulator working only in forward mode allowing to sample from $p(x|m)$ is possible.
This problem is known as the inverse problem. 
The objective is to infer causal parameters from observations hence reversing the forward process which goes from causal parameters to observations.


The example that motivates this work is comming from High Energy Physics (HEP) where events are collisions in a particle accelerator.
Signals are events giving birth to a Higgs boson and backgrounds gather all the other collisions.
$m$ is therefore connected to branch factor (cross section ?) of the Higgs boson which is the parameter we want to measure.

\victor{Link between cross-section, branch factor, luminosity ?}

Measurements are made from a large bunch of independant and identically distributed events $D=\{x_i\}_{i=1}^N$.

\begin{align*}
	p(D|m) =& \prod_{i=1}^N m p(x|s) + (1-m) p(x|b) \\
	       =& \prod_{i=1}^N p(x|b) \left [(1-m) + m \frac{p(x|s)}{p(x|b)} \right ]\\
	       =& \underbrace{\left[ \prod_{i=1}^N p(x|b) \right ]}_{h(x)} \times 
	       \underbrace{\left [\prod_{i=1}^N (1-m) + m \frac{p(x|s)}{p(x|b)} \right ]}_{g_m(T(x))}
\end{align*}
with $T(x) = \frac{p(x|s)}{p(x|b)} $

Fisher-Neyman factorization theorem states that $T(x)$ is a sufficient summary statistic to obtain $m$

The maximum likelihood estimator, noted $\hat m$ is commonly used to estimate the parameter of interest.
Recall that maximum likelihood estimator are strictly equivalent to a maximum a posteriori estimator using a uniform prior.
This is a reasonable choice when we do not have prior knowledge as in this example.

\begin{equation}
	\hat m = \argmax_m p(m | D)
\end{equation}

It is more convenient to express the result as a deviation from the prediction of the Standard Model.
The deviation is defined as :

\begin{equation}
	\mu = \frac{p(s)}{p_{SM}(s)} = \frac{m}{p_{SM}(s)}
\end{equation}
$p_{SM}(s)$ is the expected probability to get a signal following the Standard Model.
Recovering $m$ from $\mu$ is trivially done with $m = \mu p_{SM}(s)$.

The estimator is now :
\begin{align}
	\hmu =& \argmax_\mu p(\mu | D) \\
	     =& \argmax_\mu \frac{p(\mu)}{p(D)} p(D | \mu) \\
	     =& \argmax_\mu p(\mu) p(D | \mu) \\
	     =& \argmax_\mu  p(D | \mu) \\
	     =& \argmax_\mu  \prod_{i=1}^N g_\mu(T(x)) \\
\end{align}


$T(x)$ can be obtained using a classifier $c$ trained to separate signals and backgrounds.
A Bayes optimal classifier output gives :
\begin{equation}
	c(x) = \frac{s p(x|s)}{(1-s) p(x|b) + s p(x|s)}
\end{equation}
where $s$ is the fraction of signals used in the training dataset.

\begin{equation}
	T(x) = \frac{c(x)}{(1-c(x))} \frac{(1-s)}{s} 
\end{equation}


Note : $c$ is also a sufficient summary statistic
\begin{equation}
	g_\mu(T(x)) = 1 - \mu p_{SM}(s) + \mu p_{SM}(s) \times \frac{c(x)}{(1-c(x))} \frac{(1-s)}{s} = f_\mu(c(x))
\end{equation}


\subsection{Machine learning} % (fold)
\label{sec:machine_learning}

\subsubsection{Notations} % (fold)
\label{sub:notations}

\subsubsection{Vocabulaire} % (fold)
\label{sub:vocabulaire}

\subsubsection{Tools to solve this problem} % (fold)
\label{sub:tools_to_solve_this_problem}


\subsubsection{State of the art on this problem} % (fold)
\label{sub:state_of_the_art_on_this_problem}



\victor{state of the art on how (physicist ?) to solve the problem}

\victor{TODO : Décrire succintement rejection algo et les idées principales d'amélioration. }

One solution to do inference with an intractable likelihood is the Approximate Bayesian Computation (ABC).
The most traditional ABC algorithm is the rejection algorithm which is computationally expensive.
Although significant improvement can be achieved with advanced Monte Carlo methods this is still known to take a long time to converge.

\victor{TODO : VI avec des NN. Stochastic VI.}
On the other hand Variational Inference (VI) is extremely fast but can lead to biased estimation which is a severe drawback when it leads to underestimating the uncertainty.

In high Energy Physics (HEP), which motivated this work, the use of machine learning produced summary statistics combined with domain knowledge allow to conduct exact inference in the absence of nuisance parameters.
\victor{"Extact" inference me semble maladroit}

\victor{TODO : Another improvement is Mining gold.}
\victor{TODO : Bayesian networks}



\emph{Before Machine learning}

How it was done before machine learning

\emph{With machine learning}

How it is done with machine learning



