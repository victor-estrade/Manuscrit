%!TEX root = ../thesis.tex
%*******************************************************************************
%********************************** Second Chapter *****************************
%*******************************************************************************

\chapter{Context}  %Title of the First Chapter
\label{chap:context}
\ifpdf
    \graphicspath{{Chapter2/Figs/Raster/}{Chapter2/Figs/PDF/}{Chapter2/Figs/}}
\else
    \graphicspath{{Chapter2/Figs/Vector/}{Chapter2/Figs/}}
\fi

\section{Physics} % (fold)
\label{sec:physics}


\subsection{Notions de particle physics} % (fold)
\label{sec:notions_de_particle_physics}

\subsection{Notion d'ingenierie des détecteurs} % (fold)
\label{sec:notion_d_ingenierie_des_détecteurs}


\subsection{Pipeline de traitement} % (fold)
\label{sec:pipeline_de_traitement}

+ flow chart

\subsection{Définition du probleme} % (fold)
\label{sec:définition_du_probleme}

On ne connait pas les paramètres de nuisance précisément
mais on a un interval/distribution  


\subsection{Machine learning} % (fold)
\label{sec:machine_learning}

\subsubsection{Notations} % (fold)
\label{sub:notations}

\subsubsection{Vocabulaire} % (fold)
\label{sub:vocabulaire}

\subsubsection{Tools to solve this problem} % (fold)
\label{sub:tools_to_solve_this_problem}


\subsubsection{State of the art on this problem} % (fold)
\label{sub:state_of_the_art_on_this_problem}


state of the art on how physics solve the problem


\emph{Before Machine learning}

How it was done before machine learning

\emph{With machine learning}

How it is done with machine learning



\section{OLD} % (fold)

\section{Systematics and nuisance parameters} % (fold)
\label{sec:systematics_and_nuisance_parameters}

% section systematics_and_nuisance_parameters (end)
\section{Measuring uncertainties} % (fold)
\label{sec:measuring_uncertainties}

% section measuring_uncertainties (end)
\section{ Performance measuring }
\label{sec:performance_measuring}

Many methods to estimate the parameter of interest and its variance are available.
If changing the set of hyper parameter for the learning procedure is considered as changing the method then countless methods are to be evaluated.
Automating the measure of the performances of a proposed method is crucial to select the best method.
In this section is described a simple but general procedure to measure the performances of a given method.

The usual criterions to evaluate an estimator $\htheta$ are the bias, the variance and the mean squared error defined as follow :
\begin{equation}
	Bias(\htheta) = \EE[\htheta] - \thetas
\end{equation}
\begin{equation}
	Var(\htheta) = \EE[ (\htheta - \EE[\htheta])^2 ] = \EE[\htheta^2] - (\EE[\htheta])^2
\end{equation}
\begin{equation}
	MSE(\htheta) = \EE[(\htheta - \thetas)^2] = Var(\htheta) + [Bias(\htheta)]^2
\end{equation}

To evaluate these criterion we need to repeat the experiement $N$ times leading to many estimation of the parameters $\hmu^{(k)}$ and $\hshmu^{(k)}$.
Repeating the experiment can be done through cross-validation methods.


\subsection{Evaluation of the parameter of interest estimator} % (fold)
\label{sub:evaluation_of_the_parameter_of_interest_estimator}

First, let's focus on evaluating the estimator of the parameter of insterest $\hmu$.
The true value of $\mu$, noted $\mus$, is available during tests since it is an input of the simulator.

From the estimation of its expected value
\begin{equation}
	\EE[\hmu] \approx <\hmu^{(k)}>_k = \frac{1}{N} \sum_{k} \hmu^{(k)}
\end{equation}
it is possible to estimated the criterions

\begin{equation}
	Bias(\hmu) \approx <\hmu^{(k)}>_k - \mus
\end{equation}
\begin{equation}
	\label{eq:var_hmu}
	Var(\hmu) \approx <\hmu^{(k)} \times \hmu^{(k)}>_k - (<\hmu^{(k)}>_k)^2
\end{equation}
\begin{equation}
	MSE(\hmu) = Var(\hmu) + [Bias(\hmu)]^2
\end{equation}

% subsection evaluation_of_the_parameter_of_interest_estimator (end)
\subsection{Evaluation of the variance estimator} % (fold)
\label{sub:evaluation_of_the_variance_estimator}

The evaluation of the variance estimator $\hshmu$ could be done in the same way if the true variance $Var(\hmu)$ can be computed.
If this is not the case an approximation is available using \autoref{eq:var_hmu}.

\begin{equation}
	Bias(\hshmu) \approx <\hshmu^{(k)}>_k - Var(\hmu)
\end{equation}
\begin{equation}
	Var(\hshmu) \approx <\hshmu^{(k)} \times \hshmu^{(k)}>_k - (<\hshmu^{(k)}>_k)^2
\end{equation}
\begin{equation}
	MSE(\hmu) = Var(\hshmu) + [Bias(\hshmu)]^2
\end{equation}

% subsection evaluation_of_the_variance_estimator (end)

