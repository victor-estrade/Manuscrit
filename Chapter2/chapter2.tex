%!TEX root = ../thesis.tex
%*******************************************************************************
%********************************** Second Chapter *****************************
%*******************************************************************************

\chapter{Systematic aware learning}  %Title of the First Chapter
\label{chap:sota}

\ifpdf
    \graphicspath{{Chapter2/Figs/Raster/}{Chapter2/Figs/PDF/}{Chapter2/Figs/}}
\else
    \graphicspath{{Chapter2/Figs/Vector/}{Chapter2/Figs/}}
\fi



\url{https://hal.archives-ouvertes.fr/hal-01665925}


\url{https://hal.archives-ouvertes.fr/hal-01715155}
\url{https://hal.archives-ouvertes.fr/hal-02403581}
\url{}



\section{Domain adaptation} % (fold)
\label{sec:domain_adaptation}

\topic{If the classifier output is independant of the nuisance parameters then we win !}

\subsection{Why it should work} % (fold)
\label{sub:why_it_should_work}

\topic{If the classifier output is independant of the nuisance parameters then we win !}
\topic{This is a very peculiar case of domain adaptation}
\content{We have a continuous domain shift. Not a discreat number of domains. + we are supervised}






\subsection{Data augmentation} % (fold)
\label{sub:data_augmentation}

\topic{The easiest (and naive) way of being good across domain}



\textbf{À chaque fois : présenter la méthode et presenter ses success stories et ses +/- et un dessin}




\subsection{Regularization} % (fold)
\label{sub:regularization}

\topic{Regularizing the network to become less sensitive to nuisance.}
\content{Tangent Prop}


\textbf{À chaque fois : présenter la méthode et presenter ses success stories et ses +/- et un dessin}





\subsection{Adversarial training} % (fold)
\label{sub:adversarial_training}

\content{DANN and Pivot}
\content{Make clear that Pivot is better founded (theory) than DANN, the bottleneck at the end is a good idea (says Maths)}


\textbf{À chaque fois : présenter la méthode et presenter ses success stories et ses +/- et un dessin}




\subsection{Cross-entropy} % (fold)
\label{sub:cross_entropy}

\topic{Is the cross-entropy the right objective function ?}
Recall that the optimality of the Bayesian classifier is no more.
Let's not stay stuck with classification then.









\section{Soft histograms} % (fold)
\label{sec:soft_histograms}

\topic{Only histograms prevent the entire pipeline to be differentiable. Fix it and you win !}

\content{INFERNO and other soft histograms + Inferted Fisher info matrix as objective function}

Two methods (\cite{DECASTRO2019170inferno} and \needcite(l'autre histogram moins bien)) have been proposed for tackling this problem without relying on the cross-entropy.
\victor{Ou alors 2 methods : Mining gold + INFERNO ?}

This section's main focus will be on the first one, INFERence aware Neural Optimization (INFERNO).
The second one is very similar (see \autoref{sub:differentiable_histograms} for details).





\subsection{Differentiable histograms} % (fold)
\label{sub:differentiable_histograms}

\topic{Using softmax or approximating gradients makes the histograms differentiable}








\subsection{Differentiable simulator} % (fold)
\label{sub:differentiable_simulator}

\topic{It is possible to make the simulator differentiable according to the nuisance parameters}

\content{Fast simulation through recomputing only the skewed features according to nuisance parameter}






\subsection{Inference aware neural network} % (fold)
\label{sub:inference_aware_neural_network}



Note : The fisher information matrix is an expectancy but we estimate it with only 1 point !  And it works !




\subsection{Limitations} % (fold)
\label{sub:limitations}

\topic{if you can make siluation differentiable enough and invert the fisher info matrix)}



Instable training : NaN gradients skipped, Nan outputs regenerate to previous checkpoint.




\section{Other methods} % (fold)
\label{sec:other_methods}

\topic{Variational inference requires likelihood and ABC does not scale.}
\content{What we are not going to devellop. But justifying why !}







\subsection{ABC methods} % (fold)
\label{sub:abc_methods}

\content{short review of ABC methods and why we cannot rely on it for this problem.}

\victor{TODO : Décrire succintement rejection algo et les idées principales d'amélioration. }

One solution to do inference with an intractable likelihood is the Approximate Bayesian Computation (ABC).
The most traditional ABC algorithm is the rejection algorithm which is computationally expensive.
Although significant improvement can be achieved with advanced Monte Carlo methods this is still known to take a long time to converge \needcite.






\subsection{Variational inference} % (fold)
\label{sub:variational_inference}

\topic{Variational inference requires likelihood}

\victor{TODO : VI avec des NN. Stochastic VI.}
On the other hand Variational Inference (VI) is extremely fast but can lead to biased estimation which is a severe drawback when it leads to underestimating the uncertainty.

In high Energy Physics (HEP), which motivated this work, the use of machine learning produced summary statistics combined with domain knowledge allow to conduct exact inference in the absence of nuisance parameters.
\victor{"Extact" inference me semble maladroit}


\cecile{Cf chapitre de la thèse d'Adrian sur l'inférence variationnelle}







\subsection{Bayesian networks} % (fold)
\label{sub:bayesian_networks}

\topic{Could be a solution. But no time to explore everything}
\content{Personne n'aime les Bayesian networks. Il faudrait exprimer pourquoi.}
\victor{Other physicists have tried and got prelimnary results. Need to see if they made progress !}







\subsection{Disentangling} % (fold)
\label{sub:disentangling}

\content{Why not using disentangling ?}








\subsection{Mining gold from simulator} % (fold)
\label{sub:mining_gold_from_simulator}

% subsection mining_gold_from_simulator (end)

\topic{Improve the simulator to give more informations about the intractable likelihood !}

\content{Can't do it for now because I don't have access to the simulator}
