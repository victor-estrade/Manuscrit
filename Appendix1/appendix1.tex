%!TEX root = ../thesis.tex
% ******************************* Thesis Appendix A ****************************
\ifpdf
    \graphicspath{{Appendix1/Figs/Raster/}{Appendix1/Figs/PDF/}{Appendix1/Figs/}}
\else
    \graphicspath{{Appendix1/Figs/Vector/}{Appendix1/Figs/}}
\fi

\chapter{Experimental details} 

List of main experiments : 
\begin{itemize}
	\item Measure \emph{performances} for each \emph{method}
	\item Compare performances between \emph{robust} method
	\item Show that the balancing of S and B is critical
	\item Reproduce the impossibility of separating \emph{domains} on Higgs
\end{itemize}



\section{What distinguish runs} % (fold)
\label{sec:what_distinguish_runs}

There is several way of estimating the parameter of interest.
These ways may differ from each other because of the pipeline, classifier training, architecture, hyper-parameters and cross-validation.
The experiments are organized as follow.

A \textbf{run} is a combinason of a \emph{problem}, a \emph{method}, a \emph{calibration}, a set of \emph{hyper-parameters}, a \emph{cross-validation} seed, and a set of \emph{true parameter}.

A \textbf{problem} is a generative process with a mixture coefficient to be infered.
Problems are assumed to be completelly determined by a set of input parameters including the parameter of interest.
The parameter of interest is the mixture coefficient or a quantity link to it by a bijective function.
List :
\begin{itemize}
	\item Gamma-Gauss [GG] : a 1D toy
	\item Synthetic 3D data [S3D2] : a replication of the 3D toy in the inferno paper
	\item Higgs ML data [HIGGS] : the data from higgsML challenge + the re-simulator
\end{itemize}

A \textbf{dataset} is a finite sampled realization of problem given some \textbf{input parameters}.
The \textbf{nominal data} is a dataset produced with the most probable input parameters according to our prior knowledge.


A \textbf{method} is represented by a training algorithm and a pipeline.
Actually once the training algorithm is chosen there is only one pipeline that fit the produced model.
Indeed classifiers can only use the classic worklow with maximum likelihood.

List of every methods :
\begin{itemize}
	\item Gradient Boosting [GB] : a gradient boosting classifier from scikit-learn trained on the nominal data.
	\item Neural Network Classifier [NN] : a neural network trained as a classifier trained on the nominal data.
	\item Data augmentation [DA] : a neural network classifier trained on a mixture of simulated data with 1000 differents input parameters.
	\item Tangent Propagation [TP] : a neural network classifier with the tangent propagation regularization.
	\item Pivotal neural network [Pivot] : a neural network classifier with adversarial training.
	\item Inferno [INFERNO] : a neural network summary computer trained with inferno method.
	\item Regressor [REG] : a dataset wise mixture density neural network regressor taking both the data and nuisance parameter as input.
	\item Marginal Regressor [REG-Marginal] : a dataset wise mixture density neural network regressor taking only the data as input.
\end{itemize}

On a second level is the \textbf{calibration} restriction.
All method at some points requires information about the nuisance parameters.
The regressor requires to sample the nuisance parameters from a distribution for marginalization.
The optimizer requires initial values and uncertainty for all parameters and the likelihood also include constrains on the nuisance parameters.
The experiments test two calibration : the given prior knowledge and the estimation of the nuisance parameters on the test data using previously trained regressors.
Method names are decorated with "-Prior" or "-Calib" (exm : GB-Prior, REG-Calib). The marginal regressor is not concerned since it is not using calibration at all.
More realistic calibrations are left for future work.

On a third level is the \textbf{hyper-parameters} and \textbf{architecture} of the models.
The hyper-parameter setting is quite straitforward : a grid search on the most relevant hyper-parameters is done.
The resolution of the grid is quite low to avoid the need of expensive computing ressources.
The architecture choice is more complicated since there is an infinite way of desining one.
It is devellopped in a later section\needcite.

The experiments are reapeated several times to accuratelly measure the variance of the performances according to data sampling.
This is called \textbf{cross-validation} in this document although it is not allways a crossing of bunches of the same dataset as in classic cross-validation practices.
Three datasets are used in the process.
The \emph{training} set is the data used to train our model.
The \emph{validation} set is the data used to produce summary statistics (is it the azimov dataset ?).
The \emph{test} set is the data on which inference is done.

On the toy problems the generative process is cheap.
Toy datasets are generated as required and a change of the \emph{seed} is creating new train, valid and test sets.
On the higgsML data the data is first randomly splited into 3 sets see \autoref{sub:fast_re_simulator} for the details.

The final level is the choice of the \textbf{true parameters} used to generate the test set.
A combinatorial grid of possible parameters is the simplest choice to study their influence on the performances.
Again to avoid too expensive computation due to combinatorial explosion the true values of the nuisance parameters takes only 3 values :
the nominal - deviation, nominal and nominal + deviation.
The parameter of interest may be given more values.
Finally the number of samples in the test set is also evolving to measure its impact on the quality of the inference.



\section{Measured quantities for performance evaluation} % (fold)
\label{sec:measured_quantities_for_performance_evaluation}


Here is given more details than in \autoref{sec:evaluation_metric}.
\victor{Donc il faudra probablement en injecter une bonne partie dans \autoref{sec:evaluation_metric} à terme.}
For each set of \emph{true parameters} several performances metrics are measured.

The main performance evaluation is the \emph{mean squared error} on the two estimated quantities : the parameter of interest and its confidence interval.
There is two sources of randomness in the estimated values.
The first one is the variance of the nuisance parameters which all methods is supposed to handle and include in its estimated a variance or a confidence interval of the estimated parameter of interest $\hshmu$.
The second is the data sampling which is empirically measured by repeating the experiment with cross-validation.
Altough the given variance/CL $\hshmu$ is assumed to be correct the variance of $\hmu$ is measured for comparison.
\victor{Oui mais du coup ces 2 quantités ne mesure pas vraiment la même chose, si ?}


Recall that :
\begin{eqnarray}
\label{eq:total_variance_law}
    \VV[Y] =& \EE_X \left (\VV[Y|X] \right ) &+ \VV_X \left (\EE[Y|X]\right ) \\
    \VV[Y] =& \EE_X \left (\VV[Y|X] \right ) &+ \EE_X \left ( (\EE [Y|X]  - \EE[Y])^2\right )
\end{eqnarray}


\begin{equation}
\label{eq:stat_and_syst_variance_definition_2}
\mathbb{V}[\hat \mu] 
	= \underbrace{\mathbb{E}_{\alpha \sim p(\alpha)} \left (\mathbb{V}[\hat \mu, \alpha] \right )}_{V_{stat}} 
	+ \underbrace{\mathbb{E}_{\alpha \sim p(\alpha)} \left ( (\mathbb{E} [\hat \mu, \alpha]  - \mathbb{E}[\hat \mu])^2\right )}_{V_{syst}}
\end{equation}
Or :
\begin{equation}
\label{eq:stat_and_syst_variance_definition_3}
\mathbb{V}[\mu|x] 
	= \underbrace{\mathbb{E}_{\alpha \sim p(\alpha|x)} \left (\mathbb{V}[\mu|x, \alpha] \right )}_{V_{stat}} 
	+ \underbrace{\mathbb{E}_{\alpha \sim p(\alpha|x)} \left ( (\mathbb{E} [\mu|x, \alpha]  - \mathbb{E}[\mu|x])^2\right )}_{V_{syst}}
\end{equation}

Et puis il y a ça aussi :
\begin{align}
\label{eq:stat_and_syst_variance_definition_4}
\mathbb{V}_{D^\star\sim p(D)}[\hat \mu | D^\star] 
 & = \underbrace{\mathbb{E}_{\alpha \sim p(\alpha|D^\star)} \left (\mathbb{V}_{D^\star\sim p(D)}[\hat \mu | D^\star, \alpha] \right )}_{V_{stat}} \\
 & ~ + \underbrace{\mathbb{E}_{\alpha \sim p(\alpha|D^\star)} \left ( (\mathbb{E}_{D^\star\sim p(D)} [\hat \mu | D^\star, \alpha]  - \mathbb{E}_{D^\star\sim p(D)}[\hat \mu | D^\star])^2\right )}_{V_{syst}}
\end{align}

\victor{Voilà c'est là que j'ai un doute. Ces 2 equations sont bien évidement correctes.  Mais laquelle correspond à notre problème ?! Du coup j'ai gardé \autoref{eq:stat_and_syst_variance_definition_2}.}


\content{Var stat and Var systs.}

\content{Number of call for MLE but also for the network at inference.}

\content{}




\section{Details} % (fold)
\label{sec:details}

Here are given the details to be able to reproduce the experiments.

The chosen global seed is 42.
This number has become famous ammong the computer science community as H2G2\needcite became more popular.
I've seen it used quite often in codes as an easter egg.




\subsection{Baselines} % (fold)
\label{sub:baselines}






\subsection{Neural network architectures} % (fold)
\label{sub:neural_network_architectures}

\content{How the architectures were chosen}
\content{One very simple + 3 sizes of Residual network because it can ignore useless parts}
\content{Equal number of neurons in all layer. Grid. But always many neurons !!}







\subsection{Hyper-parameter grid} % (fold)
\label{sub:hyper_parameter_grid}

\content{Grid search on which HP}

\content{Donner la grille pour chaque modèle et expliquer pourquoi ces HP sont choisis.}







