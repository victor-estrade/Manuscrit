%!TEX root = ../thesis.tex
% ******************************* Thesis Appendix A ****************************
\ifpdf
    \graphicspath{{Appendix1/Figs/Raster/}{Appendix1/Figs/PDF/}{Appendix1/Figs/}}
\else
    \graphicspath{{Appendix1/Figs/Vector/}{Appendix1/Figs/}}
\fi

\chapter{Experimental details} 

List of main experiments : 
\begin{itemize}
	\item Measure \emph{performances} for each \emph{method}
	\item Compare performances between \emph{robust} method
	\item Show that the balancing of S and B is critical
	\item Reproduce the impossibility of separating \emph{domains} on Higgs
\end{itemize}



\section{What distinguish runs} % (fold)
\label{sec:what_distinguish_runs}

There is several way of estimating the parameter of interest.
These ways may differ from each other because of the pipeline, classifier training, architecture, hyper-parameters and cross-validation.
The experiments are organized as follow.

A \textbf{run} is a combinason of a \emph{problem}, a \emph{method}, a \emph{calibration}, a set of \emph{hyper-parameters}, a \emph{cross-validation} seed, and a set of \emph{true parameter}.

A \textbf{problem} is a generative process with a mixture coefficient to be infered.
Problems are assumed to be completelly determined by a set of input parameters including the parameter of interest.
The parameter of interest is the mixture coefficient or a quantity link to it by a bijective function.
List :
\begin{itemize}
	\item Gamma-Gauss [GG] : a 1D toy
	\item Synthetic 3D data [S3D2] : a replication of the 3D toy in the inferno paper
	\item Higgs ML data [HIGGS] : the data from higgsML challenge + the re-simulator
\end{itemize}

A \textbf{dataset} is a finite sampled realization of problem given some \textbf{input parameters}.
The \textbf{nominal data} is a dataset produced with the most probable input parameters according to our prior knowledge.


A \textbf{method} is represented by a training algorithm and a pipeline.
Actually once the training algorithm is chosen there is only one pipeline that fit the produced model.
Indeed classifiers can only use the classic worklow with maximum likelihood.

List of every methods :
\begin{itemize}
	\item Gradient Boosting [GB] : a gradient boosting classifier from scikit-learn trained on the nominal data.
	\item Neural Network Classifier [NN] : a neural network trained as a classifier trained on the nominal data.
	\item Data augmentation [DA] : a neural network classifier trained on a mixture of simulated data with 1000 differents input parameters.
	\item Tangent Propagation [TP] : a neural network classifier with the tangent propagation regularization.
	\item Pivotal neural network [Pivot] : a neural network classifier with adversarial training.
	\item Inferno [INFERNO] : a neural network summary computer trained with inferno method.
	\item Regressor [REG] : a dataset wise mixture density neural network regressor taking both the data and nuisance parameter as input.
	\item Marginal Regressor [REG-Marginal] : a dataset wise mixture density neural network regressor taking only the data as input.
\end{itemize}

On a second level is the \textbf{calibration} restriction.
All method at some points requires information about the nuisance parameters.
The regressor requires to sample the nuisance parameters from a distribution for marginalization.
The optimizer requires initial values and uncertainty for all parameters and the likelihood also include constrains on the nuisance parameters.
The experiments test two calibration : the given prior knowledge and the estimation of the nuisance parameters on the test data using previously trained regressors.
Method names are decorated with "-Prior" or "-Calib" (exm : GB-Prior, REG-Calib). The marginal regressor is not concerned since it is not using calibration at all.
More realistic calibrations are left for future work.

On a third level is the \textbf{hyper-parameters} and \textbf{architecture} of the models.
The hyper-parameter setting is quite straitforward : a grid search on the most relevant hyper-parameters is done.
The resolution of the grid is quite low to avoid the need of expensive computing ressources.
The architecture choice is more complicated since there is an infinite way of desining one.
It is devellopped in a later section\needcite.

The experiments are reapeated several times to accuratelly measure the variance of the performances according to data sampling.
This is called \textbf{cross-validation} in this document although it is not allways a crossing of bunches of the same dataset as in classic cross-validation practices.
Three datasets are used in the process.
The \emph{training} set is the data used to train our model.
The \emph{validation} set is the data used to produce summary statistics (is it the azimov dataset ?).
The \emph{test} set is the data on which inference is done.

On the toy problems the generative process is cheap.
Toy datasets are generated as required and a change of the \emph{seed} is creating new train, valid and test sets.
On the higgsML data the data is first randomly splited into 3 sets see \autoref{sub:fast_re_simulator} for the details.

The final level is the choice of the \textbf{true parameters} used to generate the test set.
A combinatorial grid of possible parameters is the simplest choice to study their influence on the performances.
Again to avoid too expensive computation due to combinatorial explosion the true values of the nuisance parameters takes only 3 values :
the nominal - deviation, nominal and nominal + deviation.
The parameter of interest may be given more values.
Finally the number of samples in the test set is also evolving to measure its impact on the quality of the inference.

\victor{Ecrire exactement la variabilité restante : le train set, le test set et l'initialisation des NN}

\section{Measured quantities for performance evaluation} % (fold)
\label{sec:measured_quantities_for_performance_evaluation}


Here is given more details than in \autoref{sec:evaluation_metric}.
\victor{Donc il faudra probablement en injecter une bonne partie dans \autoref{sec:evaluation_metric} à terme.}
For each set of \emph{true parameters} several performances metrics are measured.
\victor{True parameters : donner la définition des paramètres ie $\mu$ et $\alpha$}

The main performance evaluation is the \emph{mean squared error} on the two estimated quantities : the parameter of interest and its confidence interval.
\victor{Définition de confidence interval comme interval centré (50\% à droite et à gauche)}
There is two sources of randomness in the estimated values.
The first one is the variance of the nuisance parameters which all methods is supposed to handle and include in its estimated a variance or a confidence interval of the estimated parameter of interest $\hshmu$.
The second is the data sampling which is empirically measured by repeating the experiment with cross-validation.
Altough the given variance/CL $\hshmu$ is assumed to be correct the variance of $\hmu$ is measured for comparison.
\victor{Oui mais du coup ces 2 quantités ne mesure pas vraiment la même chose, si ?}


Recall that :
\begin{eqnarray}
\label{eq:total_variance_law_2}
    \VV[Y] =& \EE_X \left (\VV[Y|X] \right ) &+ \VV_X \left (\EE[Y|X]\right ) \\
    \VV[Y] =& \EE_X \left (\VV[Y|X] \right ) &+ \EE_X \left ( (\EE [Y|X]  - \EE[Y])^2\right )
\end{eqnarray}
Giving :
\begin{equation}
\label{eq:stat_and_syst_variance_definition_2}
\mathbb{V}[\hat \mu] 
	= \underbrace{\mathbb{E}_{\alpha \sim p(\alpha)} \left (\mathbb{V}[\hat \mu, \alpha] \right )}_{V_{stat}} 
	+ \underbrace{\mathbb{V}_{\alpha \sim p(\alpha)} \left ( \mathbb{E} [\hat \mu, \alpha] \right )}_{V_{syst}}
\end{equation}
\victor{Voilà c'est là que j'ai un doute. Mon interrogation est sur la distribution de $\alpha$, on prend $p(\alpha | D)$ ou bien $p(\alpha)$ ? }
The variance can be splitted between the part coming from the nuisance parameter and the part coming from the data sampling.
The inference done asuming the value of $\alpha$ is known with $\alpha$ varying along a regular grid.
To see the influence of nuisance parameters on the inference but also to compute $V_{stat}$ and $V_{syst}$ assuming a uniform prior on $\alpha$.

Moreover we expect $V_{stat}$ to decerase in $\mathcal O \frac{1}{\sqrt{\#samples}}$ and $V_{syst}$ to be quite independant from $\#samples$.

The number predictions made by the model is also reported as a proxy of computation cost.















\section{Details} % (fold)
\label{sec:details}

Here are given the details to be able to reproduce the experiments.

\subsection{Global constants} % (fold)
\label{sub:global_constants}

The chosen global seed is 42.
This number has become famous with the popularity of H2G2\needcite.
I've seen it used quite often in codes as an easter egg.


The number of cross-validation is always 30.



\subsection{List of methods} % (fold)
\label{sub:list_of_methods}


Here is the list of all methods with a short description and their short names.
\begin{itemize}
	\item Gradient Boosting [GB] : a gradient boosting classifier from scikit-learn trained on the nominal data.
	\item Neural Network Classifier [NN] : a neural network trained as a classifier trained on the nominal data.
	\item Data augmentation [DA] : a neural network classifier trained on a mixture of simulated data with 1000 differents input parameters.
	\item Tangent Propagation [TP] : a neural network classifier with the tangent propagation regularization.
	\item Pivotal neural network [Pivot] : a neural network classifier with adversarial training.
	\item Inferno [INFERNO] : a neural network summary computer trained with inferno method.
	\item Regressor [REG] : a dataset wise mixture density neural network regressor taking both the data and nuisance parameter as input.
	\item Marginal Regressor [REG-Marginal] : a dataset wise mixture density neural network regressor taking only the data as input.
\end{itemize}



\subsection{Neural network architectures} % (fold)
\label{sub:neural_network_architectures}


The choice of neural network architecture is a critical hyper-parameter but also the most difficult to tune because of its complexity.
Automatization of neural network architecture search is still an open problem \needcite.
Although better architectures will leads to better performances, neural network architecture is not the focus of this study.
To give a fair comparison between methods at an affordable computation cost the chosen architectures are splited into 3 classes.

\emph{Small} architectures to give base performances of maybe undersized networks.
\emph{Large} architectures are expected to provide good if not best performances.
\emph{Huge} architecture which are deliberately over-sized to make sure that bigger won't be better.


Each layers have the exact same amount of neurons which is the only hyper-parameter of the architecture.
In hope to better limit the inlfuence of the architecture on the performances residual conections are used in large network.
Networks are built using simple blocs :
\begin{itemize}
	\item \textbf{Linear} layers are simple neural networks $o = W.x + b$
	\item \textbf{Residual} blocs are 2 simple layers with an intermediate space half the size of the input and separated with an activation. $o = x + W_2 . a(W_1.x + b)$
	\item \textbf{Average} layers are described \autoref{sub:datasetwise_input} $o = U.X+b+V.r(X)$
	\item \textbf{Residual Average} blocs are 2 average layers with an intermediate space half the size of the input and separated with an activation. $o^\prime = a( U_1.X+b+V_1.r(X) )$ and $o = x + U_2.o^\prime + V_2.r(o^\prime) $
\end{itemize}
Making the intermediate space half the size of the input in residual blocs is assumed to encourage the network to learn simpler functions.
This way the residual blocs are learning a simple correction of the input.
Chaining these corrections should allow the network to produce arbitrarily complex functions but encourage it toward ones close to a linear function.

All architectures follow a simple code :
\begin{itemize}
	\item L = Linear
	\item R = Residual block
	\item M = Mean operation (reduction)
	\item A = Average layer
	\item AR = Average Residual block
\end{itemize}
Followed by a number representing the number of layers.
Since residual blocs includes 2 layers a single residual bloc is always followed by a even number representing twice the number of blocs.

The classic architectures is used for classifiers
\begin{itemize}
	\item \textbf{L4} [small] : simple feed forward neural network with 4 layers with ReLU activations.
	\item \textbf{L1R8L1} [large] : a linear layer followed by 4 residual blocs and a final linear layer.
	\item \textbf{L1R20L1} [huge] : a linear layer followed by 10 residual blocs and a final linear layer.
\end{itemize}

The average architectures is an addition to INFERNO.
INFERNO training can leverage the datasetwise architecture so additional architectures were included to see what happens.
\begin{itemize}
	\item \textbf{L1A2L1} [small] : a linear layer followed by 2 average layers and a final linear layer.
	\item \textbf{A1AR8L1} [large] : an average layer followed by 4 average residual blocs and a final linear layer.
	\item \textbf{A1AR20L1} [huge] : an average layer followed by 10 average residual blocs and a final linear layer.
\end{itemize}

The regressor architectures are a bit bigger because the learned functions are expected to be more complex.
\begin{itemize}
	\item \textbf{A3ML3} [small] : 3 average layers an average reduction and 3 linear layers.
	\item \textbf{A1AR2ML1} [small] : an average layer followed by 1 average residual bloc, an average reduction and a final linear layer.
	\item \textbf{A1AR8MR8L1} [large] : an average layer followed by 4 average residual blocs then an average reduction followed by 4 residual blocs and a final linear layer.
	\item \textbf{A1AR18MR4L1} [huge] : an average layer followed by 9 average residual blocs, an average reduction followed by 2 residual blocs and a final linear layer.
\end{itemize}

Finaly the choice of activation determines the smoothness of a network.
Only relying on ReLU will likely end in a piecewise linear function while a sigmoid of tanh activation may lead more curves.
Morover ReLU is never saturating which led to some gradient explosion is preliminary experiments. 
To avoid having to test many activations the default activation is applying ReLU6 to the first half of the neurons and tanh to the rest of the neurons.
This is assumed to allow the network to use the best of both world.






\subsection{Hyper-parameter grid} % (fold)
\label{sub:hyper_parameter_grid}

\content{Donner la grille pour chaque modèle et \textbf{expliquer pourquoi ces HP sont choisis}}


\begin{table}[ht!]
\centering
\begin{tabular}{||c c c c||} 
 \hline
 \multicolumn{4}{|c|}{Gradient boosting}\\
 \hline
 Problem & GG & S3D2 & HiggsML \\ [0.5ex] 
 \hline
 \# estimators & [100, 300, 1000] & [100, 300, 1000] & [500, 1000, 2000] \\ 
 max-depth     & [3, 5, 10] & [3, 5, 10] & [3, 5, 10] \\
 learning-rate & [0.1, 0.05, 0.01] & [0.1, 0.05, 0.01] & [0.1, 0.05, 0.01] \\
 \hline
\end{tabular}
\caption{Table of hyper-parameters for Gradient boosting}
\label{table:HP_GB}
\end{table}


\begin{table}[ht!]
\centering
\begin{tabular}{||c c c c||} 
 \hline
 \multicolumn{4}{|c|}{Neural network classifier}\\
 \hline
 Problem & GG & S3D2 & HiggsML \\ [0.5ex] 
 \hline
 \# units & [50, 100, 200, 500] & [50, 100, 200, 500] & [50, 100, 200, 500] \\ 
 \# steps & [2000, 5000] & [2000, 5000] & [2000, 5000, 10 000] \\
 learning-rate & [1e-3] & [1e-3] & [1e-3] \\
 Optimizer & Adam & Adam & Adam \\
 $\beta_1$ & 0.9 & 0.9 & 0.9 \\
 $\beta_2$ & 0.999 & 0.999 & 0.999 \\
 \hline
\end{tabular}
\caption{Table of hyper-parameters for Neural network classifier}
\label{table:HP_NN}
\end{table}


\begin{table}[ht!]
\centering
\begin{tabular}{||c c c c||} 
 \hline
 \multicolumn{4}{|c|}{Data Augmentation}\\
 \hline
 Problem & GG & S3D2 & HiggsML \\ [0.5ex] 
 \hline
 \# units & [50, 100, 200, 500] & [50, 100, 200, 500] & [50, 100, 200, 500] \\ 
 \# steps & [2000, 5000] & [2000, 5000] & [2000, 5000, 10 000] \\
 learning-rate & [1e-3] & [1e-3] & [1e-3] \\
 Optimizer & Adam & Adam & Adam \\
 $\beta_1$ & 0.9 & 0.9 & 0.9 \\
 $\beta_2$ & 0.999 & 0.999 & 0.999 \\
 \hline
\end{tabular}
\caption{Table of hyper-parameters for Data Augmentation}
\label{table:HP_DA}
\end{table}


\begin{table}[ht!]
\centering
\begin{tabular}{||c c c c||} 
 \hline
 \multicolumn{4}{|c|}{Tangent propagation}\\
 \hline
 Problem & GG & S3D2 & HiggsML \\ [0.5ex] 
 \hline
 \# units & [50, 100, 200, 500] & [50, 100, 200, 500] & [50, 100, 200, 500] \\ 
 \# steps & [2000, 5000] & [2000, 5000] & [2000, 5000, 10 000] \\
 learning-rate & [1e-3] & [1e-3] & [1e-3] \\
 Optimizer & Adam & Adam & Adam \\
 $\beta_1$ & 0.9 & 0.9 & 0.9 \\
 $\beta_2$ & 0.999 & 0.999 & 0.999 \\
 trade-off & [1, 0.1, 1e-2, 1e-3] & [1, 0.1, 1e-2, 1e-3] & [1, 0.1, 1e-2, 1e-3] \\
 \hline
\end{tabular}
\caption{Table of hyper-parameters for Tangent Propagation}
\label{table:HP_TP}
\end{table}


\begin{table}[ht!]
\centering
\begin{tabular}{||c c c c||} 
 \hline
 \multicolumn{4}{|c|}{Pivot}\\
 \hline
 Problem & GG & S3D2 & HiggsML \\ [0.5ex] 
 \hline
 \# units & [50, 100, 200, 500] & [50, 100, 200, 500] & [50, 100, 200, 500] \\ 
 \# steps & [2000, 5000] & [2000, 5000] & [2000, 5000, 10 000] \\
 learning-rate & [1e-3] & [1e-3] & [1e-3] \\
 Optimizer & Adam & Adam & Adam \\
 $\beta_1$ & 0.9 & 0.9 & 0.9 \\
 $\beta_2$ & 0.999 & 0.999 & 0.999 \\
 trade-off & [1, 0.1, 1e-2, 1e-3] & [1, 0.1, 1e-2, 1e-3] & [1, 0.1, 1e-2, 1e-3] \\
 \hline
\end{tabular}
\caption{Table of hyper-parameters for Pivot}
\label{table:HP_PIVOT}
\end{table}


\begin{table}[ht!]
\centering
\begin{tabular}{||c c c c||} 
 \hline
 \multicolumn{4}{|c|}{INFERNO}\\
 \hline
 Problem & GG & S3D2 & HiggsML \\ [0.5ex] 
 \hline
 \# units & [50, 100, 200, 500] & [50, 100, 200, 500] & [50, 100, 200, 500] \\ 
 \# steps & [2000, 5000] & [2000, 5000] & [2000, 5000, 10 000] \\
 learning-rate & [1e-3] & [1e-3] & [1e-3] \\
 Optimizer & Adam & Adam & Adam \\
 $\beta_1$ & 0.5 & 0.5 & 0.5 \\
 $\beta_2$ & 0.9 & 0.9 & 0.9 \\
 temperature & 1 & 1 & 1 \\
 \hline
\end{tabular}
\caption{Table of hyper-parameters for INFERNO}
\label{table:HP_INF}
\end{table}



\begin{table}[ht!]
\centering
\begin{tabular}{||c c c c||} 
 \hline
 \multicolumn{4}{|c|}{Regressor}\\
 \hline
 Problem & GG & S3D2 & HiggsML \\ [0.5ex] 
 \hline
 \# units & [50, 100, 200, 500] & [50, 100, 200, 500] & [50, 100, 200, 500] \\ 
 \# steps & [2000, 5000] & [2000, 5000] & [2000, 5000, 10 000] \\
 learning-rate & [1e-3] & [1e-3] & [1e-3] \\
 Optimizer & Adam & Adam & Adam \\
 $\beta_1$ & 0.5 & 0.5 & 0.5 \\
 $\beta_2$ & 0.9 & 0.9 & 0.9 \\
 \hline
\end{tabular}
\caption{Table of hyper-parameters for Neural network datasetwise regressor}
\label{table:HP_REG}
\end{table}





\subsection{True parameter grid} % (fold)
\label{sub:true_parameter_grid}


\begin{table}[ht!]
\centering
\begin{tabular}{||c c||} 
 \hline
 \multicolumn{2}{|c|}{1D Toy GG true parameter grid}\\
 \hline
 $\mu$ & [0.1, 0.3, 0.5, 0.7, 0.9] \\ 
 $\alpha$ & [0.8, 1. , 1.2] \\ 
 \hline
\end{tabular}
\caption{Table of true parameter grid for the 1D Toy GG}
\label{table:GG_true_grid}
\end{table}

The fine range of $\alpha$ for systematic/statistical variance estimation is 31 equidistant values in $[0.5, 1.5]$ . 


\subsection{Prior distributions} % (fold)
\label{sub:prior_distributions}


\victor{TODO !}



\subsection{Data size} % (fold)
\label{sub:data_size}


\begin{table}[ht!]
\centering
\begin{tabular}{||c c c c||} 
 \hline
 \multicolumn{4}{|c|}{Data size}\\
 \hline
 Problem & GG & S3D2 & HiggsML \\ [0.5ex] 
 \hline
 \# training samples & 10 000 & 30 000 & ?? \\ 
 \# validation samples & 5 000 & 30 000 & ?? \\
 \# test samples & [50, 100, 300, 500, 2000] & [??] & [??] \\
 \hline
\end{tabular}
\caption{Table of the base number of samples in training, validation or testing datasets. }
\label{table:data_sizes}
\end{table}





