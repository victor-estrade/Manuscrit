@misc{prime-number-theorem,
annote = {A strong form of the prime number theorem, 19th century},
author = {{de la Vall{\'{e}}e Poussin}, Charles Louis Xavier Joseph},
title = {{No Title}}
}
@inproceedings{Neal:2007zz,
author = {Neal, Radford M},
booktitle = {Statistical issues for LHC physics. Proceedings, Workshop, PHYSTAT-LHC, Geneva, Switzerland, June 27-29, 2007},
pages = {111--118},
title = {{Computing likelihood functions for high-energy physics experiments when distributions are defined by simulators with nuisance parameters}},
url = {http://cds.cern.ch/record/1099977/files/p111.pdf},
year = {2007}
}
@misc{prime-number-theorem,
annote = {A strong form of the prime number theorem, 19th century},
author = {{de la Vall{\'{e}}e Poussin}, Charles Louis Xavier Joseph},
title = {{No Title}}
}
@misc{prime-number-theorem,
annote = {A strong form of the prime number theorem, 19th century},
author = {{de la Vall{\'{e}}e Poussin}, Charles Louis Xavier Joseph},
title = {{No Title}}
}
@article{neyman_pearson_1933,
author = {Jerzy, Neyman and Sharpe, Pearson Egon and Karl, Pearson},
doi = {10.1098/rsta.1933.0009},
journal = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
pages = {289--337},
title = {{On the problem of the most efficient tests of statistical hypotheses}},
volume = {231},
year = {1933}
}
@misc{prime-number-theorem,
annote = {A strong form of the prime number theorem, 19th century},
author = {{de la Vall{\'{e}}e Poussin}, Charles Louis Xavier Joseph},
title = {{No Title}}
}
@inproceedings{Mcgregor_variadic_neural_net,
author = {Mcgregor, Simon},
doi = {10.1007/978-3-540-74690-4_47},
pages = {460--470},
title = {{Neural Network Processing for Multiset Data}},
year = {2007}
}
@article{Mcgregor_variadic_neural_net_2,
author = {Mcgregor, Simon},
doi = {10.1016/j.neunet.2008.06.020},
journal = {Neural networks : the official journal of the International Neural Network Society},
pages = {830--837},
title = {{Further results in multiset processing with neural networks}},
volume = {21},
year = {2008}
}
@misc{prime-number-theorem,
annote = {A strong form of the prime number theorem, 19th century},
author = {{de la Vall{\'{e}}e Poussin}, Charles Louis Xavier Joseph},
title = {{No Title}}
}
@inproceedings{lucas:hal-01791126,
address = {Stockholm, Sweden},
author = {Lucas, Thomas and Tallec, Corentin and Verbeek, Jakob and Ollivier, Yann},
booktitle = {ICML - 35th International Conference on Machine Learning},
pages = {2844--2853},
series = {Proceedings of Machine Learning Research},
title = {{Mixed batches and symmetric discriminators for GAN training}},
url = {https://hal.inria.fr/hal-01791126},
volume = {80},
year = {2018}
}
@misc{prime-number-theorem,
annote = {A strong form of the prime number theorem, 19th century},
author = {{de la Vall{\'{e}}e Poussin}, Charles Louis Xavier Joseph},
title = {{No Title}}
}
@techreport{Bishop94mixturedensity,
author = {Bishop, Christopher M},
title = {{Mixture density networks}},
year = {1994}
}
@article{DECASTRO2019170inferno,
abstract = {Complex computer simulations are commonly required for accurate data modelling in many scientific disciplines, making statistical inference challenging due to the intractability of the likelihood evaluation for the observed data. Furthermore, sometimes one is interested on inference drawn over a subset of the generative model parameters while taking into account model uncertainty or misspecification on the remaining nuisance parameters. In this work, we show how non-linear summary statistics can be constructed by minimising inference-motivated losses via stochastic gradient descent such that they provide the smallest uncertainty for the parameters of interest. As a use case, the problem of confidence interval estimation for the mixture coefficient in a multi-dimensional two-component mixture model (i.e.Â signal vs background) is considered, where the proposed technique clearly outperforms summary statistics based on probabilistic classification, a commonly used alternative which does not account for the presence of nuisance parameters.},
author = {de Castro, Pablo and Dorigo, Tommaso},
doi = {https://doi.org/10.1016/j.cpc.2019.06.007},
issn = {0010-4655},
journal = {Computer Physics Communications},
keywords = { High energy physics, Neural networks, Nuisance parameters,Likelihood-free inference},
pages = {170--179},
title = {{INFERNO: Inference-Aware Neural Optimisation}},
url = {http://www.sciencedirect.com/science/article/pii/S0010465519301948},
volume = {244},
year = {2019}
}
@misc{prime-number-theorem,
annote = {A strong form of the prime number theorem, 19th century},
author = {{de la Vall{\'{e}}e Poussin}, Charles Louis Xavier Joseph},
title = {{No Title}}
}
@inproceedings{53ff0dc9643843d8ac4b40b699cb6bd8,
abstract = {An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes. We refer to our model as a neural statistician, and by this we mean a neural network that can learn to compute summary statistics of datasets without supervision.},
author = {Edwards, Harrison and Storkey, Amos},
booktitle = {5th International Conference on Learning Representations (ICLR 2017)},
title = {{Towards a Neural Statistician}},
year = {2017}
}
@inproceedings{estrade:hal-01715155,
address = {Bruges, Belgium},
author = {Estrade, Victor and Germain, C{\'{e}}cile and Guyon, Isabelle and Rousseau, David},
booktitle = {ESANN 2018 - 26th European Symposium on Artificial Neural Networks},
title = {{Systematics aware learning: a case study in High Energy Physics}},
url = {https://hal.inria.fr/hal-01715155},
year = {2018}
}
@conference{EstradeNIPS,
author = {Estrade, Victor and Germain, C{\'{e}}cile and Guyon, Isabelle and Rousseau, David},
booktitle = {Deep Learning for Physical Sciences @ NIPS},
title = {{Adversarial learning to eliminate systematic errors: a case study in HEP}},
year = {2017}
}
@article{zou2018potrojan,
author = {Zou, Minhui and Shi, Yang and Wang, Chengliang and Li, Fangyu and Song, WenZhan and Wang, Yu},
journal = {arXiv preprint arXiv:1802.03043},
title = {{PoTrojan: powerful neural-level trojan designs in deep learning models}},
year = {2018}
}
@article{milano2005sensitivity,
author = {Milano, Federico and Ca{\~{n}}izares, Claudio A and Conejo, Antonio J},
journal = {IEEE Transactions on power systems},
number = {4},
pages = {2051--2060},
publisher = {IEEE},
title = {{Sensitivity-based security-constrained OPF market clearing model}},
volume = {20},
year = {2005}
}
@article{DBLP:journals/corr/MirzaO14,
archivePrefix = {arXiv},
arxivId = {1411.1784},
author = {Mirza, Mehdi and Osindero, Simon},
eprint = {1411.1784},
journal = {CoRR},
title = {{Conditional Generative Adversarial Nets}},
url = {http://arxiv.org/abs/1411.1784},
volume = {abs/1411.1784},
year = {2014}
}
@inproceedings{donnot:hal-01783669,
address = {Rio de Janeiro, Brazil},
author = {Donnot, Benjamin and Guyon, Isabelle and Schoenauer, Marc and Marot, Antoine and Panciatici, Patrick},
booktitle = { IEEE WCCI 2018},
title = {{Anticipating contingengies in power grids using fast neural net screening}},
url = {https://hal.archives-ouvertes.fr/hal-01783669},
year = {2018}
}
@article{bengio2013representation,
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {8},
pages = {1798--1828},
publisher = {IEEE},
title = {{Representation learning: A review and new perspectives}},
volume = {35},
year = {2013}
}
@book{Dav88,
address = {Harlow UK},
author = {Davidson, Kenneth R},
publisher = {Longman Scientific & Technical},
series = {Pitman Research Notes in Mathematics Series},
title = {{Nest algebras}},
volume = {191},
year = {1988}
}
@book{Aup91,
address = {New York},
author = {Aupetit, B},
publisher = {Springer-Verlag},
title = {{A {Primer} on {Spectral} {Theory}}},
year = {1991}
}
@book{Hal82,
address = {New York},
author = {Halmos, P R},
edition = {Second},
publisher = {Springer-Verlag},
title = {{A {Hilbert} {Space} {Problem} {Book}}},
year = {1982}
}
@misc{cve-2008-1368,
annote = {[online] \url{http://nvd.nist.gov/nvd.cfm?cvename=CVE-2008-1368}},
institution = {NIST},
title = {{Publication quality tables in \LaTeX*}},
url = {http://nvd.nist.gov/nvd.cfm?cvename=CVE-2008-1368},
year = {2008}
}
@article{Rad87,
author = {Radjavi, H},
journal = {J. Alg.},
pages = {427--430},
title = {{The {Engel}-{Jacobson} {Theorem} {Revisited}}},
volume = {111},
year = {1987}
}
@article{Zimmerman11matpowersteadystate,
author = {Zimmerman, R D and Et al.},
journal = {IEEE Trans. on Power Systems},
pages = {12--19},
title = {{MATPOWER}},
year = {2011}
}
@inproceedings{yosinski2014transferable,
author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
booktitle = {Advances in neural information processing systems},
pages = {3320--3328},
title = {{How transferable are features in deep neural networks?}},
year = {2014}
}
@article{Sim96a,
author = {Simoni{\v{c}}, A},
journal = {Pacific J. Math.},
pages = {257--270},
title = {{A {Construction} of {Lomonosov} {Functions} and {Applications} to the {Invariant} {Subspace} {Problem}}},
volume = {175},
year = {1996}
}
@article{AAB95,
annote = {Birkh{\"{a}}user Verlag},
author = {Abramovich, Y A and Aliprantis, C D and Burkinshaw, O},
journal = {Operator Theory in Function Spaces and Banach Lattices. {\em The A.C.\,Zaanen Anniversary Volume}, Operator Theory: Advances and Applications},
pages = {15--31},
title = {{Another Characterization of the Invariant Subspace Problem}},
volume = {75},
year = {1995}
}
@book{wehenkel2012automatic,
author = {Wehenkel, Louis A},
publisher = {Springer Science & Business Media},
title = {{Automatic learning techniques in power systems}},
year = {2012}
}
@article{capitanescu2016critical,
author = {Capitanescu, Florin},
journal = {Electric Power Systems Research},
pages = {57--68},
publisher = {Elsevier},
title = {{Critical review of recent advances and further developments needed in AC optimal power flow}},
volume = {136},
year = {2016}
}
@inproceedings{xiao2016learning,
author = {Xiao, T and Et al.},
booktitle = {Proc. CVPR},
pages = {1249--1258},
title = {{Learning deep feature representations with domain guided dropout for person re-identification}},
year = {2016}
}
@book{Goodfellow-et-al-2016,
annote = {\url{http://www.deeplearningbook.org}},
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
publisher = {MIT Press},
title = {{Deep Learning}},
year = {2016}
}
@article{Sim92,
author = {Simoni{\v{c}}, A},
journal = {Linear Algebra Appl.},
pages = {57--76},
title = {{Matrix {Groups} with {Positive} {Spectra}}},
volume = {173},
year = {1992}
}
@article{capitanescu2011day,
author = {Capitanescu, Florin and Fliscounakis, St{\'{e}}phane and Panciatici, Patrick and Wehenkel, Louis},
journal = {PSCC proceedings Stockholm (Sweden) 2011},
title = {{Day-ahead security assessment under uncertainty relying on the combination of preventive and corrective controls to face worst-case scenarios}},
year = {2011}
}
@unpublished{donnot:hal-01906170,
annote = {working paper or preprint},
author = {Donnot, Benjamin and Guyon, Isabelle and Liu, Zhengying and Schoenauer, Marc and Marot, Antoine and Panciatici, Patrick},
title = {{Latent Surgical Interventions in Residual Neural Networks}},
url = {https://hal.archives-ouvertes.fr/hal-01906170},
year = {2018}
}
@book{texbook,
author = {Knuth, Donald E},
publisher = {Addison-Wesley},
title = {{The {{\TeX}book}}},
year = {1984}
}
@article{silver_mastering_2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ?value networks? to evaluate board positions and ?policy networks? to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
author = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
issn = {0028-0836},
journal = {Nature},
number = {7587},
pages = {484--489},
title = {{Mastering the game of {Go} with deep neural networks and tree search}},
url = {http://dx.doi.org/10.1038/nature16961},
volume = {529},
year = {2016}
}
@inproceedings{koch2015siamese,
author = {Koch, Gregory and Zemel, Richard and Salakhutdinov, Ruslan},
booktitle = {ICML Deep Learning Workshop},
title = {{Siamese neural networks for one-shot image recognition}},
volume = {2},
year = {2015}
}
@book{Dou72,
address = {New York},
author = {Douglas, R G},
publisher = {Academic Press},
title = {{Banach {Algebra} {Techniques} in {Operator} {Theory}}},
year = {1972}
}
@inproceedings{dalal2016hierarchical,
author = {Dalal, Gal and Gilboa, Elad and Mannor, Shie},
booktitle = {International Conference on Machine Learning},
pages = {2197--2206},
title = {{Hierarchical decision making in electricity grid management}},
year = {2016}
}
@inproceedings{fliscounakis2007topology,
author = {Fliscounakis, St{\'{e}}phane and Zaoui, Fabrice and Sim{\'{e}}ant, Gabriel and Gonzalez, Robert},
booktitle = {Power Tech, 2007 IEEE Lausanne},
organization = {IEEE},
pages = {1987--1990},
title = {{Topology influence on loss reduction as a mixed integer linear programming problem}},
year = {2007}
}
@inproceedings{oh2015action,
author = {Oh, Junhyuk and Guo, Xiaoxiao and Lee, Honglak and Lewis, Richard L and Singh, Satinder},
booktitle = {Advances in neural information processing systems},
pages = {2863--2871},
title = {{Action-conditional video prediction using deep networks in atari games}},
year = {2015}
}
@article{rosenblatt1958perceptron,
author = {Rosenblatt, Frank},
journal = {Psychological review},
number = {6},
pages = {386},
publisher = {American Psychological Association},
title = {{The perceptron: a probabilistic model for information storage and organization in the brain.}},
volume = {65},
year = {1958}
}
@inproceedings{he2016identity,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {European Conference on Computer Vision},
organization = {Springer},
pages = {630--645},
title = {{Identity mappings in deep residual networks}},
year = {2016}
}
@article{hinton2006reducing,
author = {Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
journal = {science},
number = {5786},
pages = {504--507},
publisher = {American Association for the Advancement of Science},
title = {{Reducing the dimensionality of data with neural networks}},
volume = {313},
year = {2006}
}
@inproceedings{nguyen2015deep,
author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
pages = {427--436},
title = {{Deep neural networks are easily fooled: High confidence predictions for unrecognizable images}},
year = {2015}
}
@inproceedings{bengio2013deep,
author = {Bengio, Yoshua},
booktitle = {International Conference on Statistical Language and Speech Processing},
organization = {Springer},
pages = {1--37},
title = {{Deep learning of representations: Looking forward}},
year = {2013}
}
@book{Spi65,
address = {New York},
author = {Spivak, Michael},
publisher = {The Benjamin/Cummings Publishing Company},
title = {{Calculus on {Manifolds}}},
year = {1965}
}
@inproceedings{krogh1992simple,
author = {Krogh, Anders and Hertz, John A},
booktitle = {Advances in neural information processing systems},
pages = {950--957},
title = {{A simple weight decay can improve generalization}},
year = {1992}
}
@inproceedings{goodfellow2014generative,
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in neural information processing systems},
pages = {2672--2680},
title = {{Generative adversarial nets}},
year = {2014}
}
@article{wehenkel1997machine,
author = {Wehenkel, Louis},
journal = {IEEE Expert},
number = {5},
pages = {60--72},
publisher = {IEEE},
title = {{Machine learning approaches to power-system security assessment}},
volume = {12},
year = {1997}
}
@phdthesis{Sim94,
author = {Simoni{\v{c}}, A},
school = {Dalhousie University, Department of Mathematics, Statistics, & Computing Science},
title = {{An {Extension} of {Lomonosov's} {Techniques} to {Non}-{Compact} {Operators}}},
year = {1994}
}
@article{bauer2017discriminative,
author = {Bauer, Matthias and Rojas-Carulla, Mateo and {\'{S}}wi\katkowski, Jakub Bart{\l}omiej and Sch{\"{o}}lkopf, Bernhard and Turner, Richard E},
journal = {arXiv preprint arXiv:1706.00326},
title = {{Discriminative k-shot learning using probabilistic models}},
year = {2017}
}
@mastersthesis{Sim90,
author = {Simoni{\v{c}}, A},
school = {Univerza v Ljubljani, FNT, Oddelek za Matematiko},
title = {{Grupe Operatorjev s Pozitivnim Spektrom}},
year = {1990}
}
@article{young2017recent,
author = {Young, Tom and Hazarika, Devamanyu and Poria, Soujanya and Cambria, Erik},
journal = {arXiv preprint arXiv:1708.02709},
title = {{Recent trends in deep learning based natural language processing}},
year = {2017}
}
@article{Lom73,
author = {Lomonosov, V I},
journal = {Functional Anal. Appl.},
pages = {213--214},
title = {{Invariant subspaces for operators commuting with compact operators}},
volume = {7},
year = {1973}
}
@book{DS57,
address = {New York},
author = {Dunford, N and Schwartz, J T},
publisher = {Interscience},
title = {{Linear {Operators}, {Part} {I}: {General} {Theory}}},
year = {1957}
}
@article{Rea85,
author = {Read, C J},
journal = {Bull. London Math. Soc.},
pages = {305--317},
title = {{A solution to the invariant subspace problem on the space $l_1$}},
volume = {17},
year = {1985}
}
@book{Hen93,
address = {London},
author = {Henderson, Peter},
publisher = {McGraw-Hill},
title = {{Object-oriented specification and design with {C}$++$}},
year = {1993}
}
@book{Con78,
address = {New York},
author = {Conway, J B},
publisher = {Springer-Verlag},
title = {{Functions of {One} {Complex} {Variable}}},
year = {1978}
}
@inproceedings{gal2016dropout,
author = {Gal, Yarin and Ghahramani, Zoubin},
booktitle = {international conference on machine learning},
pages = {1050--1059},
title = {{Dropout as a Bayesian approximation: Representing model uncertainty in deep learning}},
year = {2016}
}
@unpublished{donnot:hal-01783685,
annote = {working paper or preprint},
author = {Donnot, Benjamin and Guyon, Isabelle and Marot, Antoine and Schoenauer, Marc and Panciatici, Patrick},
title = {{Optimization of computational budget for power system risk assessment}},
url = {https://hal.archives-ouvertes.fr/hal-01783685},
year = {2018}
}
@article{lecun1998gradient,
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
journal = {Proceedings of the IEEE},
number = {11},
pages = {2278--2324},
publisher = {IEEE},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@inproceedings{caruana1995learning,
author = {Caruana, Rich},
booktitle = {Advances in neural information processing systems},
pages = {657--664},
title = {{Learning many related tasks at the same time with backpropagation}},
year = {1995}
}
@article{stott1974fast,
author = {Stott, Brian and Alsac, Of},
journal = {IEEE transactions on power apparatus and systems},
number = {3},
pages = {859--869},
publisher = {IEEE},
title = {{Fast decoupled load flow}},
year = {1974}
}
@article{cho2014exponentially,
author = {Cho, Kyunghyun and Bengio, Yoshua},
journal = {arXiv preprint arXiv:1406.7362},
title = {{Exponentially increasing the capacity-to-computation ratio for conditional computation in deep learning}},
year = {2014}
}
@article{fisher1919xv,
author = {Fisher, Ronald A},
journal = {Earth and Environmental Science Transactions of the Royal Society of Edinburgh},
number = {2},
pages = {399--433},
publisher = {Royal Society of Edinburgh Scotland Foundation},
title = {{XV.âThe correlation between relatives on the supposition of Mendelian inheritance.}},
volume = {52},
year = {1919}
}
@inproceedings{shashua2003ranking,
author = {Shashua, Amnon and Levin, Anat},
booktitle = {Advances in neural information processing systems},
pages = {961--968},
title = {{Ranking with large margin principle: Two approaches}},
year = {2003}
}
@book{Pau86,
address = {Harlow UK},
author = {Paulsen, Vern I},
publisher = {Longman Scientific & Technical},
series = {Pitman Research Notes in Mathematics Series},
title = {{Completely bounded maps and dilations}},
volume = {146},
year = {1986}
}
@article{alsac1974optimal,
author = {Alsac, O and Stott, B},
journal = {IEEE transactions on power apparatus and systems},
number = {3},
pages = {745--751},
publisher = {IEEE},
title = {{Optimal load flow with steady-state security}},
year = {1974}
}
@article{kadurin_cornucopia_2016,
abstract = {Oncotarget {\textbar} doi:10.18632/oncotarget.14073. Artur Kadurin, Alexander Aliper, Andrey Kazennov, Polina Mamoshina, Quentin Vanhaelen, Kuzma Khrabrov, Alex Zhavoronkov},
author = {Kadurin, Artur and Aliper, Alexander and Kazennov, Andrey and Mamoshina, Polina and Vanhaelen, Quentin and Khrabrov, Kuzma and Zhavoronkov, Alex and Kadurin, Artur and Aliper, Alexander and Kazennov, Andrey and Mamoshina, Polina and Vanhaelen, Quentin and Khrabrov, Kuzma and Zhavoronkov, Alex},
doi = {10.18632/oncotarget.14073},
issn = {1949-2553},
journal = {Oncotarget},
number = {7},
pages = {10883--10890},
shorttitle = {The cornucopia of meaningful leads},
title = {{The cornucopia of meaningful leads: {Applying} deep adversarial autoencoders for new molecule development in oncology}},
url = {http://www.impactjournals.com/oncotarget/index.php?journal=oncotarget&page=article&op=view&path%5B%5D=14073&path%5B%5D=44886},
volume = {8},
year = {2016}
}
@article{Lom91,
author = {Lomonosov, V I},
journal = {Israel J. Math},
pages = {329--339},
title = {{An extension of {Burnside}'s theorem to infinite dimensional spaces}},
volume = {75},
year = {1991}
}
@article{dB59,
author = {de Branges, L},
journal = {Proc. Amer. Math. Soc.},
pages = {822--824},
title = {{The {Stone}-{Weierstrass} {Theorem}}},
volume = {10},
year = {1959}
}
@article{srivastava2014dropout,
author = {Srivastava, Nitish and Hinton, Geoffrey E and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
journal = {Journal of Machine Learning Research},
number = {1},
pages = {1929--1958},
title = {{Dropout: a simple way to prevent neural networks from overfitting.}},
volume = {15},
year = {2014}
}
@book{freedman2009statistical,
author = {Freedman, David A},
publisher = {cambridge university press},
title = {{Statistical models: theory and practice}},
year = {2009}
}
@inproceedings{zemel2013learning,
author = {Zemel, Rich and Wu, Yu and Swersky, Kevin and Pitassi, Toni and Dwork, Cynthia},
booktitle = {International Conference on Machine Learning},
pages = {325--333},
title = {{Learning fair representations}},
year = {2013}
}
@article{DBLP:journals/corr/KingmaRMW14,
archivePrefix = {arXiv},
arxivId = {1406.5298},
author = {Kingma, Diederik P and Rezende, Danilo Jimenez and Mohamed, Shakir and Welling, Max},
eprint = {1406.5298},
journal = {CoRR},
title = {{Semi-Supervised Learning with Deep Generative Models}},
url = {http://arxiv.org/abs/1406.5298},
volume = {abs/1406.5298},
year = {2014}
}
@article{Ancey1996,
author = {Ancey, Christophe and Coussot, Philippe and Evesque, Pierre},
journal = {Mechanics of Cohesive-frictional Materials},
number = {4},
pages = {385--403},
title = {{Examination of the possibility of a fluid-mechanics treatment of dense granular flows}},
url = {http://doi.wiley.com/10.1002/(SICI)1099-1484(199610)1:4%3C385::AID-CFM20%3E3.0.CO;2-0},
volume = {1},
year = {1996}
}
@inbook{SFPT,
address = {New York},
author = {Dunford, N and Schwartz, J T},
pages = {456},
publisher = {Interscience},
title = {{Linear {Operators}, {Part} {I}: {General} {Theory}}},
year = {1957}
}
@article{hussain2011,
author = {Hussain, Zakir and Chen, Zhe and Th{\o}gersen, Paul},
isbn = {978-1-4577-1083-4},
journal = {2011 IEEE Jordan Conference on Applied Electrical Engineering and Computing Technologies, AEECT 2011},
pages = {1--7},
title = {{Fast and precise method of contingency ranking in modern power system}},
year = {2011}
}
@inproceedings{zaoui2005coupling,
author = {Zaoui, Fabrice and Fliscounakis, St{\'{e}}phane and Gonzalez, Robert},
booktitle = {15th Power Systems Computation Conference},
pages = {22--26},
title = {{Coupling OPF and topology optimization for security purposes}},
year = {2005}
}
@article{Enf87,
annote = {Seminare Maurey-Schwartz (1975-1976)},
author = {Enflo, P},
journal = {Acta. Math.},
pages = {213--313},
title = {{On the invariant subspaces problem for {Banach} spaces}},
volume = {158},
year = {1987}
}
@article{LedigTHCATTWS16,
author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Aitken, Andrew P and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
journal = {CoRR},
title = {{Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network}},
url = {http://arxiv.org/abs/1609.04802},
volume = {abs/1609.04802},
year = {2016}
}
@article{fliscounakis2013contingency,
author = {Fliscounakis, St{\'{e}}phane and Panciatici, Patrick and Capitanescu, Florin and Wehenkel, Louis},
journal = {IEEE Transactions on Power Systems},
number = {4},
pages = {4909--4917},
publisher = {IEEE},
title = {{Contingency ranking with respect to overloads in very large power systems taking into account uncertainty, preventive, and corrective actions}},
volume = {28},
year = {2013}
}
@article{hoerl1970ridge,
author = {Hoerl, Arthur E and Kennard, Robert W},
journal = {Technometrics},
number = {1},
pages = {55--67},
publisher = {Taylor & Francis Group},
title = {{Ridge regression: Biased estimation for nonorthogonal problems}},
volume = {12},
year = {1970}
}
@book{Con90,
address = {New York},
author = {Conway, J B},
edition = {Second},
publisher = {Springer-Verlag},
title = {{A {Course} in {Functional} {Analysis}}},
year = {1990}
}
@article{lecun1989backpropagation,
author = {LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
journal = {Neural computation},
number = {4},
pages = {541--551},
publisher = {MIT Press},
title = {{Backpropagation applied to handwritten zip code recognition}},
volume = {1},
year = {1989}
}
@article{abdulrazzaq2015contingency,
author = {Abdulrazzaq, Ali Abdulwahhab},
journal = {International research journal of engineering and technology},
number = {2},
pages = {180--183},
title = {{Contingency ranking of power systems using a performance index}},
volume = {2},
year = {2015}
}
@book{KR86,
address = {New York},
author = {Kadison, R V and Ringrose, J R},
publisher = {Academic Press},
title = {{Fundamentals of the {Theory} of {Operator} {Algebras}, {Part} {II}}},
year = {1986}
}
@inproceedings{Socher:2013:ZLT:2999611.2999716,
address = {USA},
author = {Socher, Richard and Ganjoo, Milind and Manning, Christopher D and Ng, Andrew Y},
booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 1},
pages = {935--943},
publisher = {Curran Associates Inc.},
series = {NIPS'13},
title = {{Zero-shot Learning Through Cross-modal Transfer}},
url = {http://dl.acm.org/citation.cfm?id=2999611.2999716},
year = {2013}
}
@inproceedings{wan2013regularization,
author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Cun, Yann L and Fergus, Rob},
booktitle = {Proceedings of the 30th International Conference on Machine Learning (ICML-13)},
pages = {1058--1066},
title = {{Regularization of neural networks using dropconnect}},
year = {2013}
}
@article{dommel1968optimal,
author = {Dommel, Hermann W and Tinney, William F},
journal = {IEEE Transactions on power apparatus and systems},
number = {10},
pages = {1866--1876},
publisher = {IEEE},
title = {{Optimal power flow solutions}},
year = {1968}
}
@inproceedings{salimans2016weight,
author = {Salimans, Tim and Kingma, Diederik P},
booktitle = {Advances in Neural Information Processing Systems},
pages = {901},
title = {{Weight normalization: A simple reparameterization to accelerate training of deep neural networks}},
year = {2016}
}
@unpublished{Sim91,
annote = {Lecture Notes, Dalhousie University,
Department of Mathematics, Statistics, & Computing Science},
author = {Simoni{\v{c}}, A},
title = {{Notes on {Subharmonic} {Functions}}},
year = {1991}
}
@article{abido2002optimal,
author = {Abido, M A},
journal = {International Journal of Electrical Power & Energy Systems},
number = {7},
pages = {563--571},
publisher = {Elsevier},
title = {{Optimal power flow using particle swarm optimization}},
volume = {24},
year = {2002}
}
@inproceedings{ewert2017structured,
author = {Ewert, S and Sandler, M B},
booktitle = {Proc. ICASSP},
organization = {IEEE},
pages = {2277--2281},
title = {{Structured dropout for weak label and multi-instance learning and its application to score-informed source separation}},
year = {2017}
}
@book{pearl2009causality,
author = {Pearl, Judea},
publisher = {Cambridge university press},
title = {{Causality}},
year = {2009}
}
@article{carpentier1962contribution,
author = {Carpentier, J},
journal = {Bulletin de la Societe Francaise des Electriciens},
number = {1},
pages = {431--447},
title = {{Contribution a l'etude du dispatching economique}},
volume = {3},
year = {1962}
}
@article{capitanescu2016critical,
author = {Capitanescu, Florin},
journal = {Electric Power Systems Research},
pages = {57--68},
publisher = {Elsevier},
title = {{Critical review of recent advances and further developments needed in AC optimal power flow}},
volume = {136},
year = {2016}
}
@article{silver_mastering_2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses âvalue networks' to evaluate board positions and âpolicy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
author = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
issn = {0028-0836},
journal = {Nature},
number = {7587},
pages = {484--489},
title = {{Mastering the game of {Go} with deep neural networks and tree search}},
url = {http://dx.doi.org/10.1038/nature16961},
volume = {529},
year = {2016}
}
@article{KPS75,
author = {Kim, H W and Pearcy, C and Shields, A L},
journal = {Michigan Math. J.},
number = {3},
pages = {193--194},
title = {{Rank-One Commutators and Hyperinvariant Subspaces}},
volume = {22},
year = {1975}
}
@article{doi:10.1021/ci9003865,
annote = {PMID: 20387860},
author = {Agarwal, Shivani and Dugar, Deepak and Sengupta, Shiladitya},
doi = {10.1021/ci9003865},
journal = {Journal of Chemical Information and Modeling},
number = {5},
pages = {716--731},
title = {{Ranking Chemical Structures for Drug Discovery: A New Machine Learning Approach}},
url = {http://dx.doi.org/10.1021/ci9003865},
volume = {50},
year = {2010}
}
@article{sunitha2013online,
author = {Sunitha, R and Kumar, Sreerama Kumar and Mathew, Abraham T},
journal = {IEEE Transactions on Power Systems},
number = {4},
pages = {4328--4335},
publisher = {IEEE},
title = {{Online static security assessment module using artificial neural networks}},
volume = {28},
year = {2013}
}
@article{sidhu2000contingency,
author = {Sidhu, Tarlochan S and Cui, Lan},
journal = {IEEE Transactions on Power Systems},
number = {1},
pages = {421--426},
publisher = {IEEE},
title = {{Contingency screening for steady-state security analysis by using FFT and artificial neural networks}},
volume = {15},
year = {2000}
}
@article{lecun1998gradient,
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
journal = {Proceedings of the IEEE},
number = {11},
pages = {2278--2324},
publisher = {IEEE},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@inproceedings{Duchesne2017,
author = {Duchesne, Laurine and Karangelos, Efthymios and Wehenkel, Louis},
booktitle = {2017 {IEEE} Manchester {PowerTech}},
doi = {10.1109/ptc.2017.7980927},
publisher = {IEEE},
title = {{Machine learning of real-time power systems reliability management response}},
url = {https://doi.org/10.1109/ptc.2017.7980927},
year = {2017}
}
@article{kimura2018imitation,
author = {Kimura, Akisato and Ghahramani, Zoubin and Takeuchi, Koh and Iwata, Tomoharu and Ueda, Naonori},
journal = {arXiv preprint arXiv:1802.03039},
title = {{Imitation networks: Few-shot learning of neural networks from scratch}},
year = {2018}
}
@book{Rud73,
address = {New York},
author = {Rudin, W},
publisher = {McGraw-Hill},
title = {{Functional {Analysis}}},
year = {1973}
}
@article{Nguyen-95,
author = {Nguyen, T T},
issn = {1350-2360},
journal = {IEE Proceedings - Generation, Transmission and Distribution},
number = {1},
pages = {51--58(7)},
title = {{Neural network load-flow}},
url = {http://digital-library.theiet.org/content/journals/10.1049/ip-gtd_19951484},
volume = {142},
year = {1995}
}
@inproceedings{he2016deep,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages = {770--778},
title = {{Deep residual learning for image recognition}},
year = {2016}
}
@article{capitanescu2011state,
author = {Capitanescu, Florin and Ramos, J L Martinez and Panciatici, Patrick and Kirschen, Daniel and Marcolini, A Marano and Platbrood, Ludovic and Wehenkel, Louis},
journal = {Electric Power Systems Research},
number = {8},
pages = {1731--1741},
publisher = {Elsevier},
title = {{State-of-the-art, challenges, and future trends in security constrained optimal power flow}},
volume = {81},
year = {2011}
}
@article{capitanescu2011day,
author = {Capitanescu, Florin and Fliscounakis, St{\'{e}}phane and Panciatici, Patrick and Wehenkel, Louis},
journal = {PSCC proceedings Stockholm (Sweden) 2011},
title = {{Day-ahead security assessment under uncertainty relying on the combination of preventive and corrective controls to face worst-case scenarios}},
year = {2011}
}
@article{dommel1968optimal,
author = {Dommel, Hermann W and Tinney, William F},
journal = {IEEE Transactions on power apparatus and systems},
number = {10},
pages = {1866--1876},
publisher = {IEEE},
title = {{Optimal power flow solutions}},
year = {1968}
}
@inproceedings{ng2004feature,
author = {Ng, Andrew Y},
booktitle = {Proceedings of the twenty-first international conference on Machine learning},
organization = {ACM},
pages = {78},
title = {{Feature selection, L 1 vs. L 2 regularization, and rotational invariance}},
year = {2004}
}
@article{li2011short,
author = {Li, Hang},
journal = {IEICE TRANSACTIONS on Information and Systems},
number = {10},
pages = {1854--1862},
publisher = {The Institute of Electronics, Information and Communication Engineers},
title = {{A short introduction to learning to rank}},
volume = {94},
year = {2011}
}
@inproceedings{saeh2008static,
author = {Saeh, I S and Khairuddin, A},
booktitle = {Power and Energy Conference, 2008. PECon 2008. IEEE 2nd International},
organization = {IEEE},
pages = {1172--1178},
title = {{Static security assessment using artificial neural network}},
year = {2008}
}
@article{Lom92,
author = {Lomonosov, V I},
journal = {Proc. Amer. Math. Soc.},
number = {3},
pages = {775--777},
title = {{On {Real} {Invariant} {Subspaces} of {Bounded} {Operators} with {Compact} {Imaginary} {Part}}},
volume = {115},
year = {1992}
}
@article{Duchesne2017MachineLO,
author = {Duchesne, Laurine and Karangelos, Efthymios and Wehenkel, Louis},
journal = {2017 IEEE Manchester PowerTech},
pages = {1--6},
title = {{Machine learning of real-time power systems reliability management response}},
year = {2017}
}
@book{pearl2009causality,
author = {Pearl, Judea},
publisher = {Cambridge university press},
title = {{Causality}},
year = {2009}
}
@inproceedings{krizhevsky2012imagenet,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in neural information processing systems},
pages = {1097--1105},
title = {{Imagenet classification with deep convolutional neural networks}},
year = {2012}
}
@book{Dev68,
address = {New York},
author = {Devinaz, Allen},
publisher = {Holt, Rinehart and Winston},
title = {{Advanced {Calculus}}},
year = {1968}
}
@article{Dau75,
author = {Daughtry, J},
journal = {Proc. Amer. Math. Soc.},
pages = {267--268},
title = {{An invariant subspace theorem}},
volume = {49},
year = {1975}
}
@article{donnot:hal-01695793,
author = {Donnot, Benjamin and Guyon, Isabelle and Schoenauer, Marc and Marot, Antoine and Panciatici, Patrick},
journal = {European Symposium on Artificial Neural Networks},
title = {{Fast Power system security analysis with Guided Dropout}},
url = {https://hal.archives-ouvertes.fr/hal-01695793},
year = {2018}
}
@article{perez2017effectiveness,
author = {Perez, Luis and Wang, Jason},
journal = {arXiv preprint arXiv:1712.04621},
title = {{The effectiveness of data augmentation in image classification using deep learning}},
year = {2017}
}
@article{gini_paper,
author = {Gini, C},
title = {{"Concentration and dependency ratios" (in Italian -- English translation in Rivista di Politica Economica, 87 (1997), 769â789)}},
year = {1909}
}
@article{mclean1991unified,
author = {McLean, R and Et al.},
journal = {The American Statistician},
number = {1},
pages = {54--64},
title = {{A unified approach to mixed linear models}},
volume = {45},
year = {1991}
}
@inproceedings{goodfellow2014generative,
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in neural information processing systems},
pages = {2672--2680},
title = {{Generative adversarial nets}},
year = {2014}
}
@inproceedings{wan2013regularization,
author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Cun, Yann L and Fergus, Rob},
booktitle = {Proceedings of the 30th International Conference on Machine Learning (ICML-13)},
pages = {1058--1066},
title = {{Regularization of neural networks using dropconnect}},
year = {2013}
}
@article{Sim96b,
author = {Simoni{\v{c}}, A},
journal = {Trans. Amer. Math. Soc.},
pages = {975--995},
title = {{An extension of {Lomonosov's} {Techniques} to non-compact {Operators}}},
volume = {348},
year = {1996}
}
@book{friedman2001elements,
author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
number = {10},
publisher = {Springer series in statistics New York, NY, USA:},
title = {{The elements of statistical learning}},
volume = {1},
year = {2001}
}
@inproceedings{Hossen-2017,
author = {Hossen, T and Plathottam, S J and Angamuthu, R K and Ranganathan, P and Salehfar, H},
booktitle = {2017 North American Power Symposium (NAPS)},
doi = {10.1109/NAPS.2017.8107271},
month = {sep},
pages = {1--6},
title = {{Short-term load forecasting using deep neural networks (DNN)}},
year = {2017}
}
@article{battaglia2018relational,
author = {Battaglia, Peter W and Hamrick, Jessica B and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Others},
journal = {arXiv preprint arXiv:1806.01261},
title = {{Relational inductive biases, deep learning, and graph networks}},
year = {2018}
}
@inproceedings{bdonnot:hal-01581719,
address = {Espinho, Portugal},
author = {Donnot, B and Et al.},
booktitle = {IREP Symposium},
keywords = {machine learning ; imitation learning ; power system},
title = {{Introducing machine learning for power system operation support}},
url = {https://hal.inria.fr/hal-01581719},
year = {2017}
}
@article{LedigTHCATTWS16,
author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Aitken, Andrew P and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
journal = {CoRR},
title = {{Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network}},
url = {http://arxiv.org/abs/1609.04802},
volume = {abs/1609.04802},
year = {2016}
}
@book{RR73,
address = {New York},
author = {Radjavi, H and Rosenthal, P},
publisher = {Springer-Verlag},
title = {{Invariant {Subspaces}}},
year = {1973}
}
@article{brown2018superhuman,
author = {Brown, Noam and Sandholm, Tuomas},
journal = {Science},
number = {6374},
pages = {418--424},
publisher = {American Association for the Advancement of Science},
title = {{Superhuman AI for heads-up no-limit poker: Libratus beats top professionals}},
volume = {359},
year = {2018}
}
@article{li2017meta,
author = {Li, Zhenguo and Zhou, Fengwei and Chen, Fei and Li, Hang},
journal = {arXiv preprint arXiv:1707.09835},
title = {{Meta-SGD: Learning to Learn Quickly for Few Shot Learning}},
year = {2017}
}
@article{dB93,
author = {de Branges, L},
journal = {Math. Nachr.},
pages = {163--175},
title = {{A construction of invariant subspaces}},
volume = {163},
year = {1993}
}
@misc{reitan1968computer,
author = {Reitan, Daniel K},
publisher = {Pergamon},
title = {{Computer methods in power-system analysis: by Glenn W. Stagg and Ahmed H. El-Abiad, 427 pages, diagrams, illustr., 6$\times$ 9 in.}},
year = {1968}
}
@article{LM65,
author = {Ljubi{\v{c}}, Ju. I and Macaev, V I},
journal = {Amer. Math. Soc. Transl. (2)},
pages = {89--129},
title = {{On Operators with a Separable Spectrum}},
volume = {47},
year = {1965}
}
@article{bengio2012unsupervised,
author = {Bengio, Yoshua and Courville, Aaron C and Vincent, Pascal},
journal = {CoRR, abs/1206.5538},
title = {{Unsupervised feature learning and deep learning: A review and new perspectives}},
volume = {1},
year = {2012}
}
@article{vaahedi1999voltage,
author = {Vaahedi, E and Fuchs, C and Xu, W and Mansour, Y and Hamadanizadeh, H and Morison, G K},
journal = {IEEE Transactions on power systems},
number = {1},
pages = {256--265},
publisher = {IEEE},
title = {{Voltage stability contingency screening and ranking}},
volume = {14},
year = {1999}
}
@article{1508.01775,
author = {Hines, Paul D H and Dobson, Ian and Rezaei, Pooya},
doi = {10.1109/TPWRS.2016.2578259},
title = {{Cascading Power Outages Propagate Locally in an Influence Graph that is not the Actual Grid Topology}},
year = {2015}
}
@article{shazeer2017outrageously,
author = {Shazeer, N and Et al.},
journal = {arXiv:1701.06538},
title = {{Outrageously large neural networks: The sparsely-gated mixture-of-experts layer}},
year = {2017}
}
@article{kadurin_cornucopia_2016,
abstract = {Oncotarget {\textbar} doi:10.18632/oncotarget.14073. Artur Kadurin, Alexander Aliper, Andrey Kazennov, Polina Mamoshina, Quentin Vanhaelen, Kuzma Khrabrov, Alex Zhavoronkov},
author = {Kadurin, Artur and Aliper, Alexander and Kazennov, Andrey and Mamoshina, Polina and Vanhaelen, Quentin and Khrabrov, Kuzma and Zhavoronkov, Alex and Kadurin, Artur and Aliper, Alexander and Kazennov, Andrey and Mamoshina, Polina and Vanhaelen, Quentin and Khrabrov, Kuzma and Zhavoronkov, Alex},
doi = {10.18632/oncotarget.14073},
issn = {1949-2553},
journal = {Oncotarget},
number = {7},
pages = {10883--10890},
shorttitle = {The cornucopia of meaningful leads},
title = {{The cornucopia of meaningful leads: {Applying} deep adversarial autoencoders for new molecule development in oncology}},
url = {http://www.impactjournals.com/oncotarget/index.php?journal=oncotarget&page=article&op=view&path%5B%5D=14073&path%5B%5D=44886},
volume = {8},
year = {2016}
}
@book{Gan59,
address = {New York},
author = {Gantmacher, F R},
publisher = {Interscience},
title = {{Applications of the {Theory} of {Matrices}}},
year = {1959}
}
@book{kundur1994power,
author = {Kundur, Prabha and Balu, Neal J and Lauby, Mark G},
publisher = {McGraw-hill New York},
title = {{Power system stability and control}},
volume = {7},
year = {1994}
}
@article{josz2015application,
author = {Josz, C{\'{e}}dric and Maeght, Jean and Panciatici, Patrick and Gilbert, Jean Charles},
journal = {IEEE Transactions on Power Systems},
number = {1},
pages = {463--470},
publisher = {IEEE},
title = {{Application of the moment-SOS approach to global optimization of the OPF problem}},
volume = {30},
year = {2015}
}
@book{KR83,
address = {New York},
author = {Kadison, R V and Ringrose, J R},
publisher = {Academic Press},
title = {{Fundamentals of the {Theory} of {Operator} {Algebras}, {Part} {I}}},
year = {1983}
}
@inproceedings{karangelos2016probabilistic,
author = {Karangelos, Efthymios and Wehenkel, Louis},
booktitle = {Power Systems Computation Conference (PSCC), 2016},
organization = {IEEE},
pages = {1--9},
title = {{Probabilistic reliability management approach and criteria for power system real-time operation}},
year = {2016}
}
@article{chiappa2017recurrent,
author = {Chiappa, Silvia and Racaniere, S{\'{e}}bastien and Wierstra, Daan and Mohamed, Shakir},
journal = {arXiv preprint arXiv:1704.02254},
title = {{Recurrent environment simulators}},
year = {2017}
}
@incollection{kundur2012power,
author = {Kundur, Prabha S},
booktitle = {Power System Stability and Control, Third Edition},
pages = {1--12},
publisher = {CRC Press},
title = {{Power system stability}},
year = {2012}
}
@article{hines2017cascading,
author = {Hines, Paul D H and Dobson, Ian and Rezaei, Pooya},
journal = {IEEE Transactions on Power Systems},
number = {2},
pages = {958--967},
publisher = {IEEE},
title = {{Cascading power outages propagate locally in an influence graph that is not the actual grid topology}},
volume = {32},
year = {2017}
}
@misc{prime-number-theorem,
annote = {A strong form of the prime number theorem, 19th century},
author = {{de la Vall{\'{e}}e Poussin}, Charles Louis Xavier Joseph},
title = {{No Title}}
}
@inproceedings{salimans2016weight,
author = {Salimans, Tim and Kingma, Diederik P},
booktitle = {Advances in Neural Information Processing Systems},
pages = {901},
title = {{Weight normalization: A simple reparameterization to accelerate training of deep neural networks}},
year = {2016}
}
@book{james2013introduction,
author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
publisher = {Springer},
title = {{An introduction to statistical learning}},
volume = {112},
year = {2013}
}
@article{tibshirani1996regression,
author = {Tibshirani, Robert},
journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
pages = {267--288},
publisher = {JSTOR},
title = {{Regression shrinkage and selection via the lasso}},
year = {1996}
}
@inproceedings{he2016identity,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {European Conference on Computer Vision},
organization = {Springer},
pages = {630--645},
title = {{Identity mappings in deep residual networks}},
year = {2016}
}
@article{bengio2013estimating,
author = {Bengio, Y and Et al.},
journal = {arXiv:1308.3432},
title = {{Estimating or propagating gradients through stochastic neurons for conditional computation}},
year = {2013}
}
@article{10.2307/2223319,
author = {Gini, Corrado},
issn = {00130133, 14680297},
journal = {The Economic Journal},
number = {121},
pages = {124--126},
publisher = {[Royal Economic Society, Wiley]},
title = {{Measurement of Inequality of Incomes}},
url = {http://www.jstor.org/stable/2223319},
volume = {31},
year = {1921}
}
@misc{reitan1968computer,
author = {Reitan, Daniel K},
publisher = {Pergamon},
title = {{Computer methods in power-system analysis: by Glenn W. Stagg and Ahmed H. El-Abiad, 427 pages, diagrams, illustr., 6$\times$ 9 in.}},
year = {1968}
}
@inproceedings{ruiz2012reduced,
author = {Ruiz, Pablo A and Rudkevich, Aleksandr and Caramanis, Michael C and Goldis, Evgenyi and Ntakou, Elli and Philbrick, C Russ},
booktitle = {Communication, Control, and Computing (Allerton), 2012 50th Annual Allerton Conference on},
organization = {IEEE},
pages = {1073--1079},
title = {{Reduced MIP formulation for transmission topology control}},
year = {2012}
}
@book{latex,
author = {Lamport, Leslie},
publisher = {Addison-Wesley},
title = {{{\LaTeX:} {A} Document Preparation System}},
year = {1986}
}
@inproceedings{trias2012holomorphic,
author = {Trias, Antonio},
booktitle = {Power and Energy Society General Meeting, 2012 IEEE},
organization = {IEEE},
pages = {1--8},
title = {{The holomorphic embedding load flow method}},
year = {2012}
}
@article{grigg1999ieee,
author = {Grigg, Cliff and Wong, Peter and Albrecht, Paul and Allan, Ron and Bhavaraju, Murty and Billinton, Roy and Chen, Quan and Fong, Clement and Haddad, Suheil and Kuruganty, Sastry and Others},
journal = {IEEE Transactions on power systems},
number = {3},
pages = {1010--1020},
publisher = {IEEE},
title = {{The IEEE reliability test system-1996. A report prepared by the reliability test system task force of the application of probability methods subcommittee}},
volume = {14},
year = {1999}
}
@book{Gam90,
address = {New York},
editor = {Gamkerlidze, R V},
publisher = {Springer-Verlag},
series = {Encyclopaedia of Mathematical Sciences},
title = {{Analysis {I}{I}: {Convex} {Analysis} and {Approximation} {Theory}}},
volume = {14},
year = {1990}
}
@article{MOR91,
author = {Mathes, B and Omladi{\v{c}}, M and Radjavi, H},
journal = {Linear Algebra Appl.},
pages = {215--225},
title = {{Linear {Spaces} of {Nilpotent} {Operators}}},
volume = {149},
year = {1991}
}
@book{adams1995hitchhiker,
author = {Adams, D},
isbn = {9781417642595},
publisher = {San Val},
title = {{The Hitchhiker's Guide to the Galaxy}},
url = {http://books.google.com/books?id=W-xMPgAACAAJ},
year = {1995}
}
@article{Zou05regularizationand,
author = {Zou, Hui and Hastie, Trevor},
journal = {Journal of the Royal Statistical Society, Series B},
pages = {301--320},
title = {{Regularization and variable selection via the Elastic Net}},
volume = {67},
year = {2005}
}
@article{Cowan2011,
abstract = {We describe likelihood-based statistical tests for use in high energy physics for the discovery of new phenomena and for construction of confidence intervals on model parameters. We focus on the properties of the test procedures that allow one to account for systematic uncertainties. Explicit formulae for the asymptotic distributions of test statistics are derived using results of Wilks and Wald. We motivate and justify the use of a representative data set, called the ``Asimov data set'', which provides a simple method to obtain the median experimental sensitivity of a search or measurement as well as fluctuations about this expectation.},
author = {Cowan, Glen and Cranmer, Kyle and Gross, Eilam and Vitells, Ofer},
doi = {10.1140/epjc/s10052-011-1554-0},
issn = {1434-6052},
journal = {The European Physical Journal C},
number = {2},
pages = {1554},
title = {{Asymptotic formulae for likelihood-based tests of new physics}},
url = {https://doi.org/10.1140/epjc/s10052-011-1554-0},
volume = {71},
year = {2011}
}
@inproceedings{Adam-Bourdarios2014,
author = {Adam-Bourdarios, Claire and Cowan, Glen and Germain, C{\'{e}}cile and Guyon, Isabelle and K{\'{e}}gl, Bal{\'{a}}zs and Rousseau, David},
booktitle = {{HEPML}@ {NIPS}},
title = {{The {Higgs} boson machine learning challenge.}},
year = {2014}
}
@article{Chen2016,
abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
archivePrefix = {arXiv},
arxivId = {1606.03657},
author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
doi = {10.1007/978-3-319-16817-3},
eprint = {1606.03657},
file = {:home/estrade/Documents/Articles/1606.03657.pdf:pdf},
isbn = {978-3-319-16816-6},
issn = {978-3-319-16807-4},
pmid = {23459267},
title = {{InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}},
url = {http://arxiv.org/abs/1606.03657},
year = {2016}
}
@article{Perdue,
archivePrefix = {arXiv},
arxivId = {arXiv:1808.08332v2},
author = {Perdue, G N and Ghosh, A and Wospakrik, M and Akbar, F and Andrade, D A and Ascencio, M and Bellantoni, L and Bercellie, A and Betancourt, M and Vera, G F R Caceres and Cai, T and Carneiro, M F and Chaves, J and Coplowe, D and Felix, J and Fields, L and Fine, R and Gago, A M and Galindo, R and Golan, T and Gran, R and Han, J Y and Harris, D A and Jena, D and Kleykamp, J and Kordosky, M and Maher, E and Mann, W A and Marshall, C M and Mcfarland, K S and Mcgowan, A M and Messerly, B and Miller, J and Nelson, J K and Nguyen, C and Norrick, A and Olivier, A and Patton, R and Ransome, R D and Ray, H and Ren, L and Rimal, D and Ruterbories, D and Schellman, H and Salinas, C J Solano and Su, H and Upadhyay, S and Valencia, E and Wolcott, J and Yaeggy, B and Young, S and Janeiro, Rio De and Janeiro, Rio De and Adams, North},
eprint = {arXiv:1808.08332v2},
file = {:home/estrade/Documents/Articles/1808.08332.pdf:pdf},
title = {{Reducing model bias in a deep learning classifier using domain adversarial neural networks in the MINER $\nu$ A}}
}
@article{Shwartz-Ziv2017,
abstract = {Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work proposed to analyze DNNs in the \textit{Information Plane}; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of DNNs. Our main results are: (i) most of the training epochs in standard DL are spent on {\emph compression} of the input to efficient representation and not on fitting the training labels. (ii) The representation compression phase begins when the training errors becomes small and the Stochastic Gradient Decent (SGD) epochs change from a fast drift to smaller training error into a stochastic relaxation, or random diffusion, constrained by the training error value. (iii) The converged layers lie on or very close to the Information Bottleneck (IB) theoretical bound, and the maps from the input to any hidden layer and from this hidden layer to the output satisfy the IB self-consistent equations. This generalization through noise mechanism is unique to Deep Neural Networks and absent in one layer networks. (iv) The training time is dramatically reduced when adding more hidden layers. Thus the main advantage of the hidden layers is computational. This can be explained by the reduced relaxation time, as this it scales super-linearly (exponentially for simple diffusion) with the information compression from the previous layer.},
archivePrefix = {arXiv},
arxivId = {1703.00810},
author = {Shwartz-Ziv, Ravid and Tishby, Naftali},
eprint = {1703.00810},
file = {:home/estrade/Documents/Articles/1703.00810.pdf:pdf},
pages = {1--19},
title = {{Opening the Black Box of Deep Neural Networks via Information}},
url = {http://arxiv.org/abs/1703.00810},
year = {2017}
}
@article{Shimmin2017,
abstract = {We describe a strategy for constructing a neural network jet substructure tagger which powerfully discriminates boosted decay signals while remaining largely uncorrelated with the jet mass. This reduces the impact of systematic uncertainties in background modeling while enhancing signal purity, resulting in improved discovery significance relative to existing taggers. The network is trained using an adversarial strategy, resulting in a tagger that learns to balance classification accuracy with decorrelation. As a benchmark scenario, we consider the case where large-radius jets originating from a boosted resonance decay are discriminated from a background of nonresonant quark and gluon jets. We show that in the presence of systematic uncertainties on the background rate, our adversarially-trained, decorrelated tagger considerably outperforms a conventionally trained neural network, despite having a slightly worse signal-background separation power. We generalize the adversarial training technique to include a parametric dependence on the signal hypothesis, training a single network that provides optimized, interpolatable decorrelated jet tagging across a continuous range of hypothetical resonance masses, after training on discrete choices of the signal mass.},
archivePrefix = {arXiv},
arxivId = {1703.03507},
author = {Shimmin, Chase and Sadowski, Peter and Baldi, Pierre and Weik, Edison and Whiteson, Daniel and Goul, Edward and S{\o}gaard, Andreas},
doi = {10.1103/PhysRevD.96.074034},
eprint = {1703.03507},
issn = {24700029},
journal = {Physical Review D},
title = {{Decorrelated jet substructure tagging using adversarial neural networks}},
year = {2017}
}
@article{Bergstra2011,
abstract = {Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel ap-proaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it pos-sible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neu-ral networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the ex-pected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreli-able for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [1] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Bergstra, James and Bardenet, R{\'{e}}mi and Bengio, Yoshua and K{\'{e}}gl, Bal{\'{a}}zs},
doi = {2012arXiv1206.2944S},
eprint = {1206.2944},
file = {:home/estrade/Documents/Articles/4443-algorithms-for-hyper-parameter-optimization.pdf:pdf},
isbn = {9781618395993},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems (NIPS)},
pages = {2546--2554},
pmid = {9377276},
title = {{Algorithms for Hyper-Parameter Optimization}},
year = {2011}
}
@article{arjovsky_wasserstein_2017,
abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
annote = {arXiv: 1701.07875},
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'{e}}on},
journal = {arXiv:1701.07875 [cs, stat]},
keywords = {Computer Science - Learning,Statistics - Machine Learning},
title = {{Wasserstein {GAN}}},
url = {http://arxiv.org/abs/1701.07875},
year = {2017}
}
@misc{HiggsMLData,
author = {Collaboration, ATLAS},
doi = {10.7483/OPENDATA.ATLAS.ZBP2.M5T8CERN},
howpublished = {\url{http://opendata.cern.ch/record/328}},
title = {{Dataset from the ATLAS Higgs Boson Machine Learning Challenge 2014}},
year = {2014}
}
@inproceedings{Bromley1993,
author = {Bromley, Jane and Guyon, Isabelle and LeCun, Yann and S{\"{a}}ckinger, Eduard and Shah, Roopak},
booktitle = {Proceedings of the 6th International Conference on Neural Information Processing Systems},
pages = {737--744},
title = {{Signature Verification Using a "Siamese" Time Delay Neural Network}},
year = {1993}
}
@article{Pan2010,
author = {Pan, Sinno Jialin and Yang, Qiang},
journal = {IEEE Trans. on Knowl. and Data Eng.},
number = {10},
pages = {1345--1359},
title = {{A Survey on Transfer Learning}},
volume = {22},
year = {2010}
}
@article{Aad2015,
author = {Aad, G and {et al. [The Atlas Collaboration]}},
journal = {Journal of High Energy Physics},
number = {4},
pages = {117},
title = {{Evidence for the Higgs-boson Yukawa coupling to tau leptons with the ATLAS detector}},
volume = {2015},
year = {2015}
}
@article{Dempster,
author = {Dempster, A P and Schatzoff, M},
journal = {Journal of the American Statistical Association},
number = {310},
pages = {420--436},
title = {{Expected Significance Level as a Sensitivity Index for Test Statistics}},
volume = {60},
year = {1965}
}
@inproceedings{Liu2014,
author = {Liu, Anqi and Ziebart, Brian D},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {37--45},
title = {{Robust Classification Under Sample Selection Bias}},
year = {2014}
}
@inproceedings{Viola2001,
abstract = {This paper develops a new approach for extremely fast detection in\ndomains where the distribution of positive and negative examples\nis highly skewed (e.g. face detection or database retrieval). In\nsuch domains a cascade of simple classifiers each trained to achieve\nhigh detection rates and modest false positive rates can yield a\nfinal detector with many desirable features: including high detection\nrates, very low false positive rates, and fast performance. Achieving\nextremely high detection rates, rather than low error, is not a task\ntypically addressed by machine learning algorithms. We propose a\nnew variant of AdaBoost as a mechanism for training the simple classifiers\nused in the cascade. Experimental results in the domain of face detection\nshow the training algorithm yields significant improvements in performance\nover conventional AdaBoost. The final face detection system can process\n15 frames per second, achieves over 90% detection, and a false positive\nrate of 1 in a 1,000,000.},
author = {Viola, Paul and Jones, Michael},
booktitle = {Advances in Neural Information Processing Systems 14},
isbn = {1049-5258},
title = {{Fast and Robust Classification using Asymmetric AdaBoost and a Detector Cascade}},
year = {2001}
}
@article{Metodiev2017,
abstract = {Modern machine learning techniques can be used to construct powerful models for difficult collider physics problems. In many applications, however, these models are trained on imperfect simulations due to a lack of truth-level information in the data, which risks the model learning artifacts of the simulation. In this paper, we introduce the paradigm of classification without labels (CWoLa) in which a classifier is trained to distinguish statistical mixtures of classes, which are common in collider physics. Crucially, neither individual labels nor class proportions are required, yet we prove that the optimal classifier in the CWoLa paradigm is also the optimal classifier in the traditional fully-supervised case where all label information is available. After demonstrating the power of this method in an analytical toy example, we consider a realistic benchmark for collider physics: distinguishing quark- versus gluon-initiated jets using mixed quark/gluon training samples. More generally, CWoLa can be applied to any classification problem where labels or class proportions are unknown or simulations are unreliable, but statistical mixtures of the classes are available.},
archivePrefix = {arXiv},
arxivId = {1708.02949},
author = {Metodiev, Eric M. and Nachman, Benjamin and Thaler, Jesse},
doi = {10.1007/JHEP10(2017)174},
eprint = {1708.02949},
issn = {10298479},
journal = {Journal of High Energy Physics},
keywords = {Jets},
title = {{Classification without labels: learning from mixed samples in high energy physics}},
year = {2017}
}
@inproceedings{Courty14,
author = {Courty, Nicolas and Flamary, R{\'{e}}mi and Tuia, Devis},
booktitle = {ECML/PKDD 2014},
pages = {1--16},
series = {LNCS},
title = {{Domain adaptation with regularized optimal transport}},
year = {2014}
}
@article{arjovsky_wasserstein_2017,
abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
annote = {arXiv: 1701.07875},
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'{e}}on},
journal = {arXiv:1701.07875 [cs, stat]},
keywords = {Computer Science - Learning,Statistics - Machine Learning},
title = {{Wasserstein {GAN}}},
url = {http://arxiv.org/abs/1701.07875},
year = {2017}
}
@article{Barlow,
author = {Barlow, R},
journal = {ArXiv High Energy Physics - Experiment e-prints},
title = {{Systematic Errors: facts and fictions}},
year = {2002}
}
@article{arjovsky_wasserstein_2017,
abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
annote = {arXiv: 1701.07875},
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'{e}}on},
journal = {arXiv:1701.07875 [cs, stat]},
keywords = {Computer Science - Learning,Statistics - Machine Learning},
title = {{Wasserstein {GAN}}},
url = {http://arxiv.org/abs/1701.07875},
year = {2017}
}
@article{arjovsky_wasserstein_2017,
abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
annote = {arXiv: 1701.07875},
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'{e}}on},
journal = {arXiv:1701.07875 [cs, stat]},
keywords = {Computer Science - Learning,Statistics - Machine Learning},
title = {{Wasserstein {GAN}}},
url = {http://arxiv.org/abs/1701.07875},
year = {2017}
}
@article{t-sne,
abstract = {We present a new technique called {\textquotedblleft}t-SNE{\textquotedblright} that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces signi&$#$64257;cantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large datasets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to in&$#$64258;uence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of datasets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are signi&$#$64257;cantly better than those produced by the other techniques on almost all of the datasets.},
author = {van der Maaten, Laurens and Hinton, Geoffrey E},
journal = {Journal of Machine Learning Research},
pages = {2579--2605},
title = {{Visualizing High-Dimensional Data Using t-SNE}},
volume = {9},
year = {2008}
}
@article{goodfellow_generative_2014,
abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
annote = {arXiv: 1406.2661},
author = {Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
journal = {arXiv:1406.2661 [cs, stat]},
keywords = {Computer Science - Learning,Statistics - Machine Learning},
title = {{Generative {Adversarial} {Networks}}},
url = {http://arxiv.org/abs/1406.2661},
year = {2014}
}
@inproceedings{simard_tangent_1991,
author = {Simard, Patrice Y and Victorri, Bernard and LeCun, Yann and Denker, John S},
booktitle = {NIPS},
editor = {Moody, John E and Hanson, Stephen Jose and Lippmann, Richard},
isbn = {1-55860-222-4},
keywords = {dblp},
pages = {895--903},
publisher = {Morgan Kaufmann},
title = {{Tangent {Prop} - {A} {Formalism} for {Specifying} {Selected} {Invariances} in an {Adaptive} {Network}.}},
url = {http://dblp.uni-trier.de/db/conf/nips/nips1991.html#SimardVLD91},
year = {1991}
}
@article{goodfellow_nips_2016,
abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
annote = {Comment: v2 and v3 are both typo fixes. No substantive changes relative to v1
arXiv: 1701.00160},
author = {Goodfellow, Ian},
journal = {arXiv:1701.00160 [cs]},
keywords = {Computer Science - Learning},
shorttitle = {{NIPS} 2016 {Tutorial}},
title = {{{NIPS} 2016 {Tutorial}: {Generative} {Adversarial} {Networks}}},
url = {http://arxiv.org/abs/1701.00160},
year = {2016}
}
@article{spectre,
address = {Cambridge, MA, USA},
author = {Belkin, Mikhail and Niyogi, Partha},
doi = {10.1162/089976603321780317},
issn = {0899-7667},
journal = {Neural Comput.},
number = {6},
pages = {1373--1396},
publisher = {MIT Press},
title = {{Laplacian Eigenmaps for Dimensionality Reduction and Data Representation}},
url = {http://dx.doi.org/10.1162/089976603321780317},
volume = {15},
year = {2003}
}
@article{baldi_searching_2014,
abstract = {Collisions at high-energy particle colliders are a traditionally fruitful source of exotic particle discoveries. Finding these rare particles requires solving difficult signal-versus-background classification problems, hence machine learning approaches are often used. Standard approaches have relied on `shallow' machine learning models that have a limited capacity to learn complex non-linear functions of the inputs, and rely on a pain-staking search through manually constructed non-linear features. Progress on this problem has slowed, as a variety of techniques have shown equivalent performance. Recent advances in the field of deep learning make it possible to learn more complex functions and better discriminate between signal and background classes. Using benchmark datasets, we show that deep learning methods need no manually constructed inputs and yet improve the classification metric by as much as 8{\textbackslash}% over the best current approaches. This demonstrates that deep learning approaches can improve the power of collider searches for exotic particles.},
annote = {Comment: Accepted by Nature Communications. Added link to deep learning code
arXiv: 1402.4735},
author = {Baldi, Pierre and Sadowski, Peter and Whiteson, Daniel},
doi = {10.1038/ncomms5308},
issn = {2041-1723},
journal = {Nature Communications},
keywords = {High Energy Physics - Experiment,High Energy Physics - Phenomenology},
title = {{Searching for {Exotic} {Particles} in {High}-{Energy} {Physics} with {Deep} {Learning}}},
url = {http://arxiv.org/abs/1402.4735},
volume = {5},
year = {2014}
}
@article{tang_tutorial_2009,
author = {Tang, Yichuan},
title = {{Tutorial on {Tangent} {Propagation}}},
url = {http://www.compneuro.uwaterloo.ca/files/tangent_prop.pdf},
year = {2009}
}
@inproceedings{rifai_manifold_2011,
author = {Rifai, Salah and Dauphin, Yann and Vincent, Pascal and Bengio, Yoshua and Muller, Xavier},
booktitle = {NIPS},
pages = {523},
title = {{The {Manifold} {Tangent} {Classifier}.}},
url = {https://papers.nips.cc/paper/4409-the-manifold-tangent-classifier.pdf},
volume = {271},
year = {2011}
}
@incollection{simard_transformation_1998,
annote = {http://research.microsoft.com/ patrice/PDF/tricks.pdf},
author = {Simard, Patrice Y and {Le Cun}, Yann A and Denker, John S and Victorri, Bernard},
booktitle = {Neural {Networks}: {Tricks} on the {Trade}},
keywords = {Invariance par transformation,Neural networks,Pattern recognition,Reconnaissance de forme,R{\'{e}}seaux connexionnistes,Tangent vectors,Transformation invariance,Vecteurs tangents},
pages = {239--274},
publisher = {Springer-Verlag},
series = {Lectures {Notes} in {Computer} {Science}},
title = {{Transformation invariance in pattern recognition - tangent distance and tangent propagation}},
url = {https://halshs.archives-ouvertes.fr/halshs-00009505},
volume = {1524},
year = {1998}
}
@inproceedings{adam-bourdarios_higgs_2014,
author = {Adam-Bourdarios, Claire and Cowan, Glen and Germain, C{\'{e}}cile and Guyon, Isabelle and K{\'{e}}gl, Bal{\'{a}}zs and Rousseau, David},
booktitle = {{HEPML}@ {NIPS}},
pages = {19--55},
title = {{The {Higgs} boson machine learning challenge.}},
url = {http://www.jmlr.org/proceedings/papers/v42/cowa14.pdf},
year = {2014}
}
@proceedings{pan_cross_2010,
abstract = {

Sentiment classification aims to automatically predict sentiment polarity (e.g., positive or negative) of users publishing sentiment data (e.g., reviews, blogs). Although traditional classification algorithms can be used to train sentiment classifiers from manually labeled text data, the labeling work can be time-consuming and expensive. Meanwhile, users often use some different words when they express sentiment in different domains. If we directly apply a classifier trained in one domain to other domains, the performance will be very low due to the differences between these domains. In this work, we develop a general solution to sentiment classification when we do not have any labels in a target domain but have some labeled data in a different domain, regarded as source domain. In this cross-domain sentiment classification setting, to bridge the gap between the domains, we propose a spectral feature alignment (SFA) algorithm to align domain-specific words from different domains into unified clusters, with the help of domainindependent words as a bridge. In this way, the clusters can be used to reduce the gap between domain-specific words of the two domains, which can be used to train sentiment classifiers in the target domain accurately. Compared to previous approaches, SFA can discover a robust representation for cross-domain data by fully exploiting the relationship between the domain-specific and domainindependent words via simultaneously co-clustering them in a common latent space. We perform extensive experiments on two real world datasets, and demonstrate that SFA significantly outperforms previous approaches to cross-domain sentiment classification.


},
author = {Pan, Sinno Jialin and Ni, Xiaochuan and Sun, Jian-Tao and Yang, Qiang and Chen, Zheng},
booktitle = {The 19th International World Wide Web Conference (WWW-10)},
publisher = {Association for Computing Machinery, Inc.},
title = {{Cross-Domain Sentiment Classification via Spectral Feature Alignment}},
url = {https://www.microsoft.com/en-us/research/publication/cross-domain-sentiment-classification-via-spectral-feature-alignment/},
year = {2010}
}
@article{louppe_learning_2016,
abstract = {Many inference problems involve data generation processes that are not uniquely specified or are uncertain in some way. In a scientific context, the presence of several plausible data generation processes is often associated to the presence of systematic uncertainties. Robust inference is possible if it is based on a pivot -- a quantity whose distribution is invariant to the unknown value of the (categorical or continuous) nuisance parameters that parametrizes this family of generation processes. In this work, we introduce a flexible training procedure based on adversarial networks for enforcing the pivotal property on a predictive model. We derive theoretical results showing that the proposed algorithm tends towards a minimax solution corresponding to a predictive model that is both optimal and independent of the nuisance parameters (if that models exists) or for which one can tune the trade-off between power and robustness. Finally, we demonstrate the effectiveness of this approach with a toy example and an example from particle physics.},
annote = {Comment: v1: Original submission. v2: Fixed references. Code available at https://github.com/glouppe/paper-learning-to-pivot
arXiv: 1611.01046},
author = {Louppe, Gilles and Kagan, Michael and Cranmer, Kyle},
journal = {arXiv:1611.01046 [physics, stat]},
keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Comput,Physics - Data Analysis,Statistics - Machine Learning,Statistics - Methodology,Statistics and Probability},
title = {{Learning to {Pivot} with {Adversarial} {Networks}}},
url = {http://arxiv.org/abs/1611.01046},
year = {2016}
}
@article{victorri_tangent_nodate,
author = {Victorri, Bernard and {Le Cun}, Yann and Denker, John},
title = {{Tangent {Prop}-{A} formalism for specifying selected invariances in an adaptive network {Patrice} {Simard} {AT}\&{T} {Bell} {Laboratories} 101 {Crawford} {Corner} {Rd}}}
}
@article{DBLP:journals/corr/LouppeKC16,
author = {Louppe, Gilles and Kagan, Michael and Cranmer, Kyle},
journal = {CoRR},
title = {{Learning to Pivot with Adversarial Networks}},
url = {http://arxiv.org/abs/1611.01046},
volume = {abs/1611.0},
year = {2016}
}
@inproceedings{melis_dissecting_2014,
author = {Melis, G{\'{a}}bor},
booktitle = {{HEPML}@ {NIPS}},
pages = {57--67},
title = {{Dissecting the {Winning} {Solution} of the {HiggsML} {Challenge}.}},
url = {http://www.jmlr.org/proceedings/papers/v42/meli14.pdf},
year = {2014}
}
@article{arjovsky_wasserstein_2017,
abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
annote = {arXiv: 1701.07875},
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'{e}}on},
journal = {arXiv:1701.07875 [cs, stat]},
keywords = {Computer Science - Learning,Statistics - Machine Learning},
title = {{Wasserstein {GAN}}},
url = {http://arxiv.org/abs/1701.07875},
year = {2017}
}
@article{journals/corr/KingmaB14,
author = {Kingma, Diederik P and Ba, Jimmy},
journal = {CoRR},
keywords = {dblp},
title = {{Adam: A Method for Stochastic Optimization.}},
url = {http://dblp.uni-trier.de/db/journals/corr/corr1412.html#KingmaB14},
volume = {abs/1412.6},
year = {2014}
}
@article{Csurka2017,
abstract = {The aim of this paper is to give an overview of domain adaptation and transfer learning with a specific view on visual applications. After a general motivation, we first position domain adaptation in the larger transfer learning problem. Second, we try to address and analyze briefly the state-of-the-art methods for different types of scenarios, first describing the historical shallow methods, addressing both the homogeneous and the heterogeneous domain adaptation methods. Third, we discuss the effect of the success of deep convolutional architectures which led to new type of domain adaptation methods that integrate the adaptation within the deep architecture. Fourth, we overview the methods that go beyond image categorization, such as object detection or image segmentation, video analyses or learning visual attributes. Finally, we conclude the paper with a section where we relate domain adaptation to other machine learning solutions.},
archivePrefix = {arXiv},
arxivId = {arXiv:1702.05374v2},
author = {Csurka, Gabriela},
eprint = {arXiv:1702.05374v2},
file = {:home/estrade/T{\'{e}}l{\'{e}}chargements/1702.05374.pdf:pdf},
pages = {1--46},
title = {{Domain Adaptation for Visual Applications: A Comprehensive Survey}},
year = {2017}
}
@article{Arjovsky2017,
abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
archivePrefix = {arXiv},
arxivId = {1701.07875},
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'{e}}on},
eprint = {1701.07875},
file = {:home/estrade/T{\'{e}}l{\'{e}}chargements/TOREAD/1701.07875.pdf:pdf},
isbn = {1406.2661},
issn = {1701.07875},
title = {{Wasserstein GAN}},
url = {http://arxiv.org/abs/1701.07875},
year = {2017}
}
@article{Whitney2016,
abstract = {Representation learning is the foundation for the recent success of neural network models. However, the distributed representations generated by neural networks are far from ideal. Due to their highly entangled nature, they are di cult to reuse and interpret, and they do a poor job of capturing the sparsity which is present in real- world transformations. In this paper, I describe methods for learning disentangled representations in the two domains of graphics and computation. These methods allow neural methods to learn representations which are easy to interpret and reuse, yet they incur little or no penalty to performance. In the Graphics section, I demonstrate the ability of these methods to infer the generating parameters of images and rerender those images under novel conditions. In the Computation section, I describe a model which is able to factorize a multitask learning problem into subtasks and which experiences no catastrophic forgetting. Together these techniques provide the tools to design a wide range of models that learn disentangled representations and better model the factors of variation in the real world.},
archivePrefix = {arXiv},
arxivId = {1602.02383},
author = {Whitney, William},
eprint = {1602.02383},
file = {:home/estrade/T{\'{e}}l{\'{e}}chargements/TOREAD/1602.02383.pdf:pdf},
number = {2013},
title = {{Disentangled Representations in Neural Models}},
url = {http://arxiv.org/abs/1602.02383},
year = {2016}
}
@article{Bengio2007,
abstract = {One long-term goal of machine learning research is to produce methods that are applicable to highly complex tasks, such as perception (vision, audition), rea- soning, intelligent control, and other artificially intelligent behaviors. We argue that in order to progress toward this goal, the Machine Learning community must endeavor to discover algorithms that can learn highly complex functions, withmin- imal need for prior knowledge, and with minimal human intervention. We present mathematical and empirical evidence suggesting that many popular approaches to non-parametric learning, particularly kernel methods, are fundamentally lim- ited in their ability to learn complex high-dimensional functions. Our analysis focuses on two problems. First, kernel machines are shallow architectures, in which one large layer of simple template matchers is followed by a single layer of trainable coefficients. We argue that shallow architectures can be very ineffi- cient in terms of required number of computational elements and examples. Sec- ond, we analyze a limitation of kernel machines with a local kernel, linked to the curse of dimensionality, that applies to supervised, unsupervised (manifold learn- ing) and semi-supervised kernel machines. Using empirical results on invariant image recognition tasks, kernel methods are compared with deep architectures, in which lower-level features or concepts are progressively combined into more ab- stract and higher-level representations. We argue that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bengio, Yoshua and {LeCun}, Yann and Lecun, Yann},
doi = {10.1.1.72.4580},
eprint = {arXiv:1011.1669v3},
file = {:home/estrade/T{\'{e}}l{\'{e}}chargements/TOREAD/bengio+lecun-chapter2007.pdf:pdf},
isbn = {1002620262},
issn = {00099104},
journal = {Large Scale Kernel Machines},
number = {1},
pages = {321--360},
pmid = {11359439},
title = {{Scaling Learning Algorithms towards AI}},
year = {2007}
}
@article{Hinton2015,
abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
archivePrefix = {arXiv},
arxivId = {1503.02531},
author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
doi = {10.1063/1.4931082},
eprint = {1503.02531},
file = {:home/estrade/T{\'{e}}l{\'{e}}chargements/TOREAD/1503.02531.pdf:pdf},
isbn = {3531207857},
issn = {0022-2488},
pages = {1--9},
pmid = {18249735},
title = {{Distilling the Knowledge in a Neural Network}},
url = {http://arxiv.org/abs/1503.02531},
year = {2015}
}
@article{lecun-mnisthandwrittendigit-2010,
author = {LeCun, Yann and Cortes, Corinna},
howpublished = {http://yann.lecun.com/exdb/mnist/},
keywords = {MSc _checked character_recognition mnist network n},
title = {{{MNIST} handwritten digit database}},
url = {http://yann.lecun.com/exdb/mnist/},
year = {2010}
}
@article{Simard2012,
abstract = {Stereotactic radiosurgery (SR) is a standard therapy for brain metastases. Radiation necrosis (RN) of the brain is a syndrome of brain coagulative and fibrinoid necrosis and cortical irritation that occurs following radiotherapy. RN following SR peaks in a delayed fashion at 9-12 months postprocedure. Vasogenic cerebral edema secondary to necrosis occurs and can affect surrounding brain function. No definitive non-invasive diagnostic study exists to differentiate post-SR RN from recurrent metastatic tumor. Magnetic resonance (MR) imaging, MR spectroscopy, positron emission tomography, and perfusion-weighted MR imaging have been used to evaluate RN and are discussed. Treatment options for post-SR brain metastases include observation, corticosteroids, pentoxifylline and vitamin E, bevacizumab, radiotherapy, laser-interstitial thermal therapy, and surgical resection.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Simard, Patrice Y. and Lecun, Yann A. and Denker, John S. and Victorri, Bernard},
doi = {10.1007/978-3-642-35289-8-17},
eprint = {arXiv:1011.1669v3},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Simard et al. - 2012 - Transformation invariance in pattern recognition - Tangent distance and tangent propagation.pdf:pdf},
isbn = {9783642352881},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {235--269},
pmid = {25600555},
title = {{Transformation invariance in pattern recognition - Tangent distance and tangent propagation}},
volume = {7700 LECTU},
year = {2012}
}
@article{JMLR:v17:15-239,
archivePrefix = {arXiv},
arxivId = {1505.07818},
author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
doi = {10.1007/978-3-319-58347-1_10},
eprint = {1505.07818},
isbn = {15324435},
issn = {1475-7516},
journal = {Journal of Machine Learning Research},
number = {59},
pages = {189--209},
title = {{Domain-Adversarial Training of Neural Networks}},
url = {http://link.springer.com/10.1007/978-3-319-58347-1_10},
volume = {17},
year = {2017}
}
@article{Glorot2010,
abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ï¬rst observe the inï¬uence of the non-linear activations functions. We ï¬nd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ï¬nd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ï¬nd that a new non-linearity that saturates less can often be beneï¬cial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difï¬cult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
author = {Glorot, Xavier and Bengio, Yoshua},
doi = {10.1.1.207.2059},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Glorot, Bengio - 2010 - Understanding the difficulty of training deep feedforward neural networks.pdf:pdf},
issn = {15324435},
journal = {Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)},
pages = {249--256},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
url = {http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GlorotB10.pdf},
volume = {9},
year = {2010}
}
@article{He2015,
abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra com-putational cost and little overfitting risk. Second, we de-rive a robust initialization method that particularly consid-ers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classifica-tion dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [29]). To our knowledge, our result is the first to surpass human-level per-formance (5.1%, [22]) on this visual recognition challenge.},
archivePrefix = {arXiv},
arxivId = {1502.01852},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/ICCV.2015.123},
eprint = {1502.01852},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classification.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {15505499},
journal = {arXiv preprint},
pages = {1--11},
title = {{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}},
year = {2015}
}
@article{Zhong,
author = {Zhong, Erheng and Fan, Wei and Yang, Qiang and Verscheure, Olivier and Ren, Jiangtao},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhong et al. - 2010 - Cross validation framework to choose amongst models and datasets for transfer learning.pdf:pdf},
journal = {Machine Learning and Knowledge Discovery in Databases},
pages = {547--562},
title = {{Cross validation framework to choose amongst models and datasets for transfer learning}},
year = {2010}
}
@article{Ganin15,
author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
journal = {Journal of Machine Leanring Research},
title = {{Domain-Adversarial Training of Neural Networks}},
year = {2015}
}
@article{Chen2012,
abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:0711.0189v1},
author = {{Von Luxburg}, Ulrike},
doi = {10.1007/s11222-007-9033-z},
eprint = {arXiv:0711.0189v1},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Von Luxburg - 2007 - A tutorial on spectral clustering.pdf:pdf},
isbn = {0960-3174},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {Graph Laplacian,Spectral clustering},
number = {4},
pages = {395--416},
pmid = {19784854},
title = {{A tutorial on spectral clustering}},
volume = {17},
year = {2007}
}
@article{Gatys2015,
abstract = {Here we introduce a new model of natural textures based on the feature spaces of convolutional neural networks optimised for object recognition. Samples from the model are of high perceptual quality demonstrating the generative power of neural networks trained in a purely discriminative fashion. Within the model, textures are represented by the correlations between feature maps in several layers of the network. We show that across layers the texture representations increasingly capture the statistical properties of natural images while making object information more and more explicit. The model provides a new tool to generate stimuli for neuroscience and might offer insights into the deep representations learned by convolutional neural networks.},
archivePrefix = {arXiv},
arxivId = {1505.07376},
author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
doi = {10.1109/CVPR.2016.265},
eprint = {1505.07376},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gatys, Ecker, Bethge - 2015 - Texture Synthesis Using Convolutional Neural Networks.pdf:pdf},
issn = {10495258},
journal = {Nips},
pages = {1--10},
title = {{Texture Synthesis Using Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1505.07376},
year = {2015}
}
@article{Dosovitskiy2015,
abstract = {We train a generative convolutional neural network which is able to generate images of objects given object type, viewpoint, and color. We train the network in a supervised manner on a dataset of rendered 3D chair models. Our experiments show that the network does not merely learn all images by heart, but rather finds a meaningful representation of a 3D chair model allowing it to assess the similarity of different chairs, interpolate between given viewpoints to generate the missing ones, or invent new chair styles by interpolating between chairs from the training set. We show that the network can be used to find correspondences between different chairs from the dataset, outperforming existing approaches on this task.},
archivePrefix = {arXiv},
arxivId = {1411.5928},
author = {Dosovitskiy, Alexey and Springenberg, Jost Tobias and Brox, Thomas},
doi = {10.1109/CVPR.2015.7298761},
eprint = {1411.5928},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dosovitskiy, Springenberg, Brox - 2015 - Learning to generate chairs with convolutional neural networks.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {1538--1546},
pmid = {25246403},
title = {{Learning to generate chairs with convolutional neural networks}},
volume = {07-12-June},
year = {2015}
}
@article{Erhan2009,
abstract = {Deep architectures have demonstrated state-of-the-art results in a variety of settings, especially with vision datasets. Beyond the model definitions and the quantitative analyses, there is a need for qualitative comparisons of the solutions learned by various deep architectures. The goal of this paper is to find good qualitative interpretations of high level features represented by such models. To this end, we contrast and compare several techniques applied on Stacked Denoising Autoencoders and Deep Belief Networks, trained on several vision datasets. We show that, perhaps counter-intuitively, such interpretation is possible at the unit level, that it is simple to accomplish and that the results are consistent across various techniques. We hope that such techniques will allow researchers in deep architectures to understand more of how and why deep architectures work},
author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Erhan et al. - 2009 - Visualizing higher-layer features of a deep network.pdf:pdf},
journal = {Bernoulli},
number = {1341},
pages = {1--13},
title = {{Visualizing higher-layer features of a deep network}},
url = {http://igva2012.wikispaces.asu.edu/file/view/Erhan+2009+Visualizing+higher+layer+features+of+a+deep+network.pdf},
year = {2009}
}
@article{Nguyen2015,
abstract = {Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between com- puter and human vision. A recent study [30] revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the- art DNNs believe to be recognizable objects with 99.99% confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neu- ral networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possi- ble to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call âfooling imagesâ (more generally, fooling ex- amples). Our results shed light on interesting differences between human vision and current DNNs, and raise ques- tions about the generality of DNN computer vision},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.1897v4},
author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
doi = {10.1109/CVPR.2015.7298640},
eprint = {arXiv:1412.1897v4},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen, Yosinski, Clune - 2015 - Deep neural networks are easily fooled High confidence predictions for unrecognizable images.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {427--436},
title = {{Deep neural networks are easily fooled: High confidence predictions for unrecognizable images}},
url = {http://arxiv.org/abs/1412.1897},
volume = {07-12-June},
year = {2015}
}
@article{Simonyan2014,
abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
archivePrefix = {arXiv},
arxivId = {1312.6034},
author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
eprint = {1312.6034},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan, Vedaldi, Zisserman - 2014 - Deep Inside Convolutional Networks Visualising Image Classification Models and Saliency Maps.pdf:pdf},
journal = {Iclr},
pages = {1--},
title = {{Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps}},
url = {http://arxiv.org/abs/1312.6034},
year = {2014}
}
@article{Villani2008,
author = {Villani, C{\'{e}}dric},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Villani - 2008 - Optimal Transport Old and New (Grundlehren der mathematischen Wissenschaften).pdf:pdf},
isbn = {3540710493},
pages = {978},
title = {{Optimal Transport: Old and New (Grundlehren der mathematischen Wissenschaften)}},
url = {http://www.amazon.com/Optimal-Transport-Grundlehren-mathematischen-Wissenschaften/dp/3540710493},
year = {2008}
}
@article{Solomon2015,
author = {Solomon, Justin and de Goes, Fernando and Peyr{\'{e}}, Gabriel and Cuturi, Marco and Butscher, Adrian and Nguyen, Andy and Du, Tao and Guibas, Leonidas},
doi = {10.1145/2766963},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Solomon et al. - 2015 - Convolutional Wasserstein Distances.pdf:pdf},
isbn = {9781450333313},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {displacement interpolation,distributions,entropy,optimal transportation,posed and analyzed,these computations are commonly,wasserstein distances,within the theory of},
number = {4},
pages = {66:1--66},
title = {{Convolutional Wasserstein Distances}},
url = {http://dl.acm.org/citation.cfm?doid=2809654.2766963},
volume = {34},
year = {2015}
}
@article{Ferradans2013,
abstract = {This article introduces a generalization of the discrete optimal transport, with applications to color image manipulations. This new formulation includes a relaxation of the mass conservation constraint and a regularization term. These two features are crucial for image processing tasks, which necessitate to take into account families of multimodal histograms, with large mass variation across modes. The corresponding relaxed and regularized transportation problem is the solution of a convex optimization problem. Depending on the regularization used, this minimization can be solved using standard linear programming methods or first order proximal splitting schemes. The resulting transportation plan can be used as a color transfer map, which is robust to mass variation across images color palettes. Furthermore, the regularization of the transport plan helps to remove colorization artifacts due to noise amplification. We also extend this framework to the computation of barycenters of distributions. The barycenter is the solution of an optimization problem, which is separately convex with respect to the barycenter and the transportation plans, but not jointly convex. A block coordinate descent scheme converges to a stationary point of the energy. We show that the resulting algorithm can be used for color normalization across several images. The relaxed and regularized barycenter defines a common color palette for those images. Applying color transfer toward this average palette performs a color normalization of the input images.},
archivePrefix = {arXiv},
arxivId = {arXiv:1307.5551v1},
author = {Ferradans, Sira and Papadakis, Nicolas and Rabin, Julien and Peyr??, Gabriel and Aujol, Jean Fran??ois},
doi = {10.1007/978-3-642-38267-3_36},
eprint = {arXiv:1307.5551v1},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ferradans et al. - 2013 - Regularized discrete optimal transport.pdf:pdf},
isbn = {9783642382666},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Optimal Transport,color transfer,convex optimization,manifold learning,proximal splitting,variational regularization},
pages = {428--439},
title = {{Regularized discrete optimal transport}},
volume = {7893 LNCS},
year = {2013}
}
@article{Patrini2014,
author = {Patrini, Giorgio and Nock, Richard and Rivera, Paul and Caetano, Tiberio},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Patrini et al. - 2014 - ( Almost ) No Label No Cry.pdf:pdf},
number = {c},
pages = {1--9},
title = {{( Almost ) No Label No Cry}},
year = {2014}
}
@article{Zhang2013,
author = {Zhang, Kun and Muandet, Krikamol and Wang, Zhikun and Others},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2013 - Domain adaptation under target and conditional shift.pdf:pdf},
journal = {Proceedings of the 30th International Conference on Machine Learning (ICML-13)},
pages = {819--827},
title = {{Domain adaptation under target and conditional shift}},
volume = {28},
year = {2013}
}
@article{Courty2014,
abstract = {We present a new and original method to solve the domain adaptation problem using optimal transport. By searching for the best transportation plan between the probability distribution functions of a source and a target domain, a non-linear and invertible transformation of the learning samples can be estimated. Any standard machine learning method can then be applied on the transformed set, which makes our method very generic. We propose a new optimal transport algorithm that incorporates label information in the optimization: this is achieved by combining an efficient matrix scaling technique together with a majoration of a non-convex regularization term. By using the proposed optimal transport with label regularization, we obtain significant increase in performance compared to the original transport solution. The proposed algorithm is computationally efficient and effective, as illustrated by its evaluation on a toy example and a challenging real life vision dataset, against which it achieves competitive results with respect to state-of-the-art methods.},
author = {Courty, Nicolas and Flamary, R{\'{e}}mi and Tuia, Devis},
doi = {10.1007/978-3-662-44848-9_18},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Courty, Flamary, Tuia - 2014 - Domain adaptation with regularized optimal transport.pdf:pdf},
isbn = {9783662448472},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 1},
pages = {274--289},
title = {{Domain adaptation with regularized optimal transport}},
volume = {8724 LNAI},
year = {2014}
}
@article{Ajakan2015,
archivePrefix = {arXiv},
arxivId = {1505.07818v1},
author = {Ajakan, Hana and Larochelle, Hugo and Marchand, Mario and Lempitsky, Victor},
doi = {10.1088/1475-7516/2015/08/013},
eprint = {1505.07818v1},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ajakan et al. - 2015 - Domain-Adversarial Training of Neural Networks.pdf:pdf},
journal = {arXiv},
pages = {1--35},
title = {{Domain-Adversarial Training of Neural Networks}},
volume = {17},
year = {2015}
}
@article{Goodfellow2014,
abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
archivePrefix = {arXiv},
arxivId = {1412.6572},
author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
eprint = {1412.6572},
file = {:home/estrade/T{\'{e}}l{\'{e}}chargements/TOREAD/1412.6572.pdf:pdf},
isbn = {1412.6572},
issn = {0012-7183},
pages = {1--11},
pmid = {729514},
title = {{Explaining and Harnessing Adversarial Examples}},
url = {http://arxiv.org/abs/1412.6572},
year = {2014}
}
@article{Adam-bourdarios2014,
author = {Adam-bourdarios, Claire and Cowan, Glen and Guyon, Isabelle},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Adam-Bourdarios et al. - 2014 - The Higgs boson machine learning challenge.pdf:pdf},
journal = {NIPS 2014 Workshop on High-energy Physics and Machine Learning},
keywords = {adam-bourdarios,b,c,c 2015 c,cowan,g,germain,guyon,higgs boson,high energy physics,i,k,machine learning,statistical tests},
pages = {19--55},
title = {{Learning to discover the Higgs boson machine learning challenge}},
url = {https://hal.inria.fr/hal-01208587 http://higgsml.lal.in2p3.fr/files/2014/04/documentation%7B_%7Dv1.8.pdf},
volume = {4237},
year = {2014}
}
@article{Cranmer2015,
abstract = {In many fields of science, generalized likelihood ratio tests are established tools for statistical inference. At the same time, it has become increasingly common that a simulator (or generative model) is used to describe complex processes that tie parameters $\theta$ of an underlying theory and measurement apparatus to high-dimensional observations $\mathbf{x}\in \mathbb{R}^p$. However, simulator often do not provide a way to evaluate the likelihood function for a given observation $\mathbf{x}$, which motivates a new class of likelihood-free inference algorithms. In this paper, we show that likelihood ratios are invariant under a specific class of dimensionality reduction maps $\mathbb{R}^p \mapsto \mathbb{R}$. As a direct consequence, we show that discriminative classifiers can be used to approximate the generalized likelihood ratio statistic when only a generative model for the data is available. This leads to a new machine learning-based approach to likelihood-free inference that is complementary to Approximate Bayesian Computation, and which does not require a prior on the model parameters. Experimental results on artificial problems with known exact likelihoods illustrate the potential of the proposed method.},
archivePrefix = {arXiv},
arxivId = {1506.02169},
author = {Cranmer, Kyle and Pavez, Juan and Louppe, Gilles},
eprint = {1506.02169},
file = {:home/estrade/Documents/Articles/1506.02169.pdf:pdf},
pages = {1--35},
title = {{Approximating Likelihood Ratios with Calibrated Discriminative Classifiers}},
url = {http://arxiv.org/abs/1506.02169},
year = {2015}
}
@article{Ben-David2010,
abstract = {Discriminative learning methods for classification perform well when\ntraining and test data are drawn from the same distribution. Often,\nhowever, we have plentiful labeled training data from a source domain\nbut wish to learn a classifier which performs well on a target domain\nwith a different distribution and little or no labeled training data. In\nthis work we investigate two questions. First, under what conditions can\na classifier trained from source data be expected to perform well on\ntarget data? Second, given a small amount of labeled target data, how\nshould we combine it during training with the large amount of labeled\nsource data to achieve the lowest target error at test time?\nWe address the first question by bounding a classifier's target error in\nterms of its source error and the divergence between the two domains. We\ngive a classifier-induced divergence measure that can be estimated from\nfinite, unlabeled samples from the domains. Under the assumption that\nthere exists some hypothesis that performs well in both domains, we show\nthat this quantity together with the empirical source error characterize\nthe target error of a source-trained classifier.\nWe answer the second question by bounding the target error of a model\nwhich minimizes a convex combination of the empirical source and target\nerrors. Previous theoretical work has considered minimizing just the\nsource error, just the target error, or weighting instances from the two\ndomains equally. We show how to choose the optimal combination of source\nand target error as a function of the divergence, the sample sizes of\nboth domains, and the complexity of the hypothesis class. The resulting\nbound generalizes the previously studied cases and is always at least as\ntight as a bound which considers minimizing only the target error or an\nequal weighting of source and target errors.},
author = {Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
doi = {10.1007/s10994-009-5152-4},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ben-David et al. - 2010 - A theory of learning from different domains.pdf:pdf},
isbn = {9780838986219},
issn = {15730565},
journal = {Machine Learning},
keywords = {Domain adaptation,Learning theory,Sample-selection bias,Transfer learning},
number = {1-2},
pages = {151--175},
title = {{A theory of learning from different domains}},
volume = {79},
year = {2010}
}
@article{Baldi2014,
abstract = {The Higgs boson is thought to provide the interaction that imparts mass to the fundamental fermions, but while measurements at the Large Hadron Collider (LHC) are consistent with this hypothesis, current analysis techniques lack the statistical power to cross the traditional 5$\sigma$ significance barrier without more data. \emph{Deep learning} techniques have the potential to increase the statistical power of this analysis by \emph{automatically} learning complex, high-level data representations. In this work, deep neural networks are used to detect the decay of the Higgs to a pair of tau leptons. A Bayesian optimization algorithm is used to tune the network architecture and training algorithm hyperparameters, resulting in a deep network of eight non-linear processing layers that improves upon the performance of shallow classifiers even without the use of features specifically engineered by physicists for this application. The improvement in discovery significance is equivalent to an increase in the accumulated dataset of 25\%.},
archivePrefix = {arXiv},
arxivId = {1410.3469},
author = {Baldi, Pierre and Sadowski, Peter and Whiteson, Daniel},
doi = {10.1103/PhysRevLett.114.111801},
eprint = {1410.3469},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baldi, Sadowski, Whiteson - 2014 - Enhanced Higgs to $tautau-$ Searches with Deep Learning.pdf:pdf},
month = {oct},
title = {{Enhanced Higgs to $\tau^+\tau^-$ Searches with Deep Learning}},
url = {http://arxiv.org/abs/1410.3469 http://dx.doi.org/10.1103/PhysRevLett.114.111801},
year = {2014}
}
@article{Estrade2018,
author = {Estrade, Victor and Guyon, Isabelle and Rousseau, David},
file = {:home/estrade/Documents/Articles/systematics-aware-learning.pdf:pdf},
title = {{Systematics aware learning: a case study in High Energy Physics}},
year = {2018}
}
@article{Rifai2011,
abstract = {We present in this paper a novel approach for training deterministic auto-encoders. We show that by adding a well chosen penalty term to the classical reconstruction cost func-tion, we can achieve results that equal or sur-pass those attained by other regularized auto-encoders as well as denoising auto-encoders on a range of datasets. This penalty term corresponds to the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input. We show that this penalty term results in a localized space contraction which in turn yields robust fea-tures on the activation layer. Furthermore, we show how this penalty term is related to both regularized auto-encoders and denoising auto-encoders and how it can be seen as a link between deterministic and non-deterministic auto-encoders. We find empirically that this penalty helps to carve a representation that better captures the local directions of varia-tion dictated by the data, corresponding to a lower-dimensional non-linear manifold, while being more invariant to the vast majority of directions orthogonal to the manifold. Fi-nally, we show that by using the learned fea-tures to initialize a MLP, we achieve state of the art classification error on a range of datasets, surpassing other methods of pre-training.},
author = {Rifai, Salah and Vincent, Pascal and Muller, Xavier and Glorot, Xavier and Bengio, Yoshua},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rifai et al. - 2011 - Contractive Auto-Encoders Explicit Invariance During Feature Extraction.pdf:pdf},
title = {{Contractive Auto-Encoders: Explicit Invariance During Feature Extraction}},
url = {http://www.icml-2011.org/papers/455_icmlpaper.pdf},
year = {2011}
}
@article{Louppe,
abstract = {Several techniques for domain adaptation have been proposed to account for differences in the distribution of the data used for training and testing. The majority of this work focuses on a binary domain label. Similar problems occur in a scientific context where there may be a continuous family of plausible data generation processes associated to the presence of systematic uncertainties. Robust inference is possible if it is based on a pivot â a quantity whose distribution does not depend on the unknown values of the nuisance parameters that parametrize this family of data generation processes. In this work, we introduce and derive theoretical results for a training procedure based on adversarial networks for enforcing the pivotal property (or, equivalently, fairness with respect to continuous attributes) on a predictive model. The method includes a hyperparameter to control the trade-off between accuracy and robustness. We demonstrate the effectiveness of this approach with a toy example and examples from particle physics.},
author = {Louppe, Gilles and Kagan, Michael and Cranmer, Kyle},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Louppe, Kagan, Cranmer - Unknown - Learning to Pivot with Adversarial Networks.pdf:pdf},
title = {{Learning to Pivot with Adversarial Networks}},
url = {https://arxiv.org/pdf/1611.01046.pdf}
}
@article{Chen2016a,
abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
archivePrefix = {arXiv},
arxivId = {1603.02754},
author = {Chen, Tianqi and Guestrin, Carlos},
doi = {10.1145/2939672.2939785},
eprint = {1603.02754},
file = {:home/estrade/T{\'{e}}l{\'{e}}chargements/1603.02754.pdf:pdf},
isbn = {9781450342322},
issn = {0146-4833},
keywords = {large-scale machine learning},
pmid = {22942019},
title = {{XGBoost: A Scalable Tree Boosting System}},
url = {http://arxiv.org/abs/1603.02754%0Ahttp://dx.doi.org/10.1145/2939672.2939785},
year = {2016}
}
@article{Barlow2002,
abstract = {The treatment of systematic errors is often mishandled. This is due to lack of under-standing and education, based on a fundamental ambiguity as to what is meant by the term. This note addresses the problems and offers guidance to good practice.},
archivePrefix = {arXiv},
arxivId = {arXiv:hep-ex/0207026v1},
author = {Barlow, Roger},
eprint = {0207026v1},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Barlow - 2002 - Systematic Errors Facts and Fictions.pdf:pdf},
primaryClass = {arXiv:hep-ex},
title = {{Systematic Errors: Facts and Fictions}},
year = {2002}
}
@article{Chaudhari2017,
abstract = {This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.},
archivePrefix = {arXiv},
arxivId = {arXiv:1611.01838v5},
author = {Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and Lecun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo and Angeles, Los},
eprint = {arXiv:1611.01838v5},
file = {:home/estrade/T{\'{e}}l{\'{e}}chargements/TOREAD/1611.01838.pdf:pdf},
journal = {Iclr},
pages = {1--19},
title = {{Entropy-SGD: Biasing Gradient Descent Into Wide Valleys}},
year = {2017}
}
@article{Celeux1992,
author = {Celeux, G and Govaert, G},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Celeux, Govaert - 1992 - A Classification {EM} Algorithm and two Stochastic Versions.pdf:pdf},
journal = {Computational Statistics & Data Analysis},
pages = {315--332},
title = {{A Classification {EM} Algorithm and two Stochastic Versions}},
volume = {14},
year = {1992}
}
@article{Ganin2016,
abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.},
archivePrefix = {arXiv},
arxivId = {1505.07818},
author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
doi = {10.1088/1475-7516/2015/08/013},
eprint = {1505.07818},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ganin et al. - 2015 - Domain-Adversarial Training of Neural Networks.pdf:pdf},
isbn = {15324435},
issn = {1475-7516},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,domain adaptation,image classification,neural network,person re-identification,representation learning,sentiment analysis,synthetic data},
mendeley-tags = {domain adaptation},
number = {08},
pages = {013--013},
title = {{Domain-Adversarial Training of Neural Networks}},
url = {http://arxiv.org/abs/1505.07818},
volume = {2015},
year = {2015}
}
@misc{Rumelhart1986,
abstract = {This paper presents a generalization of the perception learning procedure for learning the correct sets of connections for arbitrary networks. The rule, falled the generalized delta rule, is a simple scheme for implementing a gradient descent method for finding weights that minimize the sum squared error of the sytem's performance. The major theoretical contribution of the work is the procedure called error propagation, whereby the gradient can be determined by individual units of the network based only on locally available information. The major empirical contribution of the work is to show that the problem of local minima not serious in this application of gradient descent.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rumelhart, D. E. and Hinton, G. E. and Williams, R. J.},
booktitle = {Readings in Cognitive Science: A Perspective from Psychology and Artificial Intelligence},
doi = {10.1016/B978-1-4832-1446-7.50035-2},
eprint = {arXiv:1011.1669v3},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rumelhart, Hinton, Williams - 1986 - Learning Internal Representations by Error Propagation.pdf:pdf},
isbn = {1558600132},
issn = {1-55860-013-2},
pages = {399--421},
pmid = {25246403},
title = {{Learning Internal Representations by Error Propagation}},
year = {1986}
}
@article{Cuturi2013,
abstract = {Optimal transportation distances are a fundamental family of parameterized distances for histograms. Despite their appealing theoretical properties, excellent performance in retrieval tasks and intuitive formulation, their computation involves the resolution of a linear program whose cost is prohibitive whenever the histograms' dimension exceeds a few hundreds. We propose in this work a new family of optimal transportation distances that look at transportation problems from a maximum-entropy perspective. We smooth the classical optimal transportation problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through Sinkhorn-Knopp's matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transportation solvers. We also report improved performance over classical optimal transportation distances on the MNIST benchmark problem.},
archivePrefix = {arXiv},
arxivId = {arXiv:1306.0895v1},
author = {Cuturi, Marco},
eprint = {arXiv:1306.0895v1},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cuturi - 2013 - Sinkhorn distances Lightspeed computation of optimal transport.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {1--9},
title = {{Sinkhorn distances: Lightspeed computation of optimal transport}},
url = {http://papers.nips.cc/paper/4927-sinkhorn-distances-lightspeed-computation-of-optimal-transport},
year = {2013}
}
@article{Arthur2007,
abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a simple, randomized seeding technique, we obtain an algorithm that is O(log k)-competitive with the optimal clustering. Experiments show our augmentation improves both the speed and the accuracy of k-means, often quite dramatically. 1},
author = {Arthur, David and Vassilvitskii, Sergei},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arthur, Vassilvitskii - 2007 - k-means The Advantages of Careful Seeding.pdf:pdf},
journal = {Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms.},
pages = {1--11},
title = {{k-means++ : The Advantages of Careful Seeding}},
volume = {8},
year = {2007}
}
@article{Courty2015,
abstract = {Domain adaptation from one data space (or domain) to another is one of the most challenging tasks of modern data analytics. If the adaptation is done correctly, models built on a specific data space become more robust when confronted to data depicting the same semantic concepts (the classes), but observed by another observation system with its own specificities. Among the many strategies proposed to adapt a domain to another, finding a common representation has shown excellent properties: by finding a common representation for both domains, a single classifier can be effective in both and use labelled samples from the source domain to predict the unlabelled samples of the target domain. In this paper, we propose a regularized unsupervised optimal transportation model to perform the alignment of the representations in the source and target domains. We learn a transportation plan matching both PDFs, which constrains labelled samples in the source domain to remain close during transport. This way, we exploit at the same time the few labeled information in the source and the unlabelled distributions observed in both domains. Experiments in toy and challenging real visual adaptation examples show the interest of the method, that consistently outperforms state of the art approaches.},
archivePrefix = {arXiv},
arxivId = {1507.00504},
author = {Courty, Nicolas and Flamary, R{\'{e}}mi and Tuia, Devis and Rakotomamonjy, Alain},
eprint = {1507.00504},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Courty et al. - 2015 - Optimal Transport for Domain Adaptation.pdf:pdf},
isbn = {9782875870148},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
keywords = {Machine Learning},
mendeley-tags = {Machine Learning},
number = {X},
pages = {22--24},
title = {{Optimal Transport for Domain Adaptation}},
url = {http://arxiv.org/abs/1507.00504},
volume = {X},
year = {2015}
}
@article{Genevay2016,
abstract = {Optimal transport (OT) defines a powerful framework to compare probability distri-butions in a geometrically faithful way. However, the practical impact of OT is still limited because of its computational burden. We propose a new class of stochastic opti-mization algorithms to cope with large-scale problems routinely encountered in machine learning applications. These methods are able to manipulate arbitrary distributions (either discrete or continuous) by simply requiring to be able to draw samples from them, which is the typical setup in high-dimensional learning problems. This alleviates the need to discretize these densities, while giving access to provably convergent methods that output the correct distance without discretization error. These algorithms rely on two main ideas: (a) the dual OT problem can be re-cast as the maximization of an expectation ; (b) entropic regularization of the primal OT problem results in a smooth dual optimization optimization which can be addressed with algorithms that have a provably faster convergence. We instantiate these ideas in three different setups: (i) when comparing a discrete distribution to another, we show that incremental stochastic optimization schemes can beat Sinkhorn's algorithm, the current state-of-the-art finite dimensional OT solver; (ii) when comparing a discrete distribution to a continuous density, a semi-discrete reformulation of the dual program is amenable to averaged stochastic gradient descent, leading to better performance than approximately solving the problem by discretization ; (iii) when dealing with two continuous densities, we propose a stochastic gradient descent over a reproducing kernel Hilbert space (RKHS). This is currently the only known method to solve this problem, apart from computing OT on finite samples. We backup these claims on a set of discrete, semi-discrete and continuous benchmark problems.},
archivePrefix = {arXiv},
arxivId = {1605.08527},
author = {Genevay, Aude and Cuturi, Marco and Peyr{\'{e}}, Gabriel and Bach, Francis},
eprint = {1605.08527},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Genevay et al. - 2016 - Stochastic Optimization for Large-scale Optimal Transport.pdf:pdf},
title = {{Stochastic Optimization for Large-scale Optimal Transport}},
year = {2016}
}
@article{Knight2008,
abstract = {As long as a square nonnegative matrix A contains sufficient nonzero elements, then the Sinkhorn-Knopp algorithm can be used to balance the matrix, that is, to find a diagonal scaling of A that is doubly stochastic. It is known that the convergence is linear, and an upper bound has been given for the rate of convergence for positive matrices. In this paper we give an explicit expression for the rate of convergence for fully indecomposable matrices. We describe how balancing algorithms can be used to give a measure of web page significance. We compare the measure with some well known alternatives, including PageRank. We show that, with an appropriate modi. cation, the Sinkhorn-Knopp algorithm is a natural candidate for computing the measure on enormous data sets.},
author = {Knight, Philip A.},
doi = {10.1137/060659624},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Knight - 2008 - The Sinkhorn-Knopp algorithm convergence and applications.pdf:pdf},
issn = {0895-4798},
journal = {SIAM J. Matrix Anal Appl.},
keywords = {Probabilities. Mathematical statistics},
number = {1},
pages = {261--275},
title = {{The Sinkhorn-Knopp algorithm: convergence and applications}},
url = {http://dx.doi.org/10.1137/060659624},
volume = {30},
year = {2008}
}
@article{Sinkhorn1967,
author = {Sinkhorn, Richard and Knopp, Paul},
doi = {10.2140/pjm.1967.21.343},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sinkhorn, Knopp - 1967 - Concerning nonnegative matrices and doubly stochastic matrices.pdf:pdf},
issn = {0030-8730},
journal = {Pacific Journal of Mathematics},
number = {2},
pages = {343--348},
title = {{Concerning nonnegative matrices and doubly stochastic matrices.}},
url = {http://projecteuclid.org/euclid.pjm/1102992505},
volume = {21},
year = {1967}
}
