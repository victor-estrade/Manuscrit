% ------------------------------------------------------------------------
% SAMPLE BIBLIOGRAPHY FILE
% ------------------------------------------------------------------------
@inproceedings{estrade:hal-01715155,
address = {Bruges, Belgium},
author = {Estrade, Victor and Germain, C{\'{e}}cile and Guyon, Isabelle and Rousseau, David},
booktitle = {ESANN 2018 - 26th European Symposium on Artificial Neural Networks},
title = {{Systematics aware learning: a case study in High Energy Physics}},
url = {https://hal.inria.fr/hal-01715155},
year = {2018}
}

@conference{EstradeNIPS,
author = {Estrade, Victor and Germain, C{\'{e}}cile and Guyon, Isabelle and Rousseau, David},
booktitle = {Deep Learning for Physical Sciences @ NIPS},
title = {{Adversarial learning to eliminate systematic errors: a case study in HEP}},
year = {2017}
}

@TECHREPORT{Bishop94mixturedensity,
    author = {Christopher M. Bishop},
    title = {Mixture density networks},
    institution = {},
    year = {1994}
}

@article{DECASTRO2019170inferno,
title = "INFERNO: Inference-Aware Neural Optimisation",
journal = "Computer Physics Communications",
volume = "244",
pages = "170 - 179",
year = "2019",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2019.06.007",
url = "http://www.sciencedirect.com/science/article/pii/S0010465519301948",
author = "Pablo de Castro and Tommaso Dorigo",
keywords = "Likelihood-free inference, High energy physics, Neural networks, Nuisance parameters",
abstract = "Complex computer simulations are commonly required for accurate data modelling in many scientific disciplines, making statistical inference challenging due to the intractability of the likelihood evaluation for the observed data. Furthermore, sometimes one is interested on inference drawn over a subset of the generative model parameters while taking into account model uncertainty or misspecification on the remaining nuisance parameters. In this work, we show how non-linear summary statistics can be constructed by minimising inference-motivated losses via stochastic gradient descent such that they provide the smallest uncertainty for the parameters of interest. As a use case, the problem of confidence interval estimation for the mixture coefficient in a multi-dimensional two-component mixture model (i.e. signal vs background) is considered, where the proposed technique clearly outperforms summary statistics based on probabilistic classification, a commonly used alternative which does not account for the presence of nuisance parameters."
}

@inproceedings{Edwards17neuralstatistician,
  title     = "Towards a Neural Statistician",
  abstract  = "An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes. We refer to our model as a neural statistician, and by this we mean a neural network that can learn to compute summary statistics of datasets without supervision.",
  author    = "Harrison Edwards and Amos Storkey",
  year      = "2017",
  month     = "4",
  day       = "26",
  language  = "English",
  booktitle = "5th International Conference on Learning Representations (ICLR 2017)",
}

@article{louppe_learning_2016,
abstract = {Many inference problems involve data generation processes that are not uniquely specified or are uncertain in some way. In a scientific context, the presence of several plausible data generation processes is often associated to the presence of systematic uncertainties. Robust inference is possible if it is based on a pivot -- a quantity whose distribution is invariant to the unknown value of the (categorical or continuous) nuisance parameters that parametrizes this family of generation processes. In this work, we introduce a flexible training procedure based on adversarial networks for enforcing the pivotal property on a predictive model. We derive theoretical results showing that the proposed algorithm tends towards a minimax solution corresponding to a predictive model that is both optimal and independent of the nuisance parameters (if that models exists) or for which one can tune the trade-off between power and robustness. Finally, we demonstrate the effectiveness of this approach with a toy example and an example from particle physics.},
annote = {Comment: v1: Original submission. v2: Fixed references. Code available at https://github.com/glouppe/paper-learning-to-pivot
arXiv: 1611.01046},
author = {Louppe, Gilles and Kagan, Michael and Cranmer, Kyle},
journal = {arXiv:1611.01046 [physics, stat]},
keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Comput,Physics - Data Analysis,Statistics - Machine Learning,Statistics - Methodology,Statistics and Probability},
title = {{Learning to {Pivot} with {Adversarial} {Networks}}},
url = {http://arxiv.org/abs/1611.01046},
year = {2016}
}

@article{Simard2012,
abstract = {Stereotactic radiosurgery (SR) is a standard therapy for brain metastases. Radiation necrosis (RN) of the brain is a syndrome of brain coagulative and fibrinoid necrosis and cortical irritation that occurs following radiotherapy. RN following SR peaks in a delayed fashion at 9-12 months postprocedure. Vasogenic cerebral edema secondary to necrosis occurs and can affect surrounding brain function. No definitive non-invasive diagnostic study exists to differentiate post-SR RN from recurrent metastatic tumor. Magnetic resonance (MR) imaging, MR spectroscopy, positron emission tomography, and perfusion-weighted MR imaging have been used to evaluate RN and are discussed. Treatment options for post-SR brain metastases include observation, corticosteroids, pentoxifylline and vitamin E, bevacizumab, radiotherapy, laser-interstitial thermal therapy, and surgical resection.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Simard, Patrice Y. and Lecun, Yann A. and Denker, John S. and Victorri, Bernard},
doi = {10.1007/978-3-642-35289-8-17},
eprint = {arXiv:1011.1669v3},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Simard et al. - 2012 - Transformation invariance in pattern recognition - Tangent distance and tangent propagation.pdf:pdf},
isbn = {9783642352881},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {235--269},
pmid = {25600555},
title = {{Transformation invariance in pattern recognition - Tangent distance and tangent propagation}},
volume = {7700 LECTU},
year = {2012}
}

@inproceedings{lucas:hal-01791126,
  TITLE = {{Mixed batches and symmetric discriminators for GAN training}},
  AUTHOR = {Lucas, Thomas and Tallec, Corentin and Verbeek, Jakob and Ollivier, Yann},
  URL = {https://hal.inria.fr/hal-01791126},
  BOOKTITLE = {{ICML - 35th International Conference on Machine Learning}},
  ADDRESS = {Stockholm, Sweden},
  SERIES = {Proceedings of Machine Learning Research},
  VOLUME = {80},
  PAGES = {2844-2853},
  YEAR = {2018},
  MONTH = Jul,
  PDF = {https://hal.inria.fr/hal-01791126/file/mixed_batches_sym_disc_for_gans_with_ack%20%281%29.pdf},
  HAL_ID = {hal-01791126},
  HAL_VERSION = {v2},
}

@article{Cowan2011,
abstract = {We describe likelihood-based statistical tests for use in high energy physics for the discovery of new phenomena and for construction of confidence intervals on model parameters. We focus on the properties of the test procedures that allow one to account for systematic uncertainties. Explicit formulae for the asymptotic distributions of test statistics are derived using results of Wilks and Wald. We motivate and justify the use of a representative data set, called the ``Asimov data set'', which provides a simple method to obtain the median experimental sensitivity of a search or measurement as well as fluctuations about this expectation.},
author = {Cowan, Glen and Cranmer, Kyle and Gross, Eilam and Vitells, Ofer},
doi = {10.1140/epjc/s10052-011-1554-0},
issn = {1434-6052},
journal = {The European Physical Journal C},
number = {2},
pages = {1554},
title = {{Asymptotic formulae for likelihood-based tests of new physics}},
url = {https://doi.org/10.1140/epjc/s10052-011-1554-0},
volume = {71},
year = {2011}
}

@inproceedings{Mcgregor_variadic_neural_net,
author = {Mcgregor, Simon},
year = {2007},
month = {09},
pages = {460-470},
title = {Neural Network Processing for Multiset Data},
doi = {10.1007/978-3-540-74690-4_47}
}

@article{Mcgregor_variadic_neural_net_2,
author = {Mcgregor, Simon},
year = {2008},
month = {09},
pages = {830-7},
title = {Further results in multiset processing with neural networks},
volume = {21},
journal = {Neural networks : the official journal of the International Neural Network Society},
doi = {10.1016/j.neunet.2008.06.020}
}


@article{neyman_pearson_1933,
author = {Neyman Jerzy and Pearson Egon Sharpe and Pearson Karl},
year = {1933},
month = {02},
pages = {289-337},
title = {On the problem of the most efficient tests of statistical hypotheses},
volume = {231},
journal = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
doi = {10.1098/rsta.1933.0009}
}

@inproceedings{Neal:2007zz,
    author = "Neal, Radford M.",
    booktitle = "{Statistical issues for LHC physics. Proceedings, Workshop, PHYSTAT-LHC, Geneva, Switzerland, June 27-29, 2007}",
    pages = "111--118",
    title = "{Computing likelihood functions for high-energy physics experiments when distributions are defined by simulators with nuisance parameters}",
    url = "http://cds.cern.ch/record/1099977/files/p111.pdf",
    year = "2007"
}

@article{Adam-Bourdarios2014,
author = {Adam-bourdarios, Claire and Cowan, Glen and Guyon, Isabelle},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Adam-Bourdarios et al. - 2014 - The Higgs boson machine learning challenge.pdf:pdf},
journal = {NIPS 2014 Workshop on High-energy Physics and Machine Learning},
keywords = {adam-bourdarios,b,c,c 2015 c,cowan,g,germain,guyon,higgs boson,high energy physics,i,k,machine learning,statistical tests},
pages = {19--55},
title = {{Learning to discover the Higgs boson machine learning challenge}},
url = {https://hal.inria.fr/hal-01208587 http://higgsml.lal.in2p3.fr/files/2014/04/documentation%7B_%7Dv1.8.pdf},
volume = {4237},
year = {2014}
}

@article{AGOSTINELLI2003250,
title = "Geant4—a simulation toolkit",
journal = "Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment",
volume = "506",
number = "3",
pages = "250 - 303",
year = "2003",
issn = "0168-9002",
doi = "https://doi.org/10.1016/S0168-9002(03)01368-8",
url = "http://www.sciencedirect.com/science/article/pii/S0168900203013688",
author = "S. Agostinelli and J. Allison and K. Amako and J. Apostolakis and H. Araujo and P. Arce and M. Asai and D. Axen and S. Banerjee and G. Barrand and F. Behner and L. Bellagamba and J. Boudreau and L. Broglia and A. Brunengo and H. Burkhardt and S. Chauvie and J. Chuma and R. Chytracek and G. Cooperman and G. Cosmo and P. Degtyarenko and A. Dell'Acqua and G. Depaola and D. Dietrich and R. Enami and A. Feliciello and C. Ferguson and H. Fesefeldt and G. Folger and F. Foppiano and A. Forti and S. Garelli and S. Giani and R. Giannitrapani and D. Gibin and J.J. {Gómez Cadenas} and I. González and G. {Gracia Abril} and G. Greeniaus and W. Greiner and V. Grichine and A. Grossheim and S. Guatelli and P. Gumplinger and R. Hamatsu and K. Hashimoto and H. Hasui and A. Heikkinen and A. Howard and V. Ivanchenko and A. Johnson and F.W. Jones and J. Kallenbach and N. Kanaya and M. Kawabata and Y. Kawabata and M. Kawaguti and S. Kelner and P. Kent and A. Kimura and T. Kodama and R. Kokoulin and M. Kossov and H. Kurashige and E. Lamanna and T. Lampén and V. Lara and V. Lefebure and F. Lei and M. Liendl and W. Lockman and F. Longo and S. Magni and M. Maire and E. Medernach and K. Minamimoto and P. {Mora de Freitas} and Y. Morita and K. Murakami and M. Nagamatu and R. Nartallo and P. Nieminen and T. Nishimura and K. Ohtsubo and M. Okamura and S. O'Neale and Y. Oohata and K. Paech and J. Perl and A. Pfeiffer and M.G. Pia and F. Ranjard and A. Rybin and S. Sadilov and E. {Di Salvo} and G. Santin and T. Sasaki and N. Savvas and Y. Sawada and S. Scherer and S. Sei and V. Sirotenko and D. Smith and N. Starkov and H. Stoecker and J. Sulkimo and M. Takahata and S. Tanaka and E. Tcherniaev and E. {Safai Tehrani} and M. Tropeano and P. Truscott and H. Uno and L. Urban and P. Urban and M. Verderi and A. Walkden and W. Wander and H. Weber and J.P. Wellisch and T. Wenaus and D.C. Williams and D. Wright and T. Yamada and H. Yoshida and D. Zschiesche",
keywords = "Simulation, Particle interactions, Geometrical modelling, Software engineering, Object-oriented technology, Distributed software development",
abstract = "Geant4 is a toolkit for simulating the passage of particles through matter. It includes a complete range of functionality including tracking, geometry, physics models and hits. The physics processes offered cover a comprehensive range, including electromagnetic, hadronic and optical processes, a large set of long-lived particles, materials and elements, over a wide energy range starting, in some cases, from 250eV and extending in others to the TeV energy range. It has been designed and constructed to expose the physics models utilised, to handle complex geometries, and to enable its easy adaptation for optimal use in different sets of applications. The toolkit is the result of a worldwide collaboration of physicists and software engineers. It has been created exploiting software engineering and object-oriented technology and implemented in the C++ programming language. It has been used in applications in particle physics, nuclear physics, accelerator design, space engineering and medical physics."
}

@ARTICLE{1610988,
  author={J. {Allison} and K. {Amako} and J. {Apostolakis} and H. {Araujo} and P. {Arce Dubois} and M. {Asai} and G. {Barrand} and R. {Capra} and S. {Chauvie} and R. {Chytracek} and G. A. P. {Cirrone} and G. {Cooperman} and G. {Cosmo} and G. {Cuttone} and G. G. {Daquino} and M. {Donszelmann} and M. {Dressel} and G. {Folger} and F. {Foppiano} and J. {Generowicz} and V. {Grichine} and S. {Guatelli} and P. {Gumplinger} and A. {Heikkinen} and I. {Hrivnacova} and A. {Howard} and S. {Incerti} and V. {Ivanchenko} and T. {Johnson} and F. {Jones} and T. {Koi} and R. {Kokoulin} and M. {Kossov} and H. {Kurashige} and V. {Lara} and S. {Larsson} and F. {Lei} and O. {Link} and F. {Longo} and M. {Maire} and A. {Mantero} and B. {Mascialino} and I. {McLaren} and P. {Mendez Lorenzo} and K. {Minamimoto} and K. {Murakami} and P. {Nieminen} and L. {Pandola} and S. {Parlati} and L. {Peralta} and J. {Perl} and A. {Pfeiffer} and M. G. {Pia} and A. {Ribon} and P. {Rodrigues} and G. {Russo} and S. {Sadilov} and G. {Santin} and T. {Sasaki} and D. {Smith} and N. {Starkov} and S. {Tanaka} and E. {Tcherniaev} and B. {Tome} and A. {Trindade} and P. {Truscott} and L. {Urban} and M. {Verderi} and A. {Walkden} and J. P. {Wellisch} and D. C. {Williams} and D. {Wright} and H. {Yoshida}},

  journal={IEEE Transactions on Nuclear Science}, 

  title={Geant4 developments and applications}, 

  year={2006},

  volume={53},

  number={1},

  pages={270-278},

  abstract={Geant4 is a software toolkit for the simulation of the passage of particles through matter. It is used by a large number of experiments and projects in a variety of application domains, including high energy physics, astrophysics and space science, medical physics and radiation protection. Its functionality and modeling capabilities continue to be extended, while its performance is enhanced. An overview of recent developments in diverse areas of the toolkit is presented. These include performance optimization for complex setups; improvements for the propagation in fields; new options for event biasing; and additions and improvements in geometry, physics processes and interactive capabilities},

  keywords={physics computing;Geant4;high energy physics;astrophysics;space science;performance optimization;Object oriented modeling;Physics;Production;Kernel;Application software;Large Hadron Collider;Software tools;Medical simulation;Astrophysics;Protection;Electromagnetic interactions;hadronic interactions;object-oriented technology;particle interactions;physics validation;simulation},

  doi={10.1109/TNS.2006.869826},

  ISSN={1558-1578},

  month={Feb},
}

@article{ALLISON2016186,
title = "Recent developments in Geant4",
journal = "Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment",
volume = "835",
pages = "186 - 225",
year = "2016",
issn = "0168-9002",
doi = "https://doi.org/10.1016/j.nima.2016.06.125",
url = "http://www.sciencedirect.com/science/article/pii/S0168900216306957",
author = "J. Allison and K. Amako and J. Apostolakis and P. Arce and M. Asai and T. Aso and E. Bagli and A. Bagulya and S. Banerjee and G. Barrand and B.R. Beck and A.G. Bogdanov and D. Brandt and J.M.C. Brown and H. Burkhardt and Ph. Canal and D. Cano-Ott and S. Chauvie and K. Cho and G.A.P. Cirrone and G. Cooperman and M.A. Cortés-Giraldo and G. Cosmo and G. Cuttone and G. Depaola and L. Desorgher and X. Dong and A. Dotti and V.D. Elvira and G. Folger and Z. Francis and A. Galoyan and L. Garnier and M. Gayer and K.L. Genser and V.M. Grichine and S. Guatelli and P. Guèye and P. Gumplinger and A.S. Howard and I. Hřivnáčová and S. Hwang and S. Incerti and A. Ivanchenko and V.N. Ivanchenko and F.W. Jones and S.Y. Jun and P. Kaitaniemi and N. Karakatsanis and M. Karamitros and M. Kelsey and A. Kimura and T. Koi and H. Kurashige and A. Lechner and S.B. Lee and F. Longo and M. Maire and D. Mancusi and A. Mantero and E. Mendoza and B. Morgan and K. Murakami and T. Nikitina and L. Pandola and P. Paprocki and J. Perl and I. Petrović and M.G. Pia and W. Pokorski and J.M. Quesada and M. Raine and M.A. Reis and A. Ribon and A. {Ristić Fira} and F. Romano and G. Russo and G. Santin and T. Sasaki and D. Sawkey and J.I. Shin and I.I. Strakovsky and A. Taborda and S. Tanaka and B. Tomé and T. Toshito and H.N. Tran and P.R. Truscott and L. Urban and V. Uzhinsky and J.M. Verbeke and M. Verderi and B.L. Wendt and H. Wenzel and D.H. Wright and D.M. Wright and T. Yamashita and J. Yarba and H. Yoshida",
keywords = "High energy physics, Nuclear physics, Radiation, Simulation, Computing",
abstract = "Geant4 is a software toolkit for the simulation of the passage of particles through matter. It is used by a large number of experiments and projects in a variety of application domains, including high energy physics, astrophysics and space science, medical physics and radiation protection. Over the past several years, major changes have been made to the toolkit in order to accommodate the needs of these user communities, and to efficiently exploit the growth of computing power made available by advances in technology. The adaptation of Geant4 to multithreading, advances in physics, detector modeling and visualization, extensions to the toolkit, including biasing and reverse Monte Carlo, and tools for physics and release validation are discussed here."
}

@article{Aad_2010,
   title={The ATLAS Simulation Infrastructure},
   volume={70},
   ISSN={1434-6052},
   url={http://dx.doi.org/10.1140/epjc/s10052-010-1429-9},
   DOI={10.1140/epjc/s10052-010-1429-9},
   number={3},
   journal={The European Physical Journal C},
   publisher={Springer Science and Business Media LLC},
   author={Aad, G. and Abbott, B. and Abdallah, J. and Abdelalim, A. A. and Abdesselam, A. and Abdinov, O. and Abi, B. and Abolins, M. and Abramowicz, H. and et al.},
   year={2010},
   month={Sep},
   pages={823–874}
}
@article{Cranmer2015,
abstract = {In many fields of science, generalized likelihood ratio tests are established tools for statistical inference. At the same time, it has become increasingly common that a simulator (or generative model) is used to describe complex processes that tie parameters $\theta$ of an underlying theory and measurement apparatus to high-dimensional observations $\mathbf{x}\in \mathbb{R}^p$. However, simulator often do not provide a way to evaluate the likelihood function for a given observation $\mathbf{x}$, which motivates a new class of likelihood-free inference algorithms. In this paper, we show that likelihood ratios are invariant under a specific class of dimensionality reduction maps $\mathbb{R}^p \mapsto \mathbb{R}$. As a direct consequence, we show that discriminative classifiers can be used to approximate the generalized likelihood ratio statistic when only a generative model for the data is available. This leads to a new machine learning-based approach to likelihood-free inference that is complementary to Approximate Bayesian Computation, and which does not require a prior on the model parameters. Experimental results on artificial problems with known exact likelihoods illustrate the potential of the proposed method.},
archivePrefix = {arXiv},
arxivId = {1506.02169},
author = {Cranmer, Kyle and Pavez, Juan and Louppe, Gilles},
eprint = {1506.02169},
file = {:home/estrade/Documents/Articles/1506.02169.pdf:pdf},
pages = {1--35},
title = {{Approximating Likelihood Ratios with Calibrated Discriminative Classifiers}},
url = {http://arxiv.org/abs/1506.02169},
year = {2015}
}
@article{Ben-David2010,
abstract = {Discriminative learning methods for classification perform well when\ntraining and test data are drawn from the same distribution. Often,\nhowever, we have plentiful labeled training data from a source domain\nbut wish to learn a classifier which performs well on a target domain\nwith a different distribution and little or no labeled training data. In\nthis work we investigate two questions. First, under what conditions can\na classifier trained from source data be expected to perform well on\ntarget data? Second, given a small amount of labeled target data, how\nshould we combine it during training with the large amount of labeled\nsource data to achieve the lowest target error at test time?\nWe address the first question by bounding a classifier's target error in\nterms of its source error and the divergence between the two domains. We\ngive a classifier-induced divergence measure that can be estimated from\nfinite, unlabeled samples from the domains. Under the assumption that\nthere exists some hypothesis that performs well in both domains, we show\nthat this quantity together with the empirical source error characterize\nthe target error of a source-trained classifier.\nWe answer the second question by bounding the target error of a model\nwhich minimizes a convex combination of the empirical source and target\nerrors. Previous theoretical work has considered minimizing just the\nsource error, just the target error, or weighting instances from the two\ndomains equally. We show how to choose the optimal combination of source\nand target error as a function of the divergence, the sample sizes of\nboth domains, and the complexity of the hypothesis class. The resulting\nbound generalizes the previously studied cases and is always at least as\ntight as a bound which considers minimizing only the target error or an\nequal weighting of source and target errors.},
author = {Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
doi = {10.1007/s10994-009-5152-4},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ben-David et al. - 2010 - A theory of learning from different domains.pdf:pdf},
isbn = {9780838986219},
issn = {15730565},
journal = {Machine Learning},
keywords = {Domain adaptation,Learning theory,Sample-selection bias,Transfer learning},
number = {1-2},
pages = {151--175},
title = {{A theory of learning from different domains}},
volume = {79},
year = {2010}
}

@article{Baldi2014,
abstract = {The Higgs boson is thought to provide the interaction that imparts mass to the fundamental fermions, but while measurements at the Large Hadron Collider (LHC) are consistent with this hypothesis, current analysis techniques lack the statistical power to cross the traditional 5$\sigma$ significance barrier without more data. \emph{Deep learning} techniques have the potential to increase the statistical power of this analysis by \emph{automatically} learning complex, high-level data representations. In this work, deep neural networks are used to detect the decay of the Higgs to a pair of tau leptons. A Bayesian optimization algorithm is used to tune the network architecture and training algorithm hyperparameters, resulting in a deep network of eight non-linear processing layers that improves upon the performance of shallow classifiers even without the use of features specifically engineered by physicists for this application. The improvement in discovery significance is equivalent to an increase in the accumulated dataset of 25\%.},
archivePrefix = {arXiv},
arxivId = {1410.3469},
author = {Baldi, Pierre and Sadowski, Peter and Whiteson, Daniel},
doi = {10.1103/PhysRevLett.114.111801},
eprint = {1410.3469},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baldi, Sadowski, Whiteson - 2014 - Enhanced Higgs to $tautau-$ Searches with Deep Learning.pdf:pdf},
month = {oct},
title = {{Enhanced Higgs to $\tau^+\tau^-$ Searches with Deep Learning}},
url = {http://arxiv.org/abs/1410.3469 http://dx.doi.org/10.1103/PhysRevLett.114.111801},
year = {2014}
}

@article{Baldi_2016,
   title={Parameterized neural networks for high-energy physics},
   volume={76},
   ISSN={1434-6052},
   url={http://dx.doi.org/10.1140/epjc/s10052-016-4099-4},
   DOI={10.1140/epjc/s10052-016-4099-4},
   number={5},
   journal={The European Physical Journal C},
   publisher={Springer Science and Business Media LLC},
   author={Baldi, Pierre and Cranmer, Kyle and Faucett, Taylor and Sadowski, Peter and Whiteson, Daniel},
   year={2016},
   month={Apr}
}


@article{Barlow2002,
abstract = {The treatment of systematic errors is often mishandled. This is due to lack of under-standing and education, based on a fundamental ambiguity as to what is meant by the term. This note addresses the problems and offers guidance to good practice.},
archivePrefix = {arXiv},
arxivId = {arXiv:hep-ex/0207026v1},
author = {Barlow, Roger},
eprint = {0207026v1},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Barlow - 2002 - Systematic Errors Facts and Fictions.pdf:pdf},
primaryClass = {arXiv:hep-ex},
title = {{Systematic Errors: Facts and Fictions}},
year = {2002}
}

% ------------------------------------------------------------------------

@book{Goodfellow-et-al-2016,
annote = {\url{http://www.deeplearningbook.org}},
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
publisher = {MIT Press},
title = {{Deep Learning}},
year = {2016}
}

@inproceedings{goodfellow2014generative,
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in neural information processing systems},
pages = {2672--2680},
title = {{Generative adversarial nets}},
year = {2014}
}

@inproceedings{gal2016dropout,
author = {Gal, Yarin and Ghahramani, Zoubin},
booktitle = {international conference on machine learning},
pages = {1050--1059},
title = {{Dropout as a Bayesian approximation: Representing model uncertainty in deep learning}},
year = {2016}
}

@article{Glorot2010,
abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ﬁrst observe the inﬂuence of the non-linear activations functions. We ﬁnd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ﬁnd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ﬁnd that a new non-linearity that saturates less can often be beneﬁcial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difﬁcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
author = {Glorot, Xavier and Bengio, Yoshua},
doi = {10.1.1.207.2059},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Glorot, Bengio - 2010 - Understanding the difficulty of training deep feedforward neural networks.pdf:pdf},
issn = {15324435},
journal = {Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)},
pages = {249--256},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
url = {http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GlorotB10.pdf},
volume = {9},
year = {2010}
}


@article{JMLR:v17:15-239,
archivePrefix = {arXiv},
arxivId = {1505.07818},
author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
doi = {10.1007/978-3-319-58347-1_10},
eprint = {1505.07818},
isbn = {15324435},
issn = {1475-7516},
journal = {Journal of Machine Learning Research},
number = {59},
pages = {189--209},
title = {{Domain-Adversarial Training of Neural Networks}},
url = {http://link.springer.com/10.1007/978-3-319-58347-1_10},
volume = {17},
year = {2017}
}


@article{Ganin2016,
abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.},
archivePrefix = {arXiv},
arxivId = {1505.07818},
author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
doi = {10.1088/1475-7516/2015/08/013},
eprint = {1505.07818},
file = {:home/estrade/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ganin et al. - 2015 - Domain-Adversarial Training of Neural Networks.pdf:pdf},
isbn = {15324435},
issn = {1475-7516},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,domain adaptation,image classification,neural network,person re-identification,representation learning,sentiment analysis,synthetic data},
mendeley-tags = {domain adaptation},
number = {08},
pages = {013--013},
title = {{Domain-Adversarial Training of Neural Networks}},
url = {http://arxiv.org/abs/1505.07818},
volume = {2015},
year = {2015}
}




% ------------------------------------------------------------------------
% MISC
% ------------------------------------------------------------------------



@article{brown2018superhuman,
  title={Superhuman AI for heads-up no-limit poker: Libratus beats top professionals},
  author={Brown, Noam and Sandholm, Tuomas},
  journal={Science},
  volume={359},
  number={6374},
  pages={418--424},
  year={2018},
  publisher={American Association for the Advancement of Science}
}

@inproceedings{donnot:hal-01783669,
address = {Rio de Janeiro, Brazil},
author = {Donnot, Benjamin and Guyon, Isabelle and Schoenauer, Marc and Marot, Antoine and Panciatici, Patrick},
booktitle = { IEEE WCCI 2018},
title = {{Anticipating contingengies in power grids using fast neural net screening}},
url = {https://hal.archives-ouvertes.fr/hal-01783669},
year = {2018}
}
@article{bengio2013representation,
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {8},
pages = {1798--1828},
publisher = {IEEE},
title = {{Representation learning: A review and new perspectives}},
volume = {35},
year = {2013}
}

@article{srivastava2014dropout,
author = {Srivastava, Nitish and Hinton, Geoffrey E and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
journal = {Journal of Machine Learning Research},
number = {1},
pages = {1929--1958},
title = {{Dropout: a simple way to prevent neural networks from overfitting.}},
volume = {15},
year = {2014}
}

@inproceedings{yosinski2014transferable,
author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
booktitle = {Advances in neural information processing systems},
pages = {3320--3328},
title = {{How transferable are features in deep neural networks?}},
year = {2014}
}


@inproceedings{xiao2016learning,
author = {Xiao, T and Et al.},
booktitle = {Proc. CVPR},
pages = {1249--1258},
title = {{Learning deep feature representations with domain guided dropout for person re-identification}},
year = {2016}
}


@article{silver_mastering_2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ?value networks? to evaluate board positions and ?policy networks? to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
author = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
issn = {0028-0836},
journal = {Nature},
number = {7587},
pages = {484--489},
title = {{Mastering the game of {Go} with deep neural networks and tree search}},
url = {http://dx.doi.org/10.1038/nature16961},
volume = {529},
year = {2016}
}

@inproceedings{koch2015siamese,
author = {Koch, Gregory and Zemel, Richard and Salakhutdinov, Ruslan},
booktitle = {ICML Deep Learning Workshop},
title = {{Siamese neural networks for one-shot image recognition}},
volume = {2},
year = {2015}
}


@article{rosenblatt1958perceptron,
author = {Rosenblatt, Frank},
journal = {Psychological review},
number = {6},
pages = {386},
publisher = {American Psychological Association},
title = {{The perceptron: a probabilistic model for information storage and organization in the brain.}},
volume = {65},
year = {1958}
}

@inproceedings{he2016identity,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {European Conference on Computer Vision},
organization = {Springer},
pages = {630--645},
title = {{Identity mappings in deep residual networks}},
year = {2016}
}

@article{hinton2006reducing,
author = {Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
journal = {science},
number = {5786},
pages = {504--507},
publisher = {American Association for the Advancement of Science},
title = {{Reducing the dimensionality of data with neural networks}},
volume = {313},
year = {2006}
}

@inproceedings{nguyen2015deep,
author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
pages = {427--436},
title = {{Deep neural networks are easily fooled: High confidence predictions for unrecognizable images}},
year = {2015}
}

@inproceedings{bengio2013deep,
author = {Bengio, Yoshua},
booktitle = {International Conference on Statistical Language and Speech Processing},
organization = {Springer},
pages = {1--37},
title = {{Deep learning of representations: Looking forward}},
year = {2013}
}


@inproceedings{krogh1992simple,
author = {Krogh, Anders and Hertz, John A},
booktitle = {Advances in neural information processing systems},
pages = {950--957},
title = {{A simple weight decay can improve generalization}},
year = {1992}
}


@inproceedings{caruana1995learning,
author = {Caruana, Rich},
booktitle = {Advances in neural information processing systems},
pages = {657--664},
title = {{Learning many related tasks at the same time with backpropagation}},
year = {1995}
}


@inproceedings{zemel2013learning,
author = {Zemel, Rich and Wu, Yu and Swersky, Kevin and Pitassi, Toni and Dwork, Cynthia},
booktitle = {International Conference on Machine Learning},
pages = {325--333},
title = {{Learning fair representations}},
year = {2013}
}


@article{lecun1989backpropagation,
author = {LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
journal = {Neural computation},
number = {4},
pages = {541--551},
publisher = {MIT Press},
title = {{Backpropagation applied to handwritten zip code recognition}},
volume = {1},
year = {1989}
}

@inproceedings{salimans2016weight,
author = {Salimans, Tim and Kingma, Diederik P},
booktitle = {Advances in Neural Information Processing Systems},
pages = {901},
title = {{Weight normalization: A simple reparameterization to accelerate training of deep neural networks}},
year = {2016}
}

% ------------------------------------------------------------------------
% ------------------------------------------------------------------------
% ------------------------------------------------------------------------
% ------------------------------------------------------------------------


@article{fisher1919xv,
author = {Fisher, Ronald A},
journal = {Earth and Environmental Science Transactions of the Royal Society of Edinburgh},
number = {2},
pages = {399--433},
publisher = {Royal Society of Edinburgh Scotland Foundation},
title = {{XV.—The correlation between relatives on the supposition of Mendelian inheritance.}},
volume = {52},
year = {1919}
}

@book{Dav88,
address = {Harlow UK},
author = {Davidson, Kenneth R},
publisher = {Longman Scientific & Technical},
series = {Pitman Research Notes in Mathematics Series},
title = {{Nest algebras}},
volume = {191},
year = {1988}
}

@article{Sim92,
author = {Simoni{\v{c}}, A},
journal = {Linear Algebra Appl.},
pages = {57--76},
title = {{Matrix {Groups} with {Positive} {Spectra}}},
volume = {173},
year = {1992}
}

@book{Spi65,
address = {New York},
author = {Spivak, Michael},
publisher = {The Benjamin/Cummings Publishing Company},
title = {{Calculus on {Manifolds}}},
year = {1965}
}


@book{freedman2009statistical,
author = {Freedman, David A},
publisher = {cambridge university press},
title = {{Statistical models: theory and practice}},
year = {2009}
}

@article{hoerl1970ridge,
author = {Hoerl, Arthur E and Kennard, Robert W},
journal = {Technometrics},
number = {1},
pages = {55--67},
publisher = {Taylor & Francis Group},
title = {{Ridge regression: Biased estimation for nonorthogonal problems}},
volume = {12},
year = {1970}
}
