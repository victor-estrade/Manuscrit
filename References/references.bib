% ------------------------------------------------------------------------
% SAMPLE BIBLIOGRAPHY FILE
% ------------------------------------------------------------------------
@misc{cve-2008-1368,
  key =          {CVE-2008-1368},
  title =        {Publication quality tables in \LaTeX*},
  howpublished = {},
  institution  = {NIST},
  day =       17,
  month =     {March},
  year =         2008,
  note =         {[online] \url{http://nvd.nist.gov/nvd.cfm?cvename=CVE-2008-1368}},
  url = {http://nvd.nist.gov/nvd.cfm?cvename=CVE-2008-1368}
}
@article{brown2018superhuman,
  title={Superhuman AI for heads-up no-limit poker: Libratus beats top professionals},
  author={Brown, Noam and Sandholm, Tuomas},
  journal={Science},
  volume={359},
  number={6374},
  pages={418--424},
  year={2018},
  publisher={American Association for the Advancement of Science}
}

@MISC{prime-number-theorem,
   author = "Charles Louis Xavier Joseph de la Vall{\'e}e Poussin",
   note = "A strong form of the prime number theorem, 19th century" }

@TECHREPORT{Bishop94mixturedensity,
    author = {Christopher M. Bishop},
    title = {Mixture density networks},
    institution = {},
    year = {1994}
}

@article{DECASTRO2019170inferno,
title = "INFERNO: Inference-Aware Neural Optimisation",
journal = "Computer Physics Communications",
volume = "244",
pages = "170 - 179",
year = "2019",
issn = "0010-4655",
doi = "https://doi.org/10.1016/j.cpc.2019.06.007",
url = "http://www.sciencedirect.com/science/article/pii/S0010465519301948",
author = "Pablo de Castro and Tommaso Dorigo",
keywords = "Likelihood-free inference, High energy physics, Neural networks, Nuisance parameters",
abstract = "Complex computer simulations are commonly required for accurate data modelling in many scientific disciplines, making statistical inference challenging due to the intractability of the likelihood evaluation for the observed data. Furthermore, sometimes one is interested on inference drawn over a subset of the generative model parameters while taking into account model uncertainty or misspecification on the remaining nuisance parameters. In this work, we show how non-linear summary statistics can be constructed by minimising inference-motivated losses via stochastic gradient descent such that they provide the smallest uncertainty for the parameters of interest. As a use case, the problem of confidence interval estimation for the mixture coefficient in a multi-dimensional two-component mixture model (i.e.Â signal vs background) is considered, where the proposed technique clearly outperforms summary statistics based on probabilistic classification, a commonly used alternative which does not account for the presence of nuisance parameters."
}

@inproceedings{Edwards17neuralstatistician,
  title     = "Towards a Neural Statistician",
  abstract  = "An efficient learner is one who reuses what they already know to tackle a new problem. For a machine learner, this means understanding the similarities amongst datasets. In order to do this, one must take seriously the idea of working with datasets, rather than datapoints, as the key objects to model. Towards this goal, we demonstrate an extension of a variational autoencoder that can learn a method for computing representations, or statistics, of datasets in an unsupervised fashion. The network is trained to produce statistics that encapsulate a generative model for each dataset. Hence the network enables efficient learning from new datasets for both unsupervised and supervised tasks. We show that we are able to learn statistics that can be used for: clustering datasets, transferring generative models to new datasets, selecting representative samples of datasets and classifying previously unseen classes. We refer to our model as a neural statistician, and by this we mean a neural network that can learn to compute summary statistics of datasets without supervision.",
  author    = "Harrison Edwards and Amos Storkey",
  year      = "2017",
  month     = "4",
  day       = "26",
  language  = "English",
  booktitle = "5th International Conference on Learning Representations (ICLR 2017)",
}

@inproceedings{lucas:hal-01791126,
  TITLE = {{Mixed batches and symmetric discriminators for GAN training}},
  AUTHOR = {Lucas, Thomas and Tallec, Corentin and Verbeek, Jakob and Ollivier, Yann},
  URL = {https://hal.inria.fr/hal-01791126},
  BOOKTITLE = {{ICML - 35th International Conference on Machine Learning}},
  ADDRESS = {Stockholm, Sweden},
  SERIES = {Proceedings of Machine Learning Research},
  VOLUME = {80},
  PAGES = {2844-2853},
  YEAR = {2018},
  MONTH = Jul,
  PDF = {https://hal.inria.fr/hal-01791126/file/mixed_batches_sym_disc_for_gans_with_ack%20%281%29.pdf},
  HAL_ID = {hal-01791126},
  HAL_VERSION = {v2},
}
% ------------------------------------------------------------------------

