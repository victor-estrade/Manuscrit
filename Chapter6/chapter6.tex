%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** sixth Chapter **********************************
%*******************************************************************************
\chapter{Discussion and conclusion}
\label{chap:conclusion}
% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter6/Figs/Raster/}{Chapter6/Figs/PDF/}{Chapter6/Figs/}}
\else
    \graphicspath{{Chapter6/Figs/Vector/}{Chapter6/Figs/}}
\fi


\victor{L'idée que c'est un graphical model mais les z sont dans le simulateur}
\victor{L'idée que si c'est sans params de nuisance alors c'est trop facile. On aurait que l'erreur statistique. On ne peut l'améliorer qu'en améliorant le classifieur. D'où vient la variance alors ?}
\victor{Sur les vrais données on doit utiliser une méthode numérique}
\victor{Quel est le budget accessible ? Combien coute la méthode normale en théorie et dans la vrai vie ? On refait vraiment tourner la simulation à chaque fois ? Combien coute ma méthode ?}
\victor{Neural network as Estimator \& estimate its variance}





\section{Calibration} % (fold)
\label{sec:calibration}

\subsection{Common calibration} % (fold)
\label{sub:common_calibration}


Multiple studies using data from the same experiment should have the same nuisance parameter estimation.

La version très conservatrice de la calibration est de prendre une région de controle où le signal est inexistant pour mesurer les paramètres de nuisance.
Puis on garde cette estimation pour toutes les études qui utilisent les données de cette expérimence.
On peut critiquer le fait que la version conservatrice rejette les données hors région de controle pour l'estimation des paramètres de nuisance.
Donc rejette eventuellement beaucoup d'information.
La meilleur inférence est celle qui utilise toutes les données disponnibles.

Une version plus puissante serait d'utiliser toutes les données disponnibles.
Cependant cela augmente de beaucoup les paramètres en jeu puisque tous les paramètres des diverses annalyses doivent être pris en compte.
En effet on ne se limite plus à une région de controle.
Donc même si cette méthode est probablement la meilleur il reste qu'écrire l'algorithme voir simplement écrire la laikelihood pour l'appliquer promet de violents maux de tête.

Une version plus relaxée serait d'utiliser l'estimation dans la région de controle comme prior puis chaque annalyse recalibre sur ses données.
Cela donne autant d'estimation des paramètres de nuisance que d'annalyse.
J'ai du mal à faire confiance à une méthode scientifique où Jean-Paul affirme que $\alpha = 3 \pm 1$ et Jeane-Pauline affirme que $\alpha = 3.3 \pm 1.1$ pour la même expérimentation et que ça ne pose pas de problème.
De plus l'argument anti-conservateur est toujours applicable : on rejette beaucoup d'information en rejettant les données des autres annalyses.

Enfin concernant notre cas la likelihood inclu déjà l'influence des paramètres de nuisance au travers du re-simulateur.
La question est donc : est-ce que cette likelihood est un bon modèle ?
J'ai quelques résultats expérimentaux qui montre que mesurer indépendement les paramètres de nuisance puis utiliser cette estimation dans le workflow donne un estimateur avec moins de variance et apparement moins de biais aussi.
Mais est-ce qu'on utilise pas 2 fois les données pour l'inférence des paramètres de nuisance ?
Est-ce que c'est grave ? Est-ce que c'est un peu de l'overfitting quelque part ?
J'ai envie de dire que c'est juste un changement de modèle ie un changement de likelihood.
Mais est-ce que ce nouveau modèle est mieux ?




\subsection{Prior vs posterior} % (fold)
\label{sub:prior_vs_posterior}

\content{Show the difference between the 2 formulas}
\content{The posterior of one is the prior of others}





\subsection{Update calibration} % (fold)
\label{sub:update_calibration}

Should we use the data to improve inference on the nuisance parameters ?

Technically maximum likelihood inference is doing it through maximizing the likelihood.
\victor{Oui ça semble être une totologie mais en fait c'est pas évident}


\section{Further works} % (fold)
\label{sec:further_works}

Ideas for the next steps

\subsection{Real simulator} % (fold)
\label{sub:real_simulator}

Use a real simulator instead of a trick to get a fast simulator.
Improve it with minong gold idea if possible.
Add calibration steps in the simulation.

This way the entire workflow is under control and monitoring.



\subsection{Estimate the progression margin} % (fold)
\label{sub:estimate_the_progression_margin}

Ideas to find a method to measure the maximum information that can be extracted from the given data.






